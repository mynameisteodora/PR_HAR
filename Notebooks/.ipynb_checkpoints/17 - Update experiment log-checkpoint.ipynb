{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for developing a function to call after executing a script, that picks up the saved stats and parameters and writes them in a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "\n",
    "from utils.constants import *\n",
    "from utils.file_readers import *\n",
    "from utils.data_exploration import *\n",
    "from utils.signal_processing import *\n",
    "from utils.sliding_window import *\n",
    "from utils.stand_norm import *\n",
    "from utils.one_vs_all_training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = '00_sit_to_stand_downsample_16_all_correctness_balance_30'\n",
    "activity_name = 'Sit to stand'\n",
    "correctness = 'all'\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "n_train_subjects = 12\n",
    "n_validation_subjects = 2\n",
    "\n",
    "n_time_steps = 38\n",
    "step = 19\n",
    "n_features = 3\n",
    "\n",
    "features = ['accel_x_normalised', 'accel_y_normalised', 'accel_z_normalised']\n",
    "features_name = 'normalised'\n",
    "\n",
    "num_filters = 64\n",
    "kernel_size = 3\n",
    "activation = 'relu'\n",
    "\n",
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "\n",
    "downsample_rate = 16\n",
    "positive_class_weight = 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_names = get_activity_name_dict()\n",
    "one_vs_all_activity = activity_names[activity_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../Plots/{}\".format(experiment_name)):\n",
    "    os.mkdir(\"../Plots/{}\".format(experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left out subject = adela\n",
      "Test subjects = ['aggie', 'andrius', 'diana', 'jack', 'joao', 'nikita', 'rim', 'ron', 'santi', 'seb', 'sharan', 'teo']\n",
      "Validation subjects = ['lukasz', 'zoe']\n",
      "--------------------------------------------------------------------------------\n",
      "Removing outliers\n",
      "--------------------------------------------------------------------------------\n",
      "Original dataframe length = 117502\n",
      "New dataframe length = 115290\n",
      "Removed 2212 outliers\n",
      "Original dataframe length = 8965\n",
      "New dataframe length = 8749\n",
      "Removed 216 outliers\n",
      "--------------------------------------------------------------------------------\n",
      "Standardising\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Normalising\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generating datasets\n",
      "--------------------------------------------------------------------------------\n",
      "Total samples for true activity = 4952\n",
      "Total samples for false activity = 0\n",
      "Training set shapes: X_train = (4952, 38, 3), y_train = (4952, 2)\n",
      "Total samples for true activity = 700\n",
      "Total samples for false activity = 0\n",
      "Valid set shapes: X_valid = (700, 38, 3), y_valid = (700, 2)\n",
      "Total samples for true activity = 430\n",
      "Total samples for false activity = 0\n",
      "Test set shapes: X_test = (430, 38, 3), y_test = (430, 2)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 36, 64)            640       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 34, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 34, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 32, 64)            12352     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               204900    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 230,702\n",
      "Trainable params: 230,574\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Train on 4952 samples, validate on 700 samples\n",
      "Epoch 1/200\n",
      "4952/4952 [==============================] - 7s 1ms/step - loss: 2.2308 - accuracy: 0.1583 - mse: 0.4875 - val_loss: 0.7313 - val_accuracy: 0.1757 - val_mse: 0.2690\n",
      "Epoch 2/200\n",
      "4952/4952 [==============================] - 4s 859us/step - loss: 1.9220 - accuracy: 0.1559 - mse: 0.5139 - val_loss: 0.7645 - val_accuracy: 0.1800 - val_mse: 0.2855\n",
      "Epoch 3/200\n",
      "4952/4952 [==============================] - 4s 899us/step - loss: 1.7805 - accuracy: 0.1571 - mse: 0.4868 - val_loss: 0.8435 - val_accuracy: 0.1829 - val_mse: 0.3234\n",
      "Epoch 4/200\n",
      "4952/4952 [==============================] - 5s 918us/step - loss: 1.6664 - accuracy: 0.1686 - mse: 0.4532 - val_loss: 0.9710 - val_accuracy: 0.1943 - val_mse: 0.3761\n",
      "Epoch 5/200\n",
      "4952/4952 [==============================] - 5s 924us/step - loss: 1.5836 - accuracy: 0.2025 - mse: 0.4344 - val_loss: 1.0282 - val_accuracy: 0.2514 - val_mse: 0.3906\n",
      "Epoch 6/200\n",
      "4952/4952 [==============================] - 5s 1ms/step - loss: 1.5075 - accuracy: 0.2666 - mse: 0.4055 - val_loss: 1.0101 - val_accuracy: 0.3214 - val_mse: 0.3763\n",
      "Epoch 7/200\n",
      "4952/4952 [==============================] - 6s 1ms/step - loss: 1.4520 - accuracy: 0.3211 - mse: 0.3846 - val_loss: 0.9787 - val_accuracy: 0.3771 - val_mse: 0.3593\n",
      "Epoch 8/200\n",
      "4952/4952 [==============================] - 5s 1ms/step - loss: 1.3952 - accuracy: 0.3817 - mse: 0.3621 - val_loss: 0.9879 - val_accuracy: 0.3957 - val_mse: 0.3583\n",
      "Epoch 9/200\n",
      "4952/4952 [==============================] - 4s 829us/step - loss: 1.3493 - accuracy: 0.4111 - mse: 0.3537 - val_loss: 0.9442 - val_accuracy: 0.4557 - val_mse: 0.3383\n",
      "Epoch 10/200\n",
      "4952/4952 [==============================] - 4s 798us/step - loss: 1.3099 - accuracy: 0.4550 - mse: 0.3347 - val_loss: 0.9330 - val_accuracy: 0.4700 - val_mse: 0.3307\n",
      "Epoch 11/200\n",
      "4952/4952 [==============================] - 4s 811us/step - loss: 1.2756 - accuracy: 0.4842 - mse: 0.3257 - val_loss: 0.8858 - val_accuracy: 0.5157 - val_mse: 0.3113\n",
      "Epoch 12/200\n",
      "4952/4952 [==============================] - 4s 856us/step - loss: 1.2488 - accuracy: 0.5186 - mse: 0.3112 - val_loss: 0.8917 - val_accuracy: 0.5314 - val_mse: 0.3108\n",
      "Epoch 13/200\n",
      "4952/4952 [==============================] - 4s 813us/step - loss: 1.2232 - accuracy: 0.5380 - mse: 0.3054 - val_loss: 0.8759 - val_accuracy: 0.5657 - val_mse: 0.3026\n",
      "Epoch 14/200\n",
      "4952/4952 [==============================] - 4s 806us/step - loss: 1.1965 - accuracy: 0.5594 - mse: 0.2986 - val_loss: 0.8606 - val_accuracy: 0.5871 - val_mse: 0.2956\n",
      "Epoch 15/200\n",
      "4952/4952 [==============================] - 4s 788us/step - loss: 1.1747 - accuracy: 0.5882 - mse: 0.2890 - val_loss: 0.8500 - val_accuracy: 0.6043 - val_mse: 0.2904\n",
      "Epoch 16/200\n",
      "4952/4952 [==============================] - 4s 793us/step - loss: 1.1555 - accuracy: 0.6048 - mse: 0.2837 - val_loss: 0.8415 - val_accuracy: 0.6043 - val_mse: 0.2870\n",
      "Epoch 17/200\n",
      "4952/4952 [==============================] - 4s 821us/step - loss: 1.1377 - accuracy: 0.6095 - mse: 0.2766 - val_loss: 0.8464 - val_accuracy: 0.6114 - val_mse: 0.2867\n",
      "Epoch 18/200\n",
      "4952/4952 [==============================] - 4s 785us/step - loss: 1.1149 - accuracy: 0.6202 - mse: 0.2743 - val_loss: 0.8191 - val_accuracy: 0.6329 - val_mse: 0.2770\n",
      "Epoch 19/200\n",
      "4952/4952 [==============================] - 4s 789us/step - loss: 1.1043 - accuracy: 0.6305 - mse: 0.2684 - val_loss: 0.7996 - val_accuracy: 0.6429 - val_mse: 0.2697\n",
      "Epoch 20/200\n",
      "4952/4952 [==============================] - 4s 774us/step - loss: 1.0877 - accuracy: 0.6414 - mse: 0.2621 - val_loss: 0.8012 - val_accuracy: 0.6400 - val_mse: 0.2694\n",
      "Epoch 21/200\n",
      "4952/4952 [==============================] - 4s 800us/step - loss: 1.0719 - accuracy: 0.6490 - mse: 0.2579 - val_loss: 0.7913 - val_accuracy: 0.6543 - val_mse: 0.2648\n",
      "Epoch 22/200\n",
      "4952/4952 [==============================] - 4s 783us/step - loss: 1.0579 - accuracy: 0.6579 - mse: 0.2529 - val_loss: 0.7776 - val_accuracy: 0.6557 - val_mse: 0.2608\n",
      "Epoch 23/200\n",
      "4952/4952 [==============================] - 4s 772us/step - loss: 1.0386 - accuracy: 0.6579 - mse: 0.2510 - val_loss: 0.7691 - val_accuracy: 0.6614 - val_mse: 0.2569\n",
      "Epoch 24/200\n",
      "4952/4952 [==============================] - 4s 757us/step - loss: 1.0289 - accuracy: 0.6676 - mse: 0.2470 - val_loss: 0.7356 - val_accuracy: 0.6800 - val_mse: 0.2463\n",
      "Epoch 25/200\n",
      "4952/4952 [==============================] - 4s 772us/step - loss: 1.0125 - accuracy: 0.6755 - mse: 0.2395 - val_loss: 0.7313 - val_accuracy: 0.6857 - val_mse: 0.2436\n",
      "Epoch 26/200\n",
      "4952/4952 [==============================] - 4s 781us/step - loss: 1.0030 - accuracy: 0.6795 - mse: 0.2374 - val_loss: 0.7330 - val_accuracy: 0.6857 - val_mse: 0.2440\n",
      "Epoch 27/200\n",
      "4952/4952 [==============================] - 4s 776us/step - loss: 0.9828 - accuracy: 0.6886 - mse: 0.2318 - val_loss: 0.7351 - val_accuracy: 0.6857 - val_mse: 0.2444\n",
      "Epoch 28/200\n",
      "4952/4952 [==============================] - 4s 753us/step - loss: 0.9693 - accuracy: 0.6874 - mse: 0.2292 - val_loss: 0.7166 - val_accuracy: 0.6929 - val_mse: 0.2380\n",
      "Epoch 29/200\n",
      "4952/4952 [==============================] - 4s 755us/step - loss: 0.9597 - accuracy: 0.6914 - mse: 0.2268 - val_loss: 0.6912 - val_accuracy: 0.7014 - val_mse: 0.2294\n",
      "Epoch 30/200\n",
      "4952/4952 [==============================] - 4s 756us/step - loss: 0.9467 - accuracy: 0.7034 - mse: 0.2202 - val_loss: 0.7142 - val_accuracy: 0.6914 - val_mse: 0.2359\n",
      "Epoch 31/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4952/4952 [==============================] - 4s 759us/step - loss: 0.9339 - accuracy: 0.7048 - mse: 0.2187 - val_loss: 0.7111 - val_accuracy: 0.6943 - val_mse: 0.2347\n",
      "Epoch 32/200\n",
      "4952/4952 [==============================] - 4s 760us/step - loss: 0.9337 - accuracy: 0.7025 - mse: 0.2164 - val_loss: 0.7152 - val_accuracy: 0.6900 - val_mse: 0.2360\n",
      "Epoch 33/200\n",
      "4952/4952 [==============================] - 4s 756us/step - loss: 0.9133 - accuracy: 0.7068 - mse: 0.2134 - val_loss: 0.6968 - val_accuracy: 0.7014 - val_mse: 0.2294\n",
      "Epoch 34/200\n",
      "4952/4952 [==============================] - 4s 745us/step - loss: 0.9045 - accuracy: 0.7124 - mse: 0.2116 - val_loss: 0.6719 - val_accuracy: 0.7129 - val_mse: 0.2216\n",
      "Epoch 35/200\n",
      "4952/4952 [==============================] - 4s 759us/step - loss: 0.8945 - accuracy: 0.7171 - mse: 0.2067 - val_loss: 0.6761 - val_accuracy: 0.7143 - val_mse: 0.2222\n",
      "Epoch 36/200\n",
      "4952/4952 [==============================] - 4s 735us/step - loss: 0.8809 - accuracy: 0.7203 - mse: 0.2054 - val_loss: 0.6551 - val_accuracy: 0.7186 - val_mse: 0.2153\n",
      "Epoch 37/200\n",
      "4952/4952 [==============================] - 4s 744us/step - loss: 0.8701 - accuracy: 0.7209 - mse: 0.2015 - val_loss: 0.6614 - val_accuracy: 0.7171 - val_mse: 0.2171\n",
      "Epoch 38/200\n",
      "4952/4952 [==============================] - 4s 742us/step - loss: 0.8595 - accuracy: 0.7292 - mse: 0.1964 - val_loss: 0.6546 - val_accuracy: 0.7214 - val_mse: 0.2146\n",
      "Epoch 39/200\n",
      "4952/4952 [==============================] - 4s 746us/step - loss: 0.8462 - accuracy: 0.7314 - mse: 0.1935 - val_loss: 0.6483 - val_accuracy: 0.7271 - val_mse: 0.2120\n",
      "Epoch 40/200\n",
      "4952/4952 [==============================] - 4s 738us/step - loss: 0.8326 - accuracy: 0.7328 - mse: 0.1926 - val_loss: 0.6299 - val_accuracy: 0.7343 - val_mse: 0.2062\n",
      "Epoch 41/200\n",
      "4952/4952 [==============================] - 4s 734us/step - loss: 0.8334 - accuracy: 0.7357 - mse: 0.1895 - val_loss: 0.6052 - val_accuracy: 0.7371 - val_mse: 0.1980\n",
      "Epoch 42/200\n",
      "4952/4952 [==============================] - 4s 736us/step - loss: 0.8173 - accuracy: 0.7393 - mse: 0.1858 - val_loss: 0.6148 - val_accuracy: 0.7343 - val_mse: 0.2009\n",
      "Epoch 43/200\n",
      "4952/4952 [==============================] - 4s 731us/step - loss: 0.8092 - accuracy: 0.7399 - mse: 0.1845 - val_loss: 0.5722 - val_accuracy: 0.7400 - val_mse: 0.1871\n",
      "Epoch 44/200\n",
      "4952/4952 [==============================] - 4s 731us/step - loss: 0.8020 - accuracy: 0.7476 - mse: 0.1782 - val_loss: 0.6007 - val_accuracy: 0.7386 - val_mse: 0.1958\n",
      "Epoch 45/200\n",
      "4952/4952 [==============================] - 4s 730us/step - loss: 0.7843 - accuracy: 0.7516 - mse: 0.1764 - val_loss: 0.6067 - val_accuracy: 0.7400 - val_mse: 0.1972\n",
      "Epoch 46/200\n",
      "4952/4952 [==============================] - 4s 737us/step - loss: 0.7789 - accuracy: 0.7500 - mse: 0.1764 - val_loss: 0.5627 - val_accuracy: 0.7486 - val_mse: 0.1831\n",
      "Epoch 47/200\n",
      "4952/4952 [==============================] - 4s 743us/step - loss: 0.7672 - accuracy: 0.7563 - mse: 0.1726 - val_loss: 0.5834 - val_accuracy: 0.7414 - val_mse: 0.1895\n",
      "Epoch 48/200\n",
      "4952/4952 [==============================] - 4s 741us/step - loss: 0.7515 - accuracy: 0.7599 - mse: 0.1682 - val_loss: 0.5579 - val_accuracy: 0.7529 - val_mse: 0.1813\n",
      "Epoch 49/200\n",
      "4952/4952 [==============================] - 4s 754us/step - loss: 0.7546 - accuracy: 0.7591 - mse: 0.1684 - val_loss: 0.5652 - val_accuracy: 0.7557 - val_mse: 0.1834\n",
      "Epoch 50/200\n",
      "4952/4952 [==============================] - 4s 737us/step - loss: 0.7438 - accuracy: 0.7649 - mse: 0.1643 - val_loss: 0.5685 - val_accuracy: 0.7543 - val_mse: 0.1843\n",
      "Epoch 51/200\n",
      "4952/4952 [==============================] - 4s 738us/step - loss: 0.7397 - accuracy: 0.7629 - mse: 0.1639 - val_loss: 0.5573 - val_accuracy: 0.7543 - val_mse: 0.1805\n",
      "Epoch 52/200\n",
      "4952/4952 [==============================] - 4s 742us/step - loss: 0.7294 - accuracy: 0.7714 - mse: 0.1595 - val_loss: 0.5612 - val_accuracy: 0.7557 - val_mse: 0.1813\n",
      "Epoch 53/200\n",
      "4952/4952 [==============================] - 4s 741us/step - loss: 0.7204 - accuracy: 0.7728 - mse: 0.1575 - val_loss: 0.5646 - val_accuracy: 0.7529 - val_mse: 0.1821\n",
      "Epoch 54/200\n",
      "4952/4952 [==============================] - 4s 740us/step - loss: 0.7217 - accuracy: 0.7730 - mse: 0.1595 - val_loss: 0.5385 - val_accuracy: 0.7571 - val_mse: 0.1739\n",
      "Epoch 55/200\n",
      "4952/4952 [==============================] - 4s 734us/step - loss: 0.7030 - accuracy: 0.7783 - mse: 0.1536 - val_loss: 0.5505 - val_accuracy: 0.7571 - val_mse: 0.1773\n",
      "Epoch 56/200\n",
      "4952/4952 [==============================] - 4s 737us/step - loss: 0.6956 - accuracy: 0.7787 - mse: 0.1527 - val_loss: 0.5462 - val_accuracy: 0.7557 - val_mse: 0.1758\n",
      "Epoch 57/200\n",
      "4952/4952 [==============================] - 4s 732us/step - loss: 0.6870 - accuracy: 0.7835 - mse: 0.1493 - val_loss: 0.5471 - val_accuracy: 0.7586 - val_mse: 0.1760\n",
      "Epoch 58/200\n",
      "4952/4952 [==============================] - 4s 734us/step - loss: 0.6810 - accuracy: 0.7815 - mse: 0.1493 - val_loss: 0.5142 - val_accuracy: 0.7671 - val_mse: 0.1655\n",
      "Epoch 59/200\n",
      "4952/4952 [==============================] - 4s 730us/step - loss: 0.6724 - accuracy: 0.7912 - mse: 0.1440 - val_loss: 0.5327 - val_accuracy: 0.7643 - val_mse: 0.1709\n",
      "Epoch 60/200\n",
      "4952/4952 [==============================] - 4s 732us/step - loss: 0.6698 - accuracy: 0.7906 - mse: 0.1448 - val_loss: 0.5002 - val_accuracy: 0.7786 - val_mse: 0.1608\n",
      "Epoch 61/200\n",
      "4952/4952 [==============================] - 4s 734us/step - loss: 0.6588 - accuracy: 0.7962 - mse: 0.1414 - val_loss: 0.5249 - val_accuracy: 0.7643 - val_mse: 0.1683\n",
      "Epoch 62/200\n",
      "4952/4952 [==============================] - 4s 728us/step - loss: 0.6587 - accuracy: 0.7954 - mse: 0.1419 - val_loss: 0.5137 - val_accuracy: 0.7714 - val_mse: 0.1646\n",
      "Epoch 63/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.6523 - accuracy: 0.7991 - mse: 0.1392 - val_loss: 0.4950 - val_accuracy: 0.7800 - val_mse: 0.1586\n",
      "Epoch 64/200\n",
      "4952/4952 [==============================] - 4s 715us/step - loss: 0.6375 - accuracy: 0.8041 - mse: 0.1361 - val_loss: 0.4957 - val_accuracy: 0.7771 - val_mse: 0.1588\n",
      "Epoch 65/200\n",
      "4952/4952 [==============================] - 4s 726us/step - loss: 0.6342 - accuracy: 0.8100 - mse: 0.1332 - val_loss: 0.4965 - val_accuracy: 0.7771 - val_mse: 0.1588\n",
      "Epoch 66/200\n",
      "4952/4952 [==============================] - 4s 730us/step - loss: 0.6286 - accuracy: 0.8035 - mse: 0.1351 - val_loss: 0.4710 - val_accuracy: 0.7900 - val_mse: 0.1506\n",
      "Epoch 67/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.6229 - accuracy: 0.8106 - mse: 0.1322 - val_loss: 0.4974 - val_accuracy: 0.7757 - val_mse: 0.1586\n",
      "Epoch 68/200\n",
      "4952/4952 [==============================] - 4s 731us/step - loss: 0.6150 - accuracy: 0.8136 - mse: 0.1289 - val_loss: 0.4828 - val_accuracy: 0.7843 - val_mse: 0.1543\n",
      "Epoch 69/200\n",
      "4952/4952 [==============================] - 4s 718us/step - loss: 0.6139 - accuracy: 0.8120 - mse: 0.1315 - val_loss: 0.4391 - val_accuracy: 0.7957 - val_mse: 0.1405\n",
      "Epoch 70/200\n",
      "4952/4952 [==============================] - 4s 721us/step - loss: 0.5995 - accuracy: 0.8197 - mse: 0.1258 - val_loss: 0.4855 - val_accuracy: 0.7829 - val_mse: 0.1546\n",
      "Epoch 71/200\n",
      "4952/4952 [==============================] - 4s 719us/step - loss: 0.5935 - accuracy: 0.8221 - mse: 0.1254 - val_loss: 0.4490 - val_accuracy: 0.7971 - val_mse: 0.1434\n",
      "Epoch 72/200\n",
      "4952/4952 [==============================] - 4s 721us/step - loss: 0.5898 - accuracy: 0.8253 - mse: 0.1234 - val_loss: 0.4381 - val_accuracy: 0.7986 - val_mse: 0.1400\n",
      "Epoch 73/200\n",
      "4952/4952 [==============================] - 4s 716us/step - loss: 0.5856 - accuracy: 0.8277 - mse: 0.1206 - val_loss: 0.4830 - val_accuracy: 0.7857 - val_mse: 0.1531\n",
      "Epoch 74/200\n",
      "4952/4952 [==============================] - 4s 712us/step - loss: 0.5741 - accuracy: 0.8284 - mse: 0.1212 - val_loss: 0.4584 - val_accuracy: 0.7957 - val_mse: 0.1456\n",
      "Epoch 75/200\n",
      "4952/4952 [==============================] - 4s 717us/step - loss: 0.5673 - accuracy: 0.8312 - mse: 0.1174 - val_loss: 0.4436 - val_accuracy: 0.8000 - val_mse: 0.1409\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4952/4952 [==============================] - 4s 711us/step - loss: 0.5617 - accuracy: 0.8354 - mse: 0.1161 - val_loss: 0.4454 - val_accuracy: 0.8000 - val_mse: 0.1414\n",
      "Epoch 77/200\n",
      "4952/4952 [==============================] - 4s 715us/step - loss: 0.5623 - accuracy: 0.8330 - mse: 0.1163 - val_loss: 0.4193 - val_accuracy: 0.8100 - val_mse: 0.1336\n",
      "Epoch 78/200\n",
      "4952/4952 [==============================] - 4s 709us/step - loss: 0.5562 - accuracy: 0.8350 - mse: 0.1143 - val_loss: 0.4290 - val_accuracy: 0.8071 - val_mse: 0.1360\n",
      "Epoch 79/200\n",
      "4952/4952 [==============================] - 4s 710us/step - loss: 0.5546 - accuracy: 0.8389 - mse: 0.1136 - val_loss: 0.4478 - val_accuracy: 0.8000 - val_mse: 0.1417\n",
      "Epoch 80/200\n",
      "4952/4952 [==============================] - 4s 710us/step - loss: 0.5444 - accuracy: 0.8419 - mse: 0.1100 - val_loss: 0.4365 - val_accuracy: 0.8057 - val_mse: 0.1383\n",
      "Epoch 81/200\n",
      "4952/4952 [==============================] - 4s 717us/step - loss: 0.5416 - accuracy: 0.8401 - mse: 0.1122 - val_loss: 0.4096 - val_accuracy: 0.8114 - val_mse: 0.1301\n",
      "Epoch 82/200\n",
      "4952/4952 [==============================] - 4s 719us/step - loss: 0.5414 - accuracy: 0.8417 - mse: 0.1110 - val_loss: 0.4228 - val_accuracy: 0.8100 - val_mse: 0.1339\n",
      "Epoch 83/200\n",
      "4952/4952 [==============================] - 4s 727us/step - loss: 0.5301 - accuracy: 0.8445 - mse: 0.1084 - val_loss: 0.4263 - val_accuracy: 0.8114 - val_mse: 0.1348\n",
      "Epoch 84/200\n",
      "4952/4952 [==============================] - 4s 713us/step - loss: 0.5227 - accuracy: 0.8477 - mse: 0.1060 - val_loss: 0.3940 - val_accuracy: 0.8243 - val_mse: 0.1251\n",
      "Epoch 85/200\n",
      "4952/4952 [==============================] - 4s 720us/step - loss: 0.5207 - accuracy: 0.8485 - mse: 0.1065 - val_loss: 0.4220 - val_accuracy: 0.8186 - val_mse: 0.1331\n",
      "Epoch 86/200\n",
      "4952/4952 [==============================] - 4s 715us/step - loss: 0.5093 - accuracy: 0.8550 - mse: 0.1028 - val_loss: 0.4528 - val_accuracy: 0.7971 - val_mse: 0.1427\n",
      "Epoch 87/200\n",
      "4952/4952 [==============================] - 4s 751us/step - loss: 0.5152 - accuracy: 0.8506 - mse: 0.1033 - val_loss: 0.3881 - val_accuracy: 0.8286 - val_mse: 0.1226\n",
      "Epoch 88/200\n",
      "4952/4952 [==============================] - 4s 855us/step - loss: 0.5043 - accuracy: 0.8534 - mse: 0.1018 - val_loss: 0.4129 - val_accuracy: 0.8200 - val_mse: 0.1303\n",
      "Epoch 89/200\n",
      "4952/4952 [==============================] - 4s 856us/step - loss: 0.5018 - accuracy: 0.8584 - mse: 0.1008 - val_loss: 0.4382 - val_accuracy: 0.8114 - val_mse: 0.1375\n",
      "Epoch 90/200\n",
      "4952/4952 [==============================] - 4s 716us/step - loss: 0.4953 - accuracy: 0.8578 - mse: 0.0997 - val_loss: 0.3930 - val_accuracy: 0.8286 - val_mse: 0.1237\n",
      "Epoch 91/200\n",
      "4952/4952 [==============================] - 4s 713us/step - loss: 0.4925 - accuracy: 0.8621 - mse: 0.0977 - val_loss: 0.4218 - val_accuracy: 0.8186 - val_mse: 0.1327\n",
      "Epoch 92/200\n",
      "4952/4952 [==============================] - 4s 710us/step - loss: 0.4902 - accuracy: 0.8584 - mse: 0.1001 - val_loss: 0.3799 - val_accuracy: 0.8343 - val_mse: 0.1200\n",
      "Epoch 93/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.4892 - accuracy: 0.8649 - mse: 0.0947 - val_loss: 0.3910 - val_accuracy: 0.8300 - val_mse: 0.1232\n",
      "Epoch 94/200\n",
      "4952/4952 [==============================] - 4s 709us/step - loss: 0.4834 - accuracy: 0.8627 - mse: 0.0973 - val_loss: 0.3758 - val_accuracy: 0.8357 - val_mse: 0.1181\n",
      "Epoch 95/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.4714 - accuracy: 0.8671 - mse: 0.0944 - val_loss: 0.3716 - val_accuracy: 0.8357 - val_mse: 0.1172\n",
      "Epoch 96/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.4694 - accuracy: 0.8667 - mse: 0.0936 - val_loss: 0.3744 - val_accuracy: 0.8357 - val_mse: 0.1181\n",
      "Epoch 97/200\n",
      "4952/4952 [==============================] - 4s 718us/step - loss: 0.4718 - accuracy: 0.8683 - mse: 0.0925 - val_loss: 0.3906 - val_accuracy: 0.8314 - val_mse: 0.1227\n",
      "Epoch 98/200\n",
      "4952/4952 [==============================] - 4s 718us/step - loss: 0.4604 - accuracy: 0.8714 - mse: 0.0922 - val_loss: 0.3773 - val_accuracy: 0.8357 - val_mse: 0.1190\n",
      "Epoch 99/200\n",
      "4952/4952 [==============================] - 4s 730us/step - loss: 0.4591 - accuracy: 0.8697 - mse: 0.0911 - val_loss: 0.3319 - val_accuracy: 0.8529 - val_mse: 0.1051\n",
      "Epoch 100/200\n",
      "4952/4952 [==============================] - 4s 725us/step - loss: 0.4572 - accuracy: 0.8746 - mse: 0.0880 - val_loss: 0.3832 - val_accuracy: 0.8329 - val_mse: 0.1208\n",
      "Epoch 101/200\n",
      "4952/4952 [==============================] - 4s 713us/step - loss: 0.4536 - accuracy: 0.8728 - mse: 0.0889 - val_loss: 0.3407 - val_accuracy: 0.8486 - val_mse: 0.1074\n",
      "Epoch 102/200\n",
      "4952/4952 [==============================] - 4s 724us/step - loss: 0.4520 - accuracy: 0.8728 - mse: 0.0898 - val_loss: 0.3629 - val_accuracy: 0.8414 - val_mse: 0.1141\n",
      "Epoch 103/200\n",
      "4952/4952 [==============================] - 4s 724us/step - loss: 0.4503 - accuracy: 0.8744 - mse: 0.0886 - val_loss: 0.3333 - val_accuracy: 0.8514 - val_mse: 0.1053\n",
      "Epoch 104/200\n",
      "4952/4952 [==============================] - 4s 709us/step - loss: 0.4378 - accuracy: 0.8823 - mse: 0.0840 - val_loss: 0.3563 - val_accuracy: 0.8414 - val_mse: 0.1121\n",
      "Epoch 105/200\n",
      "4952/4952 [==============================] - 4s 733us/step - loss: 0.4426 - accuracy: 0.8760 - mse: 0.0867 - val_loss: 0.3476 - val_accuracy: 0.8529 - val_mse: 0.1089\n",
      "Epoch 106/200\n",
      "4952/4952 [==============================] - 4s 719us/step - loss: 0.4444 - accuracy: 0.8784 - mse: 0.0866 - val_loss: 0.3466 - val_accuracy: 0.8500 - val_mse: 0.1088\n",
      "Epoch 107/200\n",
      "4952/4952 [==============================] - 4s 715us/step - loss: 0.4299 - accuracy: 0.8809 - mse: 0.0835 - val_loss: 0.3359 - val_accuracy: 0.8486 - val_mse: 0.1060\n",
      "Epoch 108/200\n",
      "4952/4952 [==============================] - 4s 712us/step - loss: 0.4300 - accuracy: 0.8827 - mse: 0.0836 - val_loss: 0.3579 - val_accuracy: 0.8486 - val_mse: 0.1122\n",
      "Epoch 109/200\n",
      "4952/4952 [==============================] - 4s 713us/step - loss: 0.4314 - accuracy: 0.8827 - mse: 0.0838 - val_loss: 0.3441 - val_accuracy: 0.8486 - val_mse: 0.1082\n",
      "Epoch 110/200\n",
      "4952/4952 [==============================] - 4s 715us/step - loss: 0.4219 - accuracy: 0.8833 - mse: 0.0828 - val_loss: 0.3690 - val_accuracy: 0.8429 - val_mse: 0.1154\n",
      "Epoch 111/200\n",
      "4952/4952 [==============================] - 4s 717us/step - loss: 0.4239 - accuracy: 0.8867 - mse: 0.0815 - val_loss: 0.3735 - val_accuracy: 0.8371 - val_mse: 0.1172\n",
      "Epoch 112/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.4166 - accuracy: 0.8885 - mse: 0.0798 - val_loss: 0.3438 - val_accuracy: 0.8486 - val_mse: 0.1081\n",
      "Epoch 113/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.4163 - accuracy: 0.8859 - mse: 0.0810 - val_loss: 0.3691 - val_accuracy: 0.8429 - val_mse: 0.1154\n",
      "Epoch 114/200\n",
      "4952/4952 [==============================] - 4s 722us/step - loss: 0.4139 - accuracy: 0.8905 - mse: 0.0789 - val_loss: 0.3654 - val_accuracy: 0.8429 - val_mse: 0.1143\n",
      "Epoch 115/200\n",
      "4952/4952 [==============================] - 4s 717us/step - loss: 0.4132 - accuracy: 0.8905 - mse: 0.0789 - val_loss: 0.3485 - val_accuracy: 0.8500 - val_mse: 0.1090\n",
      "Epoch 116/200\n",
      "4952/4952 [==============================] - 4s 740us/step - loss: 0.4143 - accuracy: 0.8857 - mse: 0.0807 - val_loss: 0.3523 - val_accuracy: 0.8529 - val_mse: 0.1100\n",
      "Epoch 117/200\n",
      "4952/4952 [==============================] - 4s 717us/step - loss: 0.4031 - accuracy: 0.8924 - mse: 0.0765 - val_loss: 0.3319 - val_accuracy: 0.8600 - val_mse: 0.1040\n",
      "Epoch 118/200\n",
      "4952/4952 [==============================] - 4s 718us/step - loss: 0.4046 - accuracy: 0.8899 - mse: 0.0784 - val_loss: 0.3596 - val_accuracy: 0.8471 - val_mse: 0.1123\n",
      "Epoch 119/200\n",
      "4952/4952 [==============================] - 4s 729us/step - loss: 0.3938 - accuracy: 0.8986 - mse: 0.0737 - val_loss: 0.3443 - val_accuracy: 0.8529 - val_mse: 0.1076\n",
      "Epoch 120/200\n",
      "4952/4952 [==============================] - 4s 721us/step - loss: 0.4020 - accuracy: 0.8893 - mse: 0.0786 - val_loss: 0.3388 - val_accuracy: 0.8586 - val_mse: 0.1060\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4952/4952 [==============================] - 4s 718us/step - loss: 0.3928 - accuracy: 0.8954 - mse: 0.0759 - val_loss: 0.3034 - val_accuracy: 0.8714 - val_mse: 0.0956\n",
      "Epoch 122/200\n",
      "4952/4952 [==============================] - 4s 710us/step - loss: 0.3924 - accuracy: 0.8956 - mse: 0.0740 - val_loss: 0.3246 - val_accuracy: 0.8614 - val_mse: 0.1019\n",
      "Epoch 123/200\n",
      "4952/4952 [==============================] - 4s 716us/step - loss: 0.3841 - accuracy: 0.9008 - mse: 0.0713 - val_loss: 0.3452 - val_accuracy: 0.8514 - val_mse: 0.1077\n",
      "Epoch 124/200\n",
      "4952/4952 [==============================] - 4s 711us/step - loss: 0.3851 - accuracy: 0.8946 - mse: 0.0740 - val_loss: 0.3035 - val_accuracy: 0.8700 - val_mse: 0.0956\n",
      "Epoch 125/200\n",
      "4952/4952 [==============================] - 4s 715us/step - loss: 0.3900 - accuracy: 0.8952 - mse: 0.0748 - val_loss: 0.3436 - val_accuracy: 0.8571 - val_mse: 0.1072\n",
      "Epoch 126/200\n",
      "4952/4952 [==============================] - 4s 707us/step - loss: 0.3812 - accuracy: 0.8974 - mse: 0.0725 - val_loss: 0.3652 - val_accuracy: 0.8529 - val_mse: 0.1134\n",
      "Epoch 127/200\n",
      "4952/4952 [==============================] - 4s 710us/step - loss: 0.3735 - accuracy: 0.8986 - mse: 0.0720 - val_loss: 0.3049 - val_accuracy: 0.8714 - val_mse: 0.0960\n",
      "Epoch 128/200\n",
      "4952/4952 [==============================] - 4s 717us/step - loss: 0.3745 - accuracy: 0.9027 - mse: 0.0701 - val_loss: 0.3039 - val_accuracy: 0.8700 - val_mse: 0.0957\n",
      "Epoch 129/200\n",
      "4952/4952 [==============================] - 4s 723us/step - loss: 0.3596 - accuracy: 0.8990 - mse: 0.0710 - val_loss: 0.2768 - val_accuracy: 0.8786 - val_mse: 0.0878\n",
      "Epoch 130/200\n",
      "4952/4952 [==============================] - 4s 707us/step - loss: 0.3733 - accuracy: 0.9041 - mse: 0.0678 - val_loss: 0.3607 - val_accuracy: 0.8486 - val_mse: 0.1123\n",
      "Epoch 131/200\n",
      "4952/4952 [==============================] - 4s 720us/step - loss: 0.3665 - accuracy: 0.9023 - mse: 0.0682 - val_loss: 0.3186 - val_accuracy: 0.8686 - val_mse: 0.0997\n",
      "Epoch 132/200\n",
      "4952/4952 [==============================] - 4s 712us/step - loss: 0.3718 - accuracy: 0.8970 - mse: 0.0726 - val_loss: 0.3092 - val_accuracy: 0.8714 - val_mse: 0.0971\n",
      "Epoch 133/200\n",
      "4952/4952 [==============================] - 4s 732us/step - loss: 0.3631 - accuracy: 0.9043 - mse: 0.0674 - val_loss: 0.3346 - val_accuracy: 0.8614 - val_mse: 0.1046\n",
      "Epoch 134/200\n",
      "4952/4952 [==============================] - 4s 718us/step - loss: 0.3604 - accuracy: 0.9031 - mse: 0.0700 - val_loss: 0.2852 - val_accuracy: 0.8771 - val_mse: 0.0901\n",
      "Epoch 135/200\n",
      "4952/4952 [==============================] - 4s 709us/step - loss: 0.3546 - accuracy: 0.9095 - mse: 0.0646 - val_loss: 0.3014 - val_accuracy: 0.8729 - val_mse: 0.0948\n",
      "Epoch 136/200\n",
      "4952/4952 [==============================] - 4s 724us/step - loss: 0.3607 - accuracy: 0.9063 - mse: 0.0674 - val_loss: 0.3697 - val_accuracy: 0.8500 - val_mse: 0.1141\n",
      "Epoch 137/200\n",
      "4952/4952 [==============================] - 4s 718us/step - loss: 0.3541 - accuracy: 0.9025 - mse: 0.0693 - val_loss: 0.2848 - val_accuracy: 0.8757 - val_mse: 0.0897\n",
      "Epoch 138/200\n",
      "4952/4952 [==============================] - 4s 712us/step - loss: 0.3556 - accuracy: 0.9061 - mse: 0.0665 - val_loss: 0.3021 - val_accuracy: 0.8729 - val_mse: 0.0947\n",
      "Epoch 139/200\n",
      "4952/4952 [==============================] - 4s 707us/step - loss: 0.3452 - accuracy: 0.9085 - mse: 0.0645 - val_loss: 0.2938 - val_accuracy: 0.8729 - val_mse: 0.0925\n",
      "Epoch 140/200\n",
      "4952/4952 [==============================] - 4s 719us/step - loss: 0.3529 - accuracy: 0.9103 - mse: 0.0652 - val_loss: 0.3099 - val_accuracy: 0.8714 - val_mse: 0.0971\n",
      "Epoch 141/200\n",
      "4952/4952 [==============================] - 4s 712us/step - loss: 0.3439 - accuracy: 0.9107 - mse: 0.0651 - val_loss: 0.3146 - val_accuracy: 0.8714 - val_mse: 0.0980\n",
      "Epoch 142/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.3384 - accuracy: 0.9111 - mse: 0.0631 - val_loss: 0.2717 - val_accuracy: 0.8800 - val_mse: 0.0858\n",
      "Epoch 143/200\n",
      "4952/4952 [==============================] - 4s 717us/step - loss: 0.3470 - accuracy: 0.9095 - mse: 0.0649 - val_loss: 0.2999 - val_accuracy: 0.8729 - val_mse: 0.0943\n",
      "Epoch 144/200\n",
      "4952/4952 [==============================] - 4s 710us/step - loss: 0.3421 - accuracy: 0.9113 - mse: 0.0649 - val_loss: 0.2931 - val_accuracy: 0.8757 - val_mse: 0.0918\n",
      "Epoch 145/200\n",
      "4952/4952 [==============================] - 4s 716us/step - loss: 0.3435 - accuracy: 0.9120 - mse: 0.0628 - val_loss: 0.3560 - val_accuracy: 0.8543 - val_mse: 0.1099\n",
      "Epoch 146/200\n",
      "4952/4952 [==============================] - 4s 718us/step - loss: 0.3392 - accuracy: 0.9095 - mse: 0.0647 - val_loss: 0.3120 - val_accuracy: 0.8729 - val_mse: 0.0974\n",
      "Epoch 147/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.3400 - accuracy: 0.9113 - mse: 0.0629 - val_loss: 0.2755 - val_accuracy: 0.8786 - val_mse: 0.0870\n",
      "Epoch 148/200\n",
      "4952/4952 [==============================] - 4s 718us/step - loss: 0.3324 - accuracy: 0.9162 - mse: 0.0617 - val_loss: 0.3446 - val_accuracy: 0.8600 - val_mse: 0.1068\n",
      "Epoch 149/200\n",
      "4952/4952 [==============================] - 4s 717us/step - loss: 0.3321 - accuracy: 0.9136 - mse: 0.0626 - val_loss: 0.2710 - val_accuracy: 0.8814 - val_mse: 0.0857\n",
      "Epoch 150/200\n",
      "4952/4952 [==============================] - 4s 734us/step - loss: 0.3334 - accuracy: 0.9168 - mse: 0.0607 - val_loss: 0.3022 - val_accuracy: 0.8729 - val_mse: 0.0947\n",
      "Epoch 151/200\n",
      "4952/4952 [==============================] - 4s 719us/step - loss: 0.3276 - accuracy: 0.9111 - mse: 0.0629 - val_loss: 0.2961 - val_accuracy: 0.8729 - val_mse: 0.0929\n",
      "Epoch 152/200\n",
      "4952/4952 [==============================] - 4s 712us/step - loss: 0.3234 - accuracy: 0.9166 - mse: 0.0589 - val_loss: 0.2918 - val_accuracy: 0.8771 - val_mse: 0.0913\n",
      "Epoch 153/200\n",
      "4952/4952 [==============================] - 4s 723us/step - loss: 0.3338 - accuracy: 0.9144 - mse: 0.0614 - val_loss: 0.3802 - val_accuracy: 0.8514 - val_mse: 0.1159\n",
      "Epoch 154/200\n",
      "4952/4952 [==============================] - 4s 721us/step - loss: 0.3246 - accuracy: 0.9156 - mse: 0.0599 - val_loss: 0.2757 - val_accuracy: 0.8771 - val_mse: 0.0868\n",
      "Epoch 155/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.3262 - accuracy: 0.9160 - mse: 0.0607 - val_loss: 0.3397 - val_accuracy: 0.8657 - val_mse: 0.1047\n",
      "Epoch 156/200\n",
      "4952/4952 [==============================] - 4s 719us/step - loss: 0.3227 - accuracy: 0.9166 - mse: 0.0596 - val_loss: 0.2748 - val_accuracy: 0.8786 - val_mse: 0.0866\n",
      "Epoch 157/200\n",
      "4952/4952 [==============================] - 4s 712us/step - loss: 0.3238 - accuracy: 0.9174 - mse: 0.0601 - val_loss: 0.3516 - val_accuracy: 0.8629 - val_mse: 0.1079\n",
      "Epoch 158/200\n",
      "4952/4952 [==============================] - 4s 712us/step - loss: 0.3205 - accuracy: 0.9182 - mse: 0.0591 - val_loss: 0.3566 - val_accuracy: 0.8586 - val_mse: 0.1100\n",
      "Epoch 159/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.3216 - accuracy: 0.9172 - mse: 0.0589 - val_loss: 0.3005 - val_accuracy: 0.8729 - val_mse: 0.0940\n",
      "Epoch 160/200\n",
      "4952/4952 [==============================] - 4s 715us/step - loss: 0.3209 - accuracy: 0.9182 - mse: 0.0591 - val_loss: 0.2866 - val_accuracy: 0.8757 - val_mse: 0.0898\n",
      "Epoch 161/200\n",
      "4952/4952 [==============================] - 4s 717us/step - loss: 0.3092 - accuracy: 0.9170 - mse: 0.0584 - val_loss: 0.3326 - val_accuracy: 0.8686 - val_mse: 0.1025\n",
      "Epoch 162/200\n",
      "4952/4952 [==============================] - 4s 711us/step - loss: 0.3164 - accuracy: 0.9214 - mse: 0.0566 - val_loss: 0.3178 - val_accuracy: 0.8714 - val_mse: 0.0986\n",
      "Epoch 163/200\n",
      "4952/4952 [==============================] - 4s 715us/step - loss: 0.3121 - accuracy: 0.9204 - mse: 0.0592 - val_loss: 0.2778 - val_accuracy: 0.8800 - val_mse: 0.0872\n",
      "Epoch 164/200\n",
      "4952/4952 [==============================] - 4s 719us/step - loss: 0.3072 - accuracy: 0.9196 - mse: 0.0553 - val_loss: 0.2876 - val_accuracy: 0.8771 - val_mse: 0.0898\n",
      "Epoch 165/200\n",
      "4952/4952 [==============================] - 4s 731us/step - loss: 0.3021 - accuracy: 0.9200 - mse: 0.0563 - val_loss: 0.2994 - val_accuracy: 0.8714 - val_mse: 0.0934\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4952/4952 [==============================] - 4s 708us/step - loss: 0.3073 - accuracy: 0.9212 - mse: 0.0569 - val_loss: 0.3106 - val_accuracy: 0.8714 - val_mse: 0.0964\n",
      "Epoch 167/200\n",
      "4952/4952 [==============================] - 4s 733us/step - loss: 0.3077 - accuracy: 0.9225 - mse: 0.0565 - val_loss: 0.2494 - val_accuracy: 0.8886 - val_mse: 0.0789\n",
      "Epoch 168/200\n",
      "4952/4952 [==============================] - 4s 717us/step - loss: 0.3045 - accuracy: 0.9223 - mse: 0.0563 - val_loss: 0.2742 - val_accuracy: 0.8800 - val_mse: 0.0859\n",
      "Epoch 169/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.3083 - accuracy: 0.9221 - mse: 0.0551 - val_loss: 0.2788 - val_accuracy: 0.8757 - val_mse: 0.0874\n",
      "Epoch 170/200\n",
      "4952/4952 [==============================] - 4s 725us/step - loss: 0.3051 - accuracy: 0.9218 - mse: 0.0561 - val_loss: 0.3207 - val_accuracy: 0.8700 - val_mse: 0.0991\n",
      "Epoch 171/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.2970 - accuracy: 0.9239 - mse: 0.0552 - val_loss: 0.2897 - val_accuracy: 0.8714 - val_mse: 0.0904\n",
      "Epoch 172/200\n",
      "4952/4952 [==============================] - 4s 713us/step - loss: 0.2968 - accuracy: 0.9271 - mse: 0.0529 - val_loss: 0.3318 - val_accuracy: 0.8686 - val_mse: 0.1022\n",
      "Epoch 173/200\n",
      "4952/4952 [==============================] - 4s 715us/step - loss: 0.2949 - accuracy: 0.9206 - mse: 0.0557 - val_loss: 0.2584 - val_accuracy: 0.8843 - val_mse: 0.0816\n",
      "Epoch 174/200\n",
      "4952/4952 [==============================] - 4s 720us/step - loss: 0.2885 - accuracy: 0.9293 - mse: 0.0517 - val_loss: 0.2981 - val_accuracy: 0.8714 - val_mse: 0.0925\n",
      "Epoch 175/200\n",
      "4952/4952 [==============================] - 3s 707us/step - loss: 0.2953 - accuracy: 0.9275 - mse: 0.0532 - val_loss: 0.2917 - val_accuracy: 0.8714 - val_mse: 0.0910\n",
      "Epoch 176/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.2989 - accuracy: 0.9267 - mse: 0.0544 - val_loss: 0.2720 - val_accuracy: 0.8814 - val_mse: 0.0852\n",
      "Epoch 177/200\n",
      "4952/4952 [==============================] - 4s 712us/step - loss: 0.2826 - accuracy: 0.9265 - mse: 0.0524 - val_loss: 0.2783 - val_accuracy: 0.8757 - val_mse: 0.0871\n",
      "Epoch 178/200\n",
      "4952/4952 [==============================] - 4s 713us/step - loss: 0.2920 - accuracy: 0.9261 - mse: 0.0531 - val_loss: 0.2656 - val_accuracy: 0.8829 - val_mse: 0.0834\n",
      "Epoch 179/200\n",
      "4952/4952 [==============================] - 4s 711us/step - loss: 0.2865 - accuracy: 0.9265 - mse: 0.0536 - val_loss: 0.2548 - val_accuracy: 0.8857 - val_mse: 0.0804\n",
      "Epoch 180/200\n",
      "4952/4952 [==============================] - 4s 712us/step - loss: 0.2922 - accuracy: 0.9257 - mse: 0.0527 - val_loss: 0.2431 - val_accuracy: 0.8943 - val_mse: 0.0769\n",
      "Epoch 181/200\n",
      "4952/4952 [==============================] - 4s 714us/step - loss: 0.2856 - accuracy: 0.9283 - mse: 0.0532 - val_loss: 0.2943 - val_accuracy: 0.8729 - val_mse: 0.0912\n",
      "Epoch 182/200\n",
      "4952/4952 [==============================] - 4s 718us/step - loss: 0.2801 - accuracy: 0.9279 - mse: 0.0519 - val_loss: 0.2644 - val_accuracy: 0.8829 - val_mse: 0.0829\n",
      "Epoch 183/200\n",
      "4952/4952 [==============================] - 4s 712us/step - loss: 0.2742 - accuracy: 0.9311 - mse: 0.0493 - val_loss: 0.2669 - val_accuracy: 0.8829 - val_mse: 0.0835\n",
      "Epoch 184/200\n",
      "4952/4952 [==============================] - 4s 734us/step - loss: 0.2869 - accuracy: 0.9291 - mse: 0.0517 - val_loss: 0.2572 - val_accuracy: 0.8857 - val_mse: 0.0809\n",
      "Epoch 185/200\n",
      "4952/4952 [==============================] - 4s 720us/step - loss: 0.2832 - accuracy: 0.9277 - mse: 0.0524 - val_loss: 0.2519 - val_accuracy: 0.8886 - val_mse: 0.0793\n",
      "Epoch 186/200\n",
      "4952/4952 [==============================] - 4s 715us/step - loss: 0.2766 - accuracy: 0.9303 - mse: 0.0507 - val_loss: 0.2698 - val_accuracy: 0.8829 - val_mse: 0.0843\n",
      "Epoch 187/200\n",
      "4952/4952 [==============================] - 4s 723us/step - loss: 0.2785 - accuracy: 0.9285 - mse: 0.0512 - val_loss: 0.2938 - val_accuracy: 0.8714 - val_mse: 0.0911\n",
      "Epoch 188/200\n",
      "4952/4952 [==============================] - 4s 716us/step - loss: 0.2833 - accuracy: 0.9344 - mse: 0.0491 - val_loss: 0.2929 - val_accuracy: 0.8714 - val_mse: 0.0908\n",
      "Epoch 189/200\n",
      "4952/4952 [==============================] - 4s 707us/step - loss: 0.2785 - accuracy: 0.9303 - mse: 0.0507 - val_loss: 0.2941 - val_accuracy: 0.8729 - val_mse: 0.0910\n",
      "Epoch 190/200\n",
      "4952/4952 [==============================] - 4s 717us/step - loss: 0.2742 - accuracy: 0.9293 - mse: 0.0499 - val_loss: 0.2474 - val_accuracy: 0.8914 - val_mse: 0.0779\n",
      "Epoch 191/200\n",
      "4952/4952 [==============================] - 4s 716us/step - loss: 0.2693 - accuracy: 0.9305 - mse: 0.0497 - val_loss: 0.2546 - val_accuracy: 0.8843 - val_mse: 0.0800\n",
      "Epoch 192/200\n",
      "4952/4952 [==============================] - 4s 721us/step - loss: 0.2740 - accuracy: 0.9330 - mse: 0.0491 - val_loss: 0.2617 - val_accuracy: 0.8843 - val_mse: 0.0820\n",
      "Epoch 193/200\n",
      "4952/4952 [==============================] - 4s 734us/step - loss: 0.2669 - accuracy: 0.9342 - mse: 0.0484 - val_loss: 0.3004 - val_accuracy: 0.8729 - val_mse: 0.0924\n",
      "Epoch 194/200\n",
      "4952/4952 [==============================] - 4s 718us/step - loss: 0.2728 - accuracy: 0.9303 - mse: 0.0502 - val_loss: 0.2767 - val_accuracy: 0.8814 - val_mse: 0.0857\n",
      "Epoch 195/200\n",
      "4952/4952 [==============================] - 4s 717us/step - loss: 0.2644 - accuracy: 0.9330 - mse: 0.0485 - val_loss: 0.2645 - val_accuracy: 0.8857 - val_mse: 0.0822\n",
      "Epoch 196/200\n",
      "4952/4952 [==============================] - 4s 715us/step - loss: 0.2697 - accuracy: 0.9305 - mse: 0.0486 - val_loss: 0.2390 - val_accuracy: 0.8943 - val_mse: 0.0752\n",
      "Epoch 197/200\n",
      "4952/4952 [==============================] - 4s 716us/step - loss: 0.2611 - accuracy: 0.9332 - mse: 0.0481 - val_loss: 0.2246 - val_accuracy: 0.8971 - val_mse: 0.0710\n",
      "Epoch 198/200\n",
      "4952/4952 [==============================] - 4s 711us/step - loss: 0.2709 - accuracy: 0.9317 - mse: 0.0480 - val_loss: 0.2481 - val_accuracy: 0.8929 - val_mse: 0.0777\n",
      "Epoch 199/200\n",
      "4952/4952 [==============================] - 4s 721us/step - loss: 0.2676 - accuracy: 0.9360 - mse: 0.0478 - val_loss: 0.2889 - val_accuracy: 0.8757 - val_mse: 0.0893\n",
      "Epoch 200/200\n",
      "4952/4952 [==============================] - 4s 715us/step - loss: 0.2620 - accuracy: 0.9346 - mse: 0.0473 - val_loss: 0.2935 - val_accuracy: 0.8743 - val_mse: 0.0908\n",
      "430/430 [==============================] - 0s 351us/step\n",
      "Final loss = 0.343265703874965\n",
      "Final accuracy = 0.8581395149230957\n",
      "Left out subject = aggie\n",
      "Test subjects = ['adela', 'andrius', 'diana', 'jack', 'joao', 'lukasz', 'nikita', 'rim', 'santi', 'sharan', 'teo', 'zoe']\n",
      "Validation subjects = ['ron', 'seb']\n",
      "--------------------------------------------------------------------------------\n",
      "Removing outliers\n",
      "--------------------------------------------------------------------------------\n",
      "Original dataframe length = 118022\n",
      "New dataframe length = 115798\n",
      "Removed 2224 outliers\n",
      "Original dataframe length = 8445\n",
      "New dataframe length = 8266\n",
      "Removed 179 outliers\n",
      "--------------------------------------------------------------------------------\n",
      "Standardising\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Normalising\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generating datasets\n",
      "--------------------------------------------------------------------------------\n",
      "Total samples for true activity = 4755\n",
      "Total samples for false activity = 0\n",
      "Training set shapes: X_train = (4755, 38, 3), y_train = (4755, 2)\n",
      "Total samples for true activity = 917\n",
      "Total samples for false activity = 0\n",
      "Valid set shapes: X_valid = (917, 38, 3), y_valid = (917, 2)\n",
      "Total samples for true activity = 406\n",
      "Total samples for false activity = 0\n",
      "Test set shapes: X_test = (406, 38, 3), y_test = (406, 2)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 36, 64)            640       \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 34, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 34, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 32, 64)            12352     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               204900    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 230,702\n",
      "Trainable params: 230,574\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4755 samples, validate on 917 samples\n",
      "Epoch 1/200\n",
      "4755/4755 [==============================] - 6s 1ms/step - loss: 2.2312 - accuracy: 0.1737 - mse: 0.4708 - val_loss: 0.7579 - val_accuracy: 0.1341 - val_mse: 0.2823\n",
      "Epoch 2/200\n",
      "4755/4755 [==============================] - 4s 743us/step - loss: 1.9278 - accuracy: 0.1659 - mse: 0.5221 - val_loss: 0.7859 - val_accuracy: 0.1341 - val_mse: 0.2960\n",
      "Epoch 3/200\n",
      "4755/4755 [==============================] - 3s 734us/step - loss: 1.8478 - accuracy: 0.1672 - mse: 0.5096 - val_loss: 0.8764 - val_accuracy: 0.1363 - val_mse: 0.3391\n",
      "Epoch 4/200\n",
      "4755/4755 [==============================] - 3s 735us/step - loss: 1.7810 - accuracy: 0.1695 - mse: 0.4935 - val_loss: 1.0252 - val_accuracy: 0.1439 - val_mse: 0.4021\n",
      "Epoch 5/200\n",
      "4755/4755 [==============================] - 3s 732us/step - loss: 1.7244 - accuracy: 0.1773 - mse: 0.4741 - val_loss: 1.1632 - val_accuracy: 0.1516 - val_mse: 0.4498\n",
      "Epoch 6/200\n",
      "4755/4755 [==============================] - 3s 731us/step - loss: 1.6748 - accuracy: 0.1886 - mse: 0.4576 - val_loss: 1.2130 - val_accuracy: 0.1614 - val_mse: 0.4618\n",
      "Epoch 7/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 1.6264 - accuracy: 0.2019 - mse: 0.4485 - val_loss: 1.1974 - val_accuracy: 0.1723 - val_mse: 0.4526\n",
      "Epoch 8/200\n",
      "4755/4755 [==============================] - 3s 734us/step - loss: 1.5843 - accuracy: 0.2299 - mse: 0.4317 - val_loss: 1.1436 - val_accuracy: 0.2323 - val_mse: 0.4302\n",
      "Epoch 9/200\n",
      "4755/4755 [==============================] - 3s 732us/step - loss: 1.5449 - accuracy: 0.2604 - mse: 0.4175 - val_loss: 1.1462 - val_accuracy: 0.2563 - val_mse: 0.4279\n",
      "Epoch 10/200\n",
      "4755/4755 [==============================] - 4s 737us/step - loss: 1.5108 - accuracy: 0.2915 - mse: 0.4046 - val_loss: 1.1230 - val_accuracy: 0.2803 - val_mse: 0.4167\n",
      "Epoch 11/200\n",
      "4755/4755 [==============================] - 3s 736us/step - loss: 1.4733 - accuracy: 0.3186 - mse: 0.3945 - val_loss: 1.0796 - val_accuracy: 0.3250 - val_mse: 0.3985\n",
      "Epoch 12/200\n",
      "4755/4755 [==============================] - 4s 765us/step - loss: 1.4451 - accuracy: 0.3516 - mse: 0.3819 - val_loss: 1.0577 - val_accuracy: 0.3533 - val_mse: 0.3877\n",
      "Epoch 13/200\n",
      "4755/4755 [==============================] - 4s 745us/step - loss: 1.4171 - accuracy: 0.3857 - mse: 0.3700 - val_loss: 1.0434 - val_accuracy: 0.3730 - val_mse: 0.3803\n",
      "Epoch 14/200\n",
      "4755/4755 [==============================] - 3s 734us/step - loss: 1.3883 - accuracy: 0.4120 - mse: 0.3607 - val_loss: 1.0313 - val_accuracy: 0.3893 - val_mse: 0.3738\n",
      "Epoch 15/200\n",
      "4755/4755 [==============================] - 4s 756us/step - loss: 1.3631 - accuracy: 0.4290 - mse: 0.3539 - val_loss: 0.9766 - val_accuracy: 0.4482 - val_mse: 0.3524\n",
      "Epoch 16/200\n",
      "4755/4755 [==============================] - 3s 733us/step - loss: 1.3375 - accuracy: 0.4597 - mse: 0.3420 - val_loss: 0.9622 - val_accuracy: 0.4689 - val_mse: 0.3450\n",
      "Epoch 17/200\n",
      "4755/4755 [==============================] - 4s 737us/step - loss: 1.3103 - accuracy: 0.4835 - mse: 0.3320 - val_loss: 0.9542 - val_accuracy: 0.4842 - val_mse: 0.3404\n",
      "Epoch 18/200\n",
      "4755/4755 [==============================] - 3s 735us/step - loss: 1.2884 - accuracy: 0.4991 - mse: 0.3254 - val_loss: 0.9304 - val_accuracy: 0.5016 - val_mse: 0.3304\n",
      "Epoch 19/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 1.2627 - accuracy: 0.5169 - mse: 0.3167 - val_loss: 0.9318 - val_accuracy: 0.5005 - val_mse: 0.3297\n",
      "Epoch 20/200\n",
      "4755/4755 [==============================] - 4s 742us/step - loss: 1.2422 - accuracy: 0.5333 - mse: 0.3091 - val_loss: 0.9110 - val_accuracy: 0.5289 - val_mse: 0.3208\n",
      "Epoch 21/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 1.2190 - accuracy: 0.5510 - mse: 0.3034 - val_loss: 0.9017 - val_accuracy: 0.5409 - val_mse: 0.3161\n",
      "Epoch 22/200\n",
      "4755/4755 [==============================] - 3s 732us/step - loss: 1.1966 - accuracy: 0.5701 - mse: 0.2925 - val_loss: 0.9052 - val_accuracy: 0.5442 - val_mse: 0.3158\n",
      "Epoch 23/200\n",
      "4755/4755 [==============================] - 3s 734us/step - loss: 1.1751 - accuracy: 0.5821 - mse: 0.2867 - val_loss: 0.8705 - val_accuracy: 0.5671 - val_mse: 0.3027\n",
      "Epoch 24/200\n",
      "4755/4755 [==============================] - 3s 734us/step - loss: 1.1639 - accuracy: 0.5922 - mse: 0.2818 - val_loss: 0.8837 - val_accuracy: 0.5551 - val_mse: 0.3063\n",
      "Epoch 25/200\n",
      "4755/4755 [==============================] - 4s 737us/step - loss: 1.1387 - accuracy: 0.6063 - mse: 0.2727 - val_loss: 0.8319 - val_accuracy: 0.5889 - val_mse: 0.2863\n",
      "Epoch 26/200\n",
      "4755/4755 [==============================] - 3s 732us/step - loss: 1.1226 - accuracy: 0.6172 - mse: 0.2683 - val_loss: 0.8129 - val_accuracy: 0.5954 - val_mse: 0.2788\n",
      "Epoch 27/200\n",
      "4755/4755 [==============================] - 4s 742us/step - loss: 1.1032 - accuracy: 0.6320 - mse: 0.2597 - val_loss: 0.8113 - val_accuracy: 0.5987 - val_mse: 0.2774\n",
      "Epoch 28/200\n",
      "4755/4755 [==============================] - 4s 737us/step - loss: 1.0817 - accuracy: 0.6383 - mse: 0.2540 - val_loss: 0.8031 - val_accuracy: 0.6052 - val_mse: 0.2735\n",
      "Epoch 29/200\n",
      "4755/4755 [==============================] - 4s 759us/step - loss: 1.0664 - accuracy: 0.6437 - mse: 0.2514 - val_loss: 0.7613 - val_accuracy: 0.6260 - val_mse: 0.2581\n",
      "Epoch 30/200\n",
      "4755/4755 [==============================] - 4s 745us/step - loss: 1.0494 - accuracy: 0.6566 - mse: 0.2440 - val_loss: 0.7610 - val_accuracy: 0.6336 - val_mse: 0.2572\n",
      "Epoch 31/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 1.0330 - accuracy: 0.6662 - mse: 0.2355 - val_loss: 0.7630 - val_accuracy: 0.6336 - val_mse: 0.2570\n",
      "Epoch 32/200\n",
      "4755/4755 [==============================] - 4s 760us/step - loss: 1.0166 - accuracy: 0.6721 - mse: 0.2332 - val_loss: 0.7489 - val_accuracy: 0.6412 - val_mse: 0.2513\n",
      "Epoch 33/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 1.0005 - accuracy: 0.6810 - mse: 0.2278 - val_loss: 0.7053 - val_accuracy: 0.6565 - val_mse: 0.2354\n",
      "Epoch 34/200\n",
      "4755/4755 [==============================] - 4s 736us/step - loss: 0.9870 - accuracy: 0.6902 - mse: 0.2205 - val_loss: 0.7080 - val_accuracy: 0.6609 - val_mse: 0.2350\n",
      "Epoch 35/200\n",
      "4755/4755 [==============================] - 3s 733us/step - loss: 0.9708 - accuracy: 0.6934 - mse: 0.2179 - val_loss: 0.6939 - val_accuracy: 0.6685 - val_mse: 0.2294\n",
      "Epoch 36/200\n",
      "4755/4755 [==============================] - 4s 737us/step - loss: 0.9540 - accuracy: 0.7009 - mse: 0.2125 - val_loss: 0.6694 - val_accuracy: 0.6728 - val_mse: 0.2201\n",
      "Epoch 37/200\n",
      "4755/4755 [==============================] - 4s 744us/step - loss: 0.9445 - accuracy: 0.7062 - mse: 0.2075 - val_loss: 0.7236 - val_accuracy: 0.6619 - val_mse: 0.2381\n",
      "Epoch 38/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 0.9284 - accuracy: 0.7136 - mse: 0.2032 - val_loss: 0.6786 - val_accuracy: 0.6728 - val_mse: 0.2221\n",
      "Epoch 39/200\n",
      "4755/4755 [==============================] - 3s 731us/step - loss: 0.9133 - accuracy: 0.7146 - mse: 0.2023 - val_loss: 0.6422 - val_accuracy: 0.6914 - val_mse: 0.2092\n",
      "Epoch 40/200\n",
      "4755/4755 [==============================] - 4s 743us/step - loss: 0.9012 - accuracy: 0.7270 - mse: 0.1940 - val_loss: 0.6327 - val_accuracy: 0.6979 - val_mse: 0.2048\n",
      "Epoch 41/200\n",
      "4755/4755 [==============================] - 4s 741us/step - loss: 0.8845 - accuracy: 0.7304 - mse: 0.1909 - val_loss: 0.6475 - val_accuracy: 0.6881 - val_mse: 0.2092\n",
      "Epoch 42/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 0.8787 - accuracy: 0.7304 - mse: 0.1900 - val_loss: 0.6167 - val_accuracy: 0.7023 - val_mse: 0.1985\n",
      "Epoch 43/200\n",
      "4755/4755 [==============================] - 3s 734us/step - loss: 0.8668 - accuracy: 0.7394 - mse: 0.1847 - val_loss: 0.5964 - val_accuracy: 0.7165 - val_mse: 0.1909\n",
      "Epoch 44/200\n",
      "4755/4755 [==============================] - 4s 743us/step - loss: 0.8534 - accuracy: 0.7478 - mse: 0.1808 - val_loss: 0.5962 - val_accuracy: 0.7154 - val_mse: 0.1905\n",
      "Epoch 45/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 0.8464 - accuracy: 0.7506 - mse: 0.1787 - val_loss: 0.6023 - val_accuracy: 0.7132 - val_mse: 0.1923\n",
      "Epoch 46/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4755/4755 [==============================] - 4s 754us/step - loss: 0.8284 - accuracy: 0.7575 - mse: 0.1736 - val_loss: 0.5949 - val_accuracy: 0.7252 - val_mse: 0.1892\n",
      "Epoch 47/200\n",
      "4755/4755 [==============================] - 4s 744us/step - loss: 0.8227 - accuracy: 0.7647 - mse: 0.1696 - val_loss: 0.5988 - val_accuracy: 0.7252 - val_mse: 0.1903\n",
      "Epoch 48/200\n",
      "4755/4755 [==============================] - 3s 726us/step - loss: 0.8089 - accuracy: 0.7695 - mse: 0.1665 - val_loss: 0.6159 - val_accuracy: 0.7186 - val_mse: 0.1955\n",
      "Epoch 49/200\n",
      "4755/4755 [==============================] - 4s 751us/step - loss: 0.8008 - accuracy: 0.7718 - mse: 0.1655 - val_loss: 0.5362 - val_accuracy: 0.7655 - val_mse: 0.1681\n",
      "Epoch 50/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 0.7822 - accuracy: 0.7746 - mse: 0.1627 - val_loss: 0.5397 - val_accuracy: 0.7666 - val_mse: 0.1691\n",
      "Epoch 51/200\n",
      "4755/4755 [==============================] - 4s 748us/step - loss: 0.7855 - accuracy: 0.7815 - mse: 0.1595 - val_loss: 0.5713 - val_accuracy: 0.7492 - val_mse: 0.1788\n",
      "Epoch 52/200\n",
      "4755/4755 [==============================] - 4s 743us/step - loss: 0.7703 - accuracy: 0.7840 - mse: 0.1581 - val_loss: 0.5576 - val_accuracy: 0.7568 - val_mse: 0.1744\n",
      "Epoch 53/200\n",
      "4755/4755 [==============================] - 3s 734us/step - loss: 0.7651 - accuracy: 0.7897 - mse: 0.1557 - val_loss: 0.5523 - val_accuracy: 0.7601 - val_mse: 0.1722\n",
      "Epoch 54/200\n",
      "4755/4755 [==============================] - 3s 735us/step - loss: 0.7589 - accuracy: 0.7939 - mse: 0.1526 - val_loss: 0.5487 - val_accuracy: 0.7655 - val_mse: 0.1707\n",
      "Epoch 55/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 0.7395 - accuracy: 0.7960 - mse: 0.1493 - val_loss: 0.5219 - val_accuracy: 0.7819 - val_mse: 0.1616\n",
      "Epoch 56/200\n",
      "4755/4755 [==============================] - 3s 728us/step - loss: 0.7379 - accuracy: 0.8015 - mse: 0.1493 - val_loss: 0.5123 - val_accuracy: 0.7884 - val_mse: 0.1586\n",
      "Epoch 57/200\n",
      "4755/4755 [==============================] - 4s 736us/step - loss: 0.7372 - accuracy: 0.8017 - mse: 0.1464 - val_loss: 0.4894 - val_accuracy: 0.8004 - val_mse: 0.1505\n",
      "Epoch 58/200\n",
      "4755/4755 [==============================] - 3s 733us/step - loss: 0.7233 - accuracy: 0.8074 - mse: 0.1446 - val_loss: 0.4912 - val_accuracy: 0.7983 - val_mse: 0.1507\n",
      "Epoch 59/200\n",
      "4755/4755 [==============================] - 4s 736us/step - loss: 0.7132 - accuracy: 0.8126 - mse: 0.1398 - val_loss: 0.5231 - val_accuracy: 0.7797 - val_mse: 0.1611\n",
      "Epoch 60/200\n",
      "4755/4755 [==============================] - 3s 729us/step - loss: 0.7103 - accuracy: 0.8097 - mse: 0.1415 - val_loss: 0.4964 - val_accuracy: 0.7983 - val_mse: 0.1521\n",
      "Epoch 61/200\n",
      "4755/4755 [==============================] - 4s 742us/step - loss: 0.7031 - accuracy: 0.8145 - mse: 0.1400 - val_loss: 0.4740 - val_accuracy: 0.8103 - val_mse: 0.1447\n",
      "Epoch 62/200\n",
      "4755/4755 [==============================] - 4s 738us/step - loss: 0.6876 - accuracy: 0.8202 - mse: 0.1352 - val_loss: 0.4944 - val_accuracy: 0.8004 - val_mse: 0.1513\n",
      "Epoch 63/200\n",
      "4755/4755 [==============================] - 4s 754us/step - loss: 0.6838 - accuracy: 0.8189 - mse: 0.1356 - val_loss: 0.4667 - val_accuracy: 0.8124 - val_mse: 0.1421\n",
      "Epoch 64/200\n",
      "4755/4755 [==============================] - 4s 742us/step - loss: 0.6875 - accuracy: 0.8282 - mse: 0.1322 - val_loss: 0.4994 - val_accuracy: 0.7961 - val_mse: 0.1525\n",
      "Epoch 65/200\n",
      "4755/4755 [==============================] - 3s 733us/step - loss: 0.6802 - accuracy: 0.8269 - mse: 0.1334 - val_loss: 0.4707 - val_accuracy: 0.8124 - val_mse: 0.1429\n",
      "Epoch 66/200\n",
      "4755/4755 [==============================] - 4s 753us/step - loss: 0.6725 - accuracy: 0.8284 - mse: 0.1311 - val_loss: 0.4712 - val_accuracy: 0.8135 - val_mse: 0.1430\n",
      "Epoch 67/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 0.6620 - accuracy: 0.8322 - mse: 0.1289 - val_loss: 0.4529 - val_accuracy: 0.8222 - val_mse: 0.1368\n",
      "Epoch 68/200\n",
      "4755/4755 [==============================] - 4s 738us/step - loss: 0.6584 - accuracy: 0.8305 - mse: 0.1277 - val_loss: 0.4531 - val_accuracy: 0.8233 - val_mse: 0.1367\n",
      "Epoch 69/200\n",
      "4755/4755 [==============================] - 4s 761us/step - loss: 0.6528 - accuracy: 0.8341 - mse: 0.1253 - val_loss: 0.4613 - val_accuracy: 0.8190 - val_mse: 0.1391\n",
      "Epoch 70/200\n",
      "4755/4755 [==============================] - 3s 736us/step - loss: 0.6447 - accuracy: 0.8414 - mse: 0.1246 - val_loss: 0.4480 - val_accuracy: 0.8266 - val_mse: 0.1350\n",
      "Epoch 71/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 0.6380 - accuracy: 0.8385 - mse: 0.1230 - val_loss: 0.5036 - val_accuracy: 0.7939 - val_mse: 0.1528\n",
      "Epoch 72/200\n",
      "4755/4755 [==============================] - 4s 737us/step - loss: 0.6331 - accuracy: 0.8410 - mse: 0.1228 - val_loss: 0.4335 - val_accuracy: 0.8332 - val_mse: 0.1303\n",
      "Epoch 73/200\n",
      "4755/4755 [==============================] - 3s 732us/step - loss: 0.6276 - accuracy: 0.8408 - mse: 0.1216 - val_loss: 0.3968 - val_accuracy: 0.8506 - val_mse: 0.1188\n",
      "Epoch 74/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 0.6188 - accuracy: 0.8469 - mse: 0.1183 - val_loss: 0.4330 - val_accuracy: 0.8364 - val_mse: 0.1299\n",
      "Epoch 75/200\n",
      "4755/4755 [==============================] - 4s 736us/step - loss: 0.6203 - accuracy: 0.8471 - mse: 0.1173 - val_loss: 0.4560 - val_accuracy: 0.8255 - val_mse: 0.1371\n",
      "Epoch 76/200\n",
      "4755/4755 [==============================] - 4s 737us/step - loss: 0.6157 - accuracy: 0.8475 - mse: 0.1183 - val_loss: 0.4102 - val_accuracy: 0.8462 - val_mse: 0.1225\n",
      "Epoch 77/200\n",
      "4755/4755 [==============================] - 4s 743us/step - loss: 0.6149 - accuracy: 0.8486 - mse: 0.1167 - val_loss: 0.3920 - val_accuracy: 0.8506 - val_mse: 0.1168\n",
      "Epoch 78/200\n",
      "4755/4755 [==============================] - 4s 745us/step - loss: 0.6039 - accuracy: 0.8511 - mse: 0.1156 - val_loss: 0.4451 - val_accuracy: 0.8310 - val_mse: 0.1330\n",
      "Epoch 79/200\n",
      "4755/4755 [==============================] - 4s 738us/step - loss: 0.5918 - accuracy: 0.8532 - mse: 0.1126 - val_loss: 0.4405 - val_accuracy: 0.8299 - val_mse: 0.1315\n",
      "Epoch 80/200\n",
      "4755/4755 [==============================] - 4s 758us/step - loss: 0.5965 - accuracy: 0.8568 - mse: 0.1124 - val_loss: 0.4627 - val_accuracy: 0.8212 - val_mse: 0.1386\n",
      "Epoch 81/200\n",
      "4755/4755 [==============================] - 4s 753us/step - loss: 0.5918 - accuracy: 0.8534 - mse: 0.1134 - val_loss: 0.4030 - val_accuracy: 0.8430 - val_mse: 0.1201\n",
      "Epoch 82/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 0.5855 - accuracy: 0.8557 - mse: 0.1120 - val_loss: 0.4224 - val_accuracy: 0.8353 - val_mse: 0.1262\n",
      "Epoch 83/200\n",
      "4755/4755 [==============================] - 4s 757us/step - loss: 0.5864 - accuracy: 0.8583 - mse: 0.1091 - val_loss: 0.4389 - val_accuracy: 0.8321 - val_mse: 0.1309\n",
      "Epoch 84/200\n",
      "4755/4755 [==============================] - 4s 941us/step - loss: 0.5827 - accuracy: 0.8591 - mse: 0.1092 - val_loss: 0.4761 - val_accuracy: 0.8157 - val_mse: 0.1422\n",
      "Epoch 85/200\n",
      "4755/4755 [==============================] - 4s 868us/step - loss: 0.5717 - accuracy: 0.8566 - mse: 0.1113 - val_loss: 0.4067 - val_accuracy: 0.8441 - val_mse: 0.1207\n",
      "Epoch 86/200\n",
      "4755/4755 [==============================] - 4s 876us/step - loss: 0.5660 - accuracy: 0.8641 - mse: 0.1052 - val_loss: 0.4065 - val_accuracy: 0.8430 - val_mse: 0.1204\n",
      "Epoch 87/200\n",
      "4755/4755 [==============================] - 5s 1ms/step - loss: 0.5745 - accuracy: 0.8559 - mse: 0.1094 - val_loss: 0.4092 - val_accuracy: 0.8441 - val_mse: 0.1210\n",
      "Epoch 88/200\n",
      "4755/4755 [==============================] - 5s 1ms/step - loss: 0.5666 - accuracy: 0.8627 - mse: 0.1060 - val_loss: 0.4676 - val_accuracy: 0.8212 - val_mse: 0.1389\n",
      "Epoch 89/200\n",
      "4755/4755 [==============================] - 4s 775us/step - loss: 0.5624 - accuracy: 0.8608 - mse: 0.1075 - val_loss: 0.3937 - val_accuracy: 0.8473 - val_mse: 0.1162\n",
      "Epoch 90/200\n",
      "4755/4755 [==============================] - 4s 765us/step - loss: 0.5449 - accuracy: 0.8675 - mse: 0.1034 - val_loss: 0.3438 - val_accuracy: 0.8680 - val_mse: 0.1010\n",
      "Epoch 91/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4755/4755 [==============================] - 4s 737us/step - loss: 0.5564 - accuracy: 0.8683 - mse: 0.1029 - val_loss: 0.4159 - val_accuracy: 0.8408 - val_mse: 0.1228\n",
      "Epoch 92/200\n",
      "4755/4755 [==============================] - 4s 736us/step - loss: 0.5455 - accuracy: 0.8650 - mse: 0.1037 - val_loss: 0.4286 - val_accuracy: 0.8386 - val_mse: 0.1268\n",
      "Epoch 93/200\n",
      "4755/4755 [==============================] - 4s 738us/step - loss: 0.5462 - accuracy: 0.8709 - mse: 0.1016 - val_loss: 0.4164 - val_accuracy: 0.8419 - val_mse: 0.1227\n",
      "Epoch 94/200\n",
      "4755/4755 [==============================] - 4s 737us/step - loss: 0.5434 - accuracy: 0.8698 - mse: 0.1015 - val_loss: 0.3859 - val_accuracy: 0.8517 - val_mse: 0.1132\n",
      "Epoch 95/200\n",
      "4755/4755 [==============================] - 3s 736us/step - loss: 0.5340 - accuracy: 0.8730 - mse: 0.0991 - val_loss: 0.4074 - val_accuracy: 0.8430 - val_mse: 0.1198\n",
      "Epoch 96/200\n",
      "4755/4755 [==============================] - 4s 763us/step - loss: 0.5312 - accuracy: 0.8709 - mse: 0.1008 - val_loss: 0.3559 - val_accuracy: 0.8637 - val_mse: 0.1042\n",
      "Epoch 97/200\n",
      "4755/4755 [==============================] - 3s 735us/step - loss: 0.5208 - accuracy: 0.8736 - mse: 0.0983 - val_loss: 0.3787 - val_accuracy: 0.8539 - val_mse: 0.1110\n",
      "Epoch 98/200\n",
      "4755/4755 [==============================] - 3s 736us/step - loss: 0.5198 - accuracy: 0.8791 - mse: 0.0958 - val_loss: 0.3961 - val_accuracy: 0.8462 - val_mse: 0.1162\n",
      "Epoch 99/200\n",
      "4755/4755 [==============================] - 4s 761us/step - loss: 0.5130 - accuracy: 0.8770 - mse: 0.0973 - val_loss: 0.3687 - val_accuracy: 0.8604 - val_mse: 0.1078\n",
      "Epoch 100/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 0.5183 - accuracy: 0.8799 - mse: 0.0949 - val_loss: 0.3832 - val_accuracy: 0.8506 - val_mse: 0.1118\n",
      "Epoch 101/200\n",
      "4755/4755 [==============================] - 4s 751us/step - loss: 0.5166 - accuracy: 0.8776 - mse: 0.0964 - val_loss: 0.3891 - val_accuracy: 0.8506 - val_mse: 0.1137\n",
      "Epoch 102/200\n",
      "4755/4755 [==============================] - 4s 738us/step - loss: 0.5141 - accuracy: 0.8755 - mse: 0.0971 - val_loss: 0.3775 - val_accuracy: 0.8561 - val_mse: 0.1101\n",
      "Epoch 103/200\n",
      "4755/4755 [==============================] - 3s 733us/step - loss: 0.5090 - accuracy: 0.8772 - mse: 0.0947 - val_loss: 0.3723 - val_accuracy: 0.8604 - val_mse: 0.1083\n",
      "Epoch 104/200\n",
      "4755/4755 [==============================] - 3s 736us/step - loss: 0.4965 - accuracy: 0.8772 - mse: 0.0949 - val_loss: 0.3322 - val_accuracy: 0.8757 - val_mse: 0.0963\n",
      "Epoch 105/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 0.4876 - accuracy: 0.8814 - mse: 0.0906 - val_loss: 0.3421 - val_accuracy: 0.8757 - val_mse: 0.0990\n",
      "Epoch 106/200\n",
      "4755/4755 [==============================] - 3s 732us/step - loss: 0.4903 - accuracy: 0.8801 - mse: 0.0919 - val_loss: 0.3305 - val_accuracy: 0.8779 - val_mse: 0.0956\n",
      "Epoch 107/200\n",
      "4755/4755 [==============================] - 4s 742us/step - loss: 0.4852 - accuracy: 0.8841 - mse: 0.0903 - val_loss: 0.3229 - val_accuracy: 0.8790 - val_mse: 0.0935\n",
      "Epoch 108/200\n",
      "4755/4755 [==============================] - 3s 735us/step - loss: 0.4888 - accuracy: 0.8841 - mse: 0.0904 - val_loss: 0.3664 - val_accuracy: 0.8615 - val_mse: 0.1062\n",
      "Epoch 109/200\n",
      "4755/4755 [==============================] - 3s 731us/step - loss: 0.4888 - accuracy: 0.8803 - mse: 0.0908 - val_loss: 0.3653 - val_accuracy: 0.8615 - val_mse: 0.1055\n",
      "Epoch 110/200\n",
      "4755/4755 [==============================] - 4s 745us/step - loss: 0.4850 - accuracy: 0.8843 - mse: 0.0883 - val_loss: 0.3744 - val_accuracy: 0.8582 - val_mse: 0.1082\n",
      "Epoch 111/200\n",
      "4755/4755 [==============================] - 4s 738us/step - loss: 0.4748 - accuracy: 0.8803 - mse: 0.0913 - val_loss: 0.3167 - val_accuracy: 0.8790 - val_mse: 0.0910\n",
      "Epoch 112/200\n",
      "4755/4755 [==============================] - 4s 736us/step - loss: 0.4829 - accuracy: 0.8848 - mse: 0.0883 - val_loss: 0.3391 - val_accuracy: 0.8735 - val_mse: 0.0973\n",
      "Epoch 113/200\n",
      "4755/4755 [==============================] - 4s 759us/step - loss: 0.4828 - accuracy: 0.8873 - mse: 0.0884 - val_loss: 0.3818 - val_accuracy: 0.8593 - val_mse: 0.1098\n",
      "Epoch 114/200\n",
      "4755/4755 [==============================] - 4s 741us/step - loss: 0.4742 - accuracy: 0.8887 - mse: 0.0880 - val_loss: 0.3751 - val_accuracy: 0.8593 - val_mse: 0.1077\n",
      "Epoch 115/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 0.4667 - accuracy: 0.8854 - mse: 0.0879 - val_loss: 0.3224 - val_accuracy: 0.8822 - val_mse: 0.0923\n",
      "Epoch 116/200\n",
      "4755/4755 [==============================] - 4s 761us/step - loss: 0.4678 - accuracy: 0.8845 - mse: 0.0867 - val_loss: 0.3290 - val_accuracy: 0.8800 - val_mse: 0.0942\n",
      "Epoch 117/200\n",
      "4755/4755 [==============================] - 4s 746us/step - loss: 0.4673 - accuracy: 0.8898 - mse: 0.0856 - val_loss: 0.3426 - val_accuracy: 0.8757 - val_mse: 0.0979\n",
      "Epoch 118/200\n",
      "4755/4755 [==============================] - 3s 735us/step - loss: 0.4558 - accuracy: 0.8917 - mse: 0.0857 - val_loss: 0.2986 - val_accuracy: 0.8920 - val_mse: 0.0853\n",
      "Epoch 119/200\n",
      "4755/4755 [==============================] - 4s 742us/step - loss: 0.4598 - accuracy: 0.8923 - mse: 0.0821 - val_loss: 0.3613 - val_accuracy: 0.8637 - val_mse: 0.1031\n",
      "Epoch 120/200\n",
      "4755/4755 [==============================] - 3s 735us/step - loss: 0.4633 - accuracy: 0.8873 - mse: 0.0861 - val_loss: 0.3621 - val_accuracy: 0.8691 - val_mse: 0.1034\n",
      "Epoch 121/200\n",
      "4755/4755 [==============================] - 4s 738us/step - loss: 0.4461 - accuracy: 0.8911 - mse: 0.0839 - val_loss: 0.3404 - val_accuracy: 0.8746 - val_mse: 0.0971\n",
      "Epoch 122/200\n",
      "4755/4755 [==============================] - 3s 732us/step - loss: 0.4528 - accuracy: 0.8892 - mse: 0.0835 - val_loss: 0.3217 - val_accuracy: 0.8866 - val_mse: 0.0915\n",
      "Epoch 123/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 0.4479 - accuracy: 0.8923 - mse: 0.0833 - val_loss: 0.3274 - val_accuracy: 0.8833 - val_mse: 0.0930\n",
      "Epoch 124/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 0.4426 - accuracy: 0.8892 - mse: 0.0832 - val_loss: 0.2960 - val_accuracy: 0.8964 - val_mse: 0.0841\n",
      "Epoch 125/200\n",
      "4755/4755 [==============================] - 3s 735us/step - loss: 0.4405 - accuracy: 0.8951 - mse: 0.0789 - val_loss: 0.3052 - val_accuracy: 0.8920 - val_mse: 0.0864\n",
      "Epoch 126/200\n",
      "4755/4755 [==============================] - 4s 736us/step - loss: 0.4379 - accuracy: 0.8890 - mse: 0.0822 - val_loss: 0.2893 - val_accuracy: 0.8975 - val_mse: 0.0817\n",
      "Epoch 127/200\n",
      "4755/4755 [==============================] - 4s 747us/step - loss: 0.4337 - accuracy: 0.8988 - mse: 0.0776 - val_loss: 0.3258 - val_accuracy: 0.8866 - val_mse: 0.0918\n",
      "Epoch 128/200\n",
      "4755/4755 [==============================] - 4s 736us/step - loss: 0.4334 - accuracy: 0.8936 - mse: 0.0802 - val_loss: 0.3546 - val_accuracy: 0.8746 - val_mse: 0.1005\n",
      "Epoch 129/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 0.4277 - accuracy: 0.8963 - mse: 0.0790 - val_loss: 0.3479 - val_accuracy: 0.8757 - val_mse: 0.0983\n",
      "Epoch 130/200\n",
      "4755/4755 [==============================] - 4s 754us/step - loss: 0.4273 - accuracy: 0.8932 - mse: 0.0800 - val_loss: 0.3281 - val_accuracy: 0.8877 - val_mse: 0.0926\n",
      "Epoch 131/200\n",
      "4755/4755 [==============================] - 4s 752us/step - loss: 0.4351 - accuracy: 0.8993 - mse: 0.0790 - val_loss: 0.2974 - val_accuracy: 0.8964 - val_mse: 0.0835\n",
      "Epoch 132/200\n",
      "4755/4755 [==============================] - 4s 742us/step - loss: 0.4276 - accuracy: 0.8976 - mse: 0.0783 - val_loss: 0.2686 - val_accuracy: 0.9051 - val_mse: 0.0753\n",
      "Epoch 133/200\n",
      "4755/4755 [==============================] - 4s 756us/step - loss: 0.4254 - accuracy: 0.8978 - mse: 0.0781 - val_loss: 0.3441 - val_accuracy: 0.8779 - val_mse: 0.0969\n",
      "Epoch 134/200\n",
      "4755/4755 [==============================] - 4s 747us/step - loss: 0.4285 - accuracy: 0.8940 - mse: 0.0803 - val_loss: 0.3187 - val_accuracy: 0.8909 - val_mse: 0.0895\n",
      "Epoch 135/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 0.4200 - accuracy: 0.8984 - mse: 0.0773 - val_loss: 0.3035 - val_accuracy: 0.8953 - val_mse: 0.0850\n",
      "Epoch 136/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4755/4755 [==============================] - 4s 739us/step - loss: 0.4150 - accuracy: 0.9014 - mse: 0.0758 - val_loss: 0.2898 - val_accuracy: 0.8986 - val_mse: 0.0810\n",
      "Epoch 137/200\n",
      "4755/4755 [==============================] - 3s 735us/step - loss: 0.4180 - accuracy: 0.8984 - mse: 0.0771 - val_loss: 0.3149 - val_accuracy: 0.8931 - val_mse: 0.0880\n",
      "Epoch 138/200\n",
      "4755/4755 [==============================] - 4s 741us/step - loss: 0.4103 - accuracy: 0.9022 - mse: 0.0751 - val_loss: 0.3199 - val_accuracy: 0.8877 - val_mse: 0.0891\n",
      "Epoch 139/200\n",
      "4755/4755 [==============================] - 3s 734us/step - loss: 0.4165 - accuracy: 0.8967 - mse: 0.0762 - val_loss: 0.3121 - val_accuracy: 0.8942 - val_mse: 0.0868\n",
      "Epoch 140/200\n",
      "4755/4755 [==============================] - 4s 745us/step - loss: 0.4086 - accuracy: 0.8995 - mse: 0.0756 - val_loss: 0.2858 - val_accuracy: 0.9019 - val_mse: 0.0795\n",
      "Epoch 141/200\n",
      "4755/4755 [==============================] - 4s 737us/step - loss: 0.4025 - accuracy: 0.9045 - mse: 0.0738 - val_loss: 0.3140 - val_accuracy: 0.8931 - val_mse: 0.0873\n",
      "Epoch 142/200\n",
      "4755/4755 [==============================] - 4s 737us/step - loss: 0.4001 - accuracy: 0.9043 - mse: 0.0738 - val_loss: 0.2957 - val_accuracy: 0.9019 - val_mse: 0.0821\n",
      "Epoch 143/200\n",
      "4755/4755 [==============================] - 3s 734us/step - loss: 0.3972 - accuracy: 0.9073 - mse: 0.0707 - val_loss: 0.3227 - val_accuracy: 0.8888 - val_mse: 0.0894\n",
      "Epoch 144/200\n",
      "4755/4755 [==============================] - 4s 742us/step - loss: 0.4042 - accuracy: 0.9016 - mse: 0.0754 - val_loss: 0.2944 - val_accuracy: 0.8997 - val_mse: 0.0817\n",
      "Epoch 145/200\n",
      "4755/4755 [==============================] - 3s 736us/step - loss: 0.3901 - accuracy: 0.9075 - mse: 0.0705 - val_loss: 0.3003 - val_accuracy: 0.9008 - val_mse: 0.0831\n",
      "Epoch 146/200\n",
      "4755/4755 [==============================] - 3s 735us/step - loss: 0.3942 - accuracy: 0.9049 - mse: 0.0715 - val_loss: 0.2968 - val_accuracy: 0.8997 - val_mse: 0.0823\n",
      "Epoch 147/200\n",
      "4755/4755 [==============================] - 4s 758us/step - loss: 0.3944 - accuracy: 0.9068 - mse: 0.0717 - val_loss: 0.3665 - val_accuracy: 0.8713 - val_mse: 0.1019\n",
      "Epoch 148/200\n",
      "4755/4755 [==============================] - 5s 1ms/step - loss: 0.3892 - accuracy: 0.9043 - mse: 0.0720 - val_loss: 0.2818 - val_accuracy: 0.9029 - val_mse: 0.0779\n",
      "Epoch 149/200\n",
      "4755/4755 [==============================] - 4s 782us/step - loss: 0.3802 - accuracy: 0.9083 - mse: 0.0689 - val_loss: 0.2959 - val_accuracy: 0.9019 - val_mse: 0.0817\n",
      "Epoch 150/200\n",
      "4755/4755 [==============================] - 4s 882us/step - loss: 0.3882 - accuracy: 0.9058 - mse: 0.0717 - val_loss: 0.2809 - val_accuracy: 0.9040 - val_mse: 0.0774\n",
      "Epoch 151/200\n",
      "4755/4755 [==============================] - 4s 820us/step - loss: 0.3992 - accuracy: 0.9047 - mse: 0.0725 - val_loss: 0.2805 - val_accuracy: 0.9051 - val_mse: 0.0774\n",
      "Epoch 152/200\n",
      "4755/4755 [==============================] - 4s 793us/step - loss: 0.3866 - accuracy: 0.9077 - mse: 0.0704 - val_loss: 0.2891 - val_accuracy: 0.9029 - val_mse: 0.0795\n",
      "Epoch 153/200\n",
      "4755/4755 [==============================] - 4s 790us/step - loss: 0.3841 - accuracy: 0.9064 - mse: 0.0693 - val_loss: 0.2800 - val_accuracy: 0.9073 - val_mse: 0.0767\n",
      "Epoch 154/200\n",
      "4755/4755 [==============================] - 4s 748us/step - loss: 0.3781 - accuracy: 0.9068 - mse: 0.0698 - val_loss: 0.2709 - val_accuracy: 0.9073 - val_mse: 0.0744\n",
      "Epoch 155/200\n",
      "4755/4755 [==============================] - 4s 756us/step - loss: 0.3748 - accuracy: 0.9129 - mse: 0.0676 - val_loss: 0.3303 - val_accuracy: 0.8899 - val_mse: 0.0908\n",
      "Epoch 156/200\n",
      "4755/4755 [==============================] - 4s 745us/step - loss: 0.3727 - accuracy: 0.9102 - mse: 0.0680 - val_loss: 0.2526 - val_accuracy: 0.9117 - val_mse: 0.0694\n",
      "Epoch 157/200\n",
      "4755/4755 [==============================] - 4s 738us/step - loss: 0.3754 - accuracy: 0.9100 - mse: 0.0678 - val_loss: 0.2766 - val_accuracy: 0.9062 - val_mse: 0.0758\n",
      "Epoch 158/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 0.3673 - accuracy: 0.9102 - mse: 0.0671 - val_loss: 0.2809 - val_accuracy: 0.9062 - val_mse: 0.0771\n",
      "Epoch 159/200\n",
      "4755/4755 [==============================] - 3s 728us/step - loss: 0.3618 - accuracy: 0.9110 - mse: 0.0665 - val_loss: 0.2818 - val_accuracy: 0.9062 - val_mse: 0.0773\n",
      "Epoch 160/200\n",
      "4755/4755 [==============================] - 4s 750us/step - loss: 0.3611 - accuracy: 0.9127 - mse: 0.0648 - val_loss: 0.3006 - val_accuracy: 0.9062 - val_mse: 0.0822\n",
      "Epoch 161/200\n",
      "4755/4755 [==============================] - 4s 740us/step - loss: 0.3684 - accuracy: 0.9119 - mse: 0.0658 - val_loss: 0.2780 - val_accuracy: 0.9062 - val_mse: 0.0759\n",
      "Epoch 162/200\n",
      "4755/4755 [==============================] - 4s 744us/step - loss: 0.3593 - accuracy: 0.9138 - mse: 0.0660 - val_loss: 0.2855 - val_accuracy: 0.9062 - val_mse: 0.0780\n",
      "Epoch 163/200\n",
      "4755/4755 [==============================] - 4s 760us/step - loss: 0.3606 - accuracy: 0.9138 - mse: 0.0651 - val_loss: 0.3076 - val_accuracy: 0.9040 - val_mse: 0.0834\n",
      "Epoch 164/200\n",
      "4755/4755 [==============================] - 4s 751us/step - loss: 0.3657 - accuracy: 0.9113 - mse: 0.0667 - val_loss: 0.2559 - val_accuracy: 0.9138 - val_mse: 0.0698\n",
      "Epoch 165/200\n",
      "4755/4755 [==============================] - 4s 742us/step - loss: 0.3584 - accuracy: 0.9144 - mse: 0.0647 - val_loss: 0.2998 - val_accuracy: 0.9040 - val_mse: 0.0816\n",
      "Epoch 166/200\n",
      "4755/4755 [==============================] - 4s 757us/step - loss: 0.3526 - accuracy: 0.9169 - mse: 0.0640 - val_loss: 0.2905 - val_accuracy: 0.9062 - val_mse: 0.0792\n",
      "Epoch 167/200\n",
      "4755/4755 [==============================] - 4s 747us/step - loss: 0.3472 - accuracy: 0.9195 - mse: 0.0624 - val_loss: 0.2739 - val_accuracy: 0.9084 - val_mse: 0.0744\n",
      "Epoch 168/200\n",
      "4755/4755 [==============================] - 4s 738us/step - loss: 0.3627 - accuracy: 0.9117 - mse: 0.0663 - val_loss: 0.2892 - val_accuracy: 0.9062 - val_mse: 0.0785\n",
      "Epoch 169/200\n",
      "4755/4755 [==============================] - 4s 743us/step - loss: 0.3465 - accuracy: 0.9169 - mse: 0.0629 - val_loss: 0.2651 - val_accuracy: 0.9106 - val_mse: 0.0721\n",
      "Epoch 170/200\n",
      "4755/4755 [==============================] - 3s 730us/step - loss: 0.3485 - accuracy: 0.9182 - mse: 0.0625 - val_loss: 0.2864 - val_accuracy: 0.9062 - val_mse: 0.0776\n",
      "Epoch 171/200\n",
      "4755/4755 [==============================] - 3s 733us/step - loss: 0.3523 - accuracy: 0.9165 - mse: 0.0628 - val_loss: 0.3005 - val_accuracy: 0.9040 - val_mse: 0.0812\n",
      "Epoch 172/200\n",
      "4755/4755 [==============================] - 3s 732us/step - loss: 0.3470 - accuracy: 0.9178 - mse: 0.0629 - val_loss: 0.3117 - val_accuracy: 0.9040 - val_mse: 0.0841\n",
      "Epoch 173/200\n",
      "4755/4755 [==============================] - 4s 738us/step - loss: 0.3445 - accuracy: 0.9171 - mse: 0.0626 - val_loss: 0.2674 - val_accuracy: 0.9095 - val_mse: 0.0724\n",
      "Epoch 174/200\n",
      "4755/4755 [==============================] - 4s 745us/step - loss: 0.3454 - accuracy: 0.9157 - mse: 0.0630 - val_loss: 0.2611 - val_accuracy: 0.9138 - val_mse: 0.0706\n",
      "Epoch 175/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 0.3342 - accuracy: 0.9211 - mse: 0.0601 - val_loss: 0.2415 - val_accuracy: 0.9160 - val_mse: 0.0655\n",
      "Epoch 176/200\n",
      "4755/4755 [==============================] - 4s 737us/step - loss: 0.3381 - accuracy: 0.9209 - mse: 0.0605 - val_loss: 0.2537 - val_accuracy: 0.9138 - val_mse: 0.0686\n",
      "Epoch 177/200\n",
      "4755/4755 [==============================] - 4s 751us/step - loss: 0.3349 - accuracy: 0.9195 - mse: 0.0608 - val_loss: 0.2421 - val_accuracy: 0.9171 - val_mse: 0.0655\n",
      "Epoch 178/200\n",
      "4755/4755 [==============================] - 4s 746us/step - loss: 0.3342 - accuracy: 0.9205 - mse: 0.0592 - val_loss: 0.3095 - val_accuracy: 0.9040 - val_mse: 0.0834\n",
      "Epoch 179/200\n",
      "4755/4755 [==============================] - 4s 749us/step - loss: 0.3361 - accuracy: 0.9216 - mse: 0.0598 - val_loss: 0.2944 - val_accuracy: 0.9073 - val_mse: 0.0792\n",
      "Epoch 180/200\n",
      "4755/4755 [==============================] - 4s 764us/step - loss: 0.3342 - accuracy: 0.9211 - mse: 0.0616 - val_loss: 0.2573 - val_accuracy: 0.9138 - val_mse: 0.0697\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4755/4755 [==============================] - 4s 745us/step - loss: 0.3389 - accuracy: 0.9195 - mse: 0.0609 - val_loss: 0.2629 - val_accuracy: 0.9128 - val_mse: 0.0710\n",
      "Epoch 182/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 0.3289 - accuracy: 0.9226 - mse: 0.0591 - val_loss: 0.2207 - val_accuracy: 0.9226 - val_mse: 0.0601\n",
      "Epoch 183/200\n",
      "4755/4755 [==============================] - 4s 753us/step - loss: 0.3356 - accuracy: 0.9207 - mse: 0.0602 - val_loss: 0.2327 - val_accuracy: 0.9193 - val_mse: 0.0630\n",
      "Epoch 184/200\n",
      "4755/4755 [==============================] - 4s 739us/step - loss: 0.3265 - accuracy: 0.9222 - mse: 0.0590 - val_loss: 0.2529 - val_accuracy: 0.9171 - val_mse: 0.0679\n",
      "Epoch 185/200\n",
      "4755/4755 [==============================] - 3s 728us/step - loss: 0.3294 - accuracy: 0.9230 - mse: 0.0589 - val_loss: 0.2720 - val_accuracy: 0.9138 - val_mse: 0.0728\n",
      "Epoch 186/200\n",
      "4755/4755 [==============================] - 4s 736us/step - loss: 0.3296 - accuracy: 0.9226 - mse: 0.0585 - val_loss: 0.2754 - val_accuracy: 0.9128 - val_mse: 0.0737\n",
      "Epoch 187/200\n",
      "4755/4755 [==============================] - 3s 730us/step - loss: 0.3250 - accuracy: 0.9245 - mse: 0.0580 - val_loss: 0.2798 - val_accuracy: 0.9106 - val_mse: 0.0747\n",
      "Epoch 188/200\n",
      "4755/4755 [==============================] - 3s 733us/step - loss: 0.3210 - accuracy: 0.9224 - mse: 0.0591 - val_loss: 0.2634 - val_accuracy: 0.9138 - val_mse: 0.0708\n",
      "Epoch 189/200\n",
      "4755/4755 [==============================] - 3s 730us/step - loss: 0.3234 - accuracy: 0.9253 - mse: 0.0576 - val_loss: 0.2539 - val_accuracy: 0.9138 - val_mse: 0.0682\n",
      "Epoch 190/200\n",
      "4755/4755 [==============================] - 3s 736us/step - loss: 0.3100 - accuracy: 0.9274 - mse: 0.0558 - val_loss: 0.2520 - val_accuracy: 0.9149 - val_mse: 0.0677\n",
      "Epoch 191/200\n",
      "4755/4755 [==============================] - 4s 738us/step - loss: 0.3124 - accuracy: 0.9272 - mse: 0.0564 - val_loss: 0.2532 - val_accuracy: 0.9138 - val_mse: 0.0679\n",
      "Epoch 192/200\n",
      "4755/4755 [==============================] - 3s 733us/step - loss: 0.3100 - accuracy: 0.9277 - mse: 0.0550 - val_loss: 0.2650 - val_accuracy: 0.9138 - val_mse: 0.0707\n",
      "Epoch 193/200\n",
      "4755/4755 [==============================] - 3s 722us/step - loss: 0.3150 - accuracy: 0.9251 - mse: 0.0569 - val_loss: 0.2526 - val_accuracy: 0.9160 - val_mse: 0.0676\n",
      "Epoch 194/200\n",
      "4755/4755 [==============================] - 4s 741us/step - loss: 0.3167 - accuracy: 0.9256 - mse: 0.0566 - val_loss: 0.2583 - val_accuracy: 0.9138 - val_mse: 0.0692\n",
      "Epoch 195/200\n",
      "4755/4755 [==============================] - 3s 732us/step - loss: 0.3142 - accuracy: 0.9279 - mse: 0.0548 - val_loss: 0.2780 - val_accuracy: 0.9138 - val_mse: 0.0741\n",
      "Epoch 196/200\n",
      "4755/4755 [==============================] - 3s 731us/step - loss: 0.3112 - accuracy: 0.9249 - mse: 0.0563 - val_loss: 0.2466 - val_accuracy: 0.9160 - val_mse: 0.0661\n",
      "Epoch 197/200\n",
      "4755/4755 [==============================] - 4s 737us/step - loss: 0.3124 - accuracy: 0.9277 - mse: 0.0566 - val_loss: 0.2386 - val_accuracy: 0.9193 - val_mse: 0.0642\n",
      "Epoch 198/200\n",
      "4755/4755 [==============================] - 4s 745us/step - loss: 0.3021 - accuracy: 0.9277 - mse: 0.0545 - val_loss: 0.2641 - val_accuracy: 0.9128 - val_mse: 0.0708\n",
      "Epoch 199/200\n",
      "4755/4755 [==============================] - 4s 738us/step - loss: 0.3179 - accuracy: 0.9256 - mse: 0.0558 - val_loss: 0.2494 - val_accuracy: 0.9149 - val_mse: 0.0666\n",
      "Epoch 200/200\n",
      "4755/4755 [==============================] - 4s 752us/step - loss: 0.3113 - accuracy: 0.9239 - mse: 0.0575 - val_loss: 0.2638 - val_accuracy: 0.9128 - val_mse: 0.0704\n",
      "406/406 [==============================] - 0s 327us/step\n",
      "Final loss = 0.4565197465932134\n",
      "Final accuracy = 0.8374384045600891\n",
      "Left out subject = andrius\n",
      "Test subjects = ['adela', 'aggie', 'jack', 'joao', 'lukasz', 'nikita', 'ron', 'santi', 'seb', 'sharan', 'teo', 'zoe']\n",
      "Validation subjects = ['rim', 'diana']\n",
      "--------------------------------------------------------------------------------\n",
      "Removing outliers\n",
      "--------------------------------------------------------------------------------\n",
      "Original dataframe length = 116617\n",
      "New dataframe length = 114235\n",
      "Removed 2382 outliers\n",
      "Original dataframe length = 9850\n",
      "New dataframe length = 9629\n",
      "Removed 221 outliers\n",
      "--------------------------------------------------------------------------------\n",
      "Standardising\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Normalising\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generating datasets\n",
      "--------------------------------------------------------------------------------\n",
      "Total samples for true activity = 4673\n",
      "Total samples for false activity = 0\n",
      "Training set shapes: X_train = (4673, 38, 3), y_train = (4673, 2)\n",
      "Total samples for true activity = 919\n",
      "Total samples for false activity = 0\n",
      "Valid set shapes: X_valid = (919, 38, 3), y_valid = (919, 2)\n",
      "Total samples for true activity = 475\n",
      "Total samples for false activity = 0\n",
      "Test set shapes: X_test = (475, 38, 3), y_test = (475, 2)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 36, 64)            640       \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 34, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 34, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 32, 64)            12352     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               204900    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 230,702\n",
      "Trainable params: 230,574\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Train on 4673 samples, validate on 919 samples\n",
      "Epoch 1/200\n",
      "4673/4673 [==============================] - 6s 1ms/step - loss: 2.7726 - accuracy: 0.2208 - mse: 0.3962 - val_loss: 0.7350 - val_accuracy: 0.1447 - val_mse: 0.2709\n",
      "Epoch 2/200\n",
      "4673/4673 [==============================] - 3s 740us/step - loss: 2.0915 - accuracy: 0.1609 - mse: 0.5031 - val_loss: 0.7808 - val_accuracy: 0.1447 - val_mse: 0.2935\n",
      "Epoch 3/200\n",
      "4673/4673 [==============================] - 3s 736us/step - loss: 1.9742 - accuracy: 0.1599 - mse: 0.5263 - val_loss: 0.8729 - val_accuracy: 0.1447 - val_mse: 0.3373\n",
      "Epoch 4/200\n",
      "4673/4673 [==============================] - 3s 739us/step - loss: 1.8995 - accuracy: 0.1605 - mse: 0.5141 - val_loss: 1.0173 - val_accuracy: 0.1469 - val_mse: 0.3992\n",
      "Epoch 5/200\n",
      "4673/4673 [==============================] - 3s 746us/step - loss: 1.8361 - accuracy: 0.1609 - mse: 0.5025 - val_loss: 1.1447 - val_accuracy: 0.1469 - val_mse: 0.4454\n",
      "Epoch 6/200\n",
      "4673/4673 [==============================] - 3s 740us/step - loss: 1.7754 - accuracy: 0.1614 - mse: 0.4870 - val_loss: 1.1839 - val_accuracy: 0.1491 - val_mse: 0.4561\n",
      "Epoch 7/200\n",
      "4673/4673 [==============================] - 3s 745us/step - loss: 1.7238 - accuracy: 0.1643 - mse: 0.4723 - val_loss: 1.1401 - val_accuracy: 0.1676 - val_mse: 0.4375\n",
      "Epoch 8/200\n",
      "4673/4673 [==============================] - 4s 754us/step - loss: 1.6687 - accuracy: 0.1697 - mse: 0.4594 - val_loss: 1.1138 - val_accuracy: 0.1828 - val_mse: 0.4256\n",
      "Epoch 9/200\n",
      "4673/4673 [==============================] - 4s 754us/step - loss: 1.6224 - accuracy: 0.1825 - mse: 0.4422 - val_loss: 1.0813 - val_accuracy: 0.2133 - val_mse: 0.4109\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4673/4673 [==============================] - 3s 746us/step - loss: 1.5767 - accuracy: 0.2007 - mse: 0.4297 - val_loss: 1.0622 - val_accuracy: 0.2383 - val_mse: 0.4013\n",
      "Epoch 11/200\n",
      "4673/4673 [==============================] - 4s 750us/step - loss: 1.5374 - accuracy: 0.2309 - mse: 0.4138 - val_loss: 1.0190 - val_accuracy: 0.2731 - val_mse: 0.3823\n",
      "Epoch 12/200\n",
      "4673/4673 [==============================] - 3s 740us/step - loss: 1.4984 - accuracy: 0.2619 - mse: 0.4025 - val_loss: 0.9880 - val_accuracy: 0.3188 - val_mse: 0.3677\n",
      "Epoch 13/200\n",
      "4673/4673 [==============================] - 3s 730us/step - loss: 1.4589 - accuracy: 0.2955 - mse: 0.3880 - val_loss: 0.9719 - val_accuracy: 0.3406 - val_mse: 0.3592\n",
      "Epoch 14/200\n",
      "4673/4673 [==============================] - 3s 739us/step - loss: 1.4263 - accuracy: 0.3263 - mse: 0.3772 - val_loss: 0.9407 - val_accuracy: 0.3808 - val_mse: 0.3443\n",
      "Epoch 15/200\n",
      "4673/4673 [==============================] - 3s 732us/step - loss: 1.3965 - accuracy: 0.3572 - mse: 0.3666 - val_loss: 0.9063 - val_accuracy: 0.4244 - val_mse: 0.3287\n",
      "Epoch 16/200\n",
      "4673/4673 [==============================] - 3s 731us/step - loss: 1.3664 - accuracy: 0.3927 - mse: 0.3537 - val_loss: 0.8919 - val_accuracy: 0.4472 - val_mse: 0.3210\n",
      "Epoch 17/200\n",
      "4673/4673 [==============================] - 3s 736us/step - loss: 1.3358 - accuracy: 0.4169 - mse: 0.3446 - val_loss: 0.8509 - val_accuracy: 0.5027 - val_mse: 0.3029\n",
      "Epoch 18/200\n",
      "4673/4673 [==============================] - 3s 733us/step - loss: 1.3078 - accuracy: 0.4490 - mse: 0.3330 - val_loss: 0.8502 - val_accuracy: 0.5103 - val_mse: 0.3009\n",
      "Epoch 19/200\n",
      "4673/4673 [==============================] - 3s 737us/step - loss: 1.2850 - accuracy: 0.4706 - mse: 0.3234 - val_loss: 0.8342 - val_accuracy: 0.5375 - val_mse: 0.2925\n",
      "Epoch 20/200\n",
      "4673/4673 [==============================] - 3s 729us/step - loss: 1.2569 - accuracy: 0.4924 - mse: 0.3176 - val_loss: 0.7869 - val_accuracy: 0.5789 - val_mse: 0.2724\n",
      "Epoch 21/200\n",
      "4673/4673 [==============================] - 3s 734us/step - loss: 1.2375 - accuracy: 0.5136 - mse: 0.3082 - val_loss: 0.7858 - val_accuracy: 0.5865 - val_mse: 0.2710\n",
      "Epoch 22/200\n",
      "4673/4673 [==============================] - 3s 736us/step - loss: 1.2107 - accuracy: 0.5365 - mse: 0.2987 - val_loss: 0.8409 - val_accuracy: 0.5539 - val_mse: 0.2901\n",
      "Epoch 23/200\n",
      "4673/4673 [==============================] - 3s 737us/step - loss: 1.1922 - accuracy: 0.5380 - mse: 0.3030 - val_loss: 0.7339 - val_accuracy: 0.6398 - val_mse: 0.2485\n",
      "Epoch 24/200\n",
      "4673/4673 [==============================] - 3s 733us/step - loss: 1.1726 - accuracy: 0.5724 - mse: 0.2846 - val_loss: 0.7248 - val_accuracy: 0.6496 - val_mse: 0.2436\n",
      "Epoch 25/200\n",
      "4673/4673 [==============================] - 3s 735us/step - loss: 1.1499 - accuracy: 0.5941 - mse: 0.2722 - val_loss: 0.7207 - val_accuracy: 0.6551 - val_mse: 0.2408\n",
      "Epoch 26/200\n",
      "4673/4673 [==============================] - 4s 763us/step - loss: 1.1291 - accuracy: 0.5955 - mse: 0.2734 - val_loss: 0.7041 - val_accuracy: 0.6692 - val_mse: 0.2335\n",
      "Epoch 27/200\n",
      "4673/4673 [==============================] - 3s 737us/step - loss: 1.1077 - accuracy: 0.6176 - mse: 0.2644 - val_loss: 0.6637 - val_accuracy: 0.6975 - val_mse: 0.2171\n",
      "Epoch 28/200\n",
      "4673/4673 [==============================] - 3s 745us/step - loss: 1.0897 - accuracy: 0.6309 - mse: 0.2579 - val_loss: 0.6654 - val_accuracy: 0.6975 - val_mse: 0.2170\n",
      "Epoch 29/200\n",
      "4673/4673 [==============================] - 4s 750us/step - loss: 1.0747 - accuracy: 0.6480 - mse: 0.2486 - val_loss: 0.6482 - val_accuracy: 0.7106 - val_mse: 0.2098\n",
      "Epoch 30/200\n",
      "4673/4673 [==============================] - 3s 739us/step - loss: 1.0545 - accuracy: 0.6568 - mse: 0.2416 - val_loss: 0.6490 - val_accuracy: 0.7116 - val_mse: 0.2090\n",
      "Epoch 31/200\n",
      "4673/4673 [==============================] - 3s 736us/step - loss: 1.0393 - accuracy: 0.6645 - mse: 0.2384 - val_loss: 0.6361 - val_accuracy: 0.7225 - val_mse: 0.2034\n",
      "Epoch 32/200\n",
      "4673/4673 [==============================] - 3s 738us/step - loss: 1.0184 - accuracy: 0.6752 - mse: 0.2317 - val_loss: 0.6081 - val_accuracy: 0.7323 - val_mse: 0.1923\n",
      "Epoch 33/200\n",
      "4673/4673 [==============================] - 4s 755us/step - loss: 1.0012 - accuracy: 0.6829 - mse: 0.2273 - val_loss: 0.6083 - val_accuracy: 0.7388 - val_mse: 0.1919\n",
      "Epoch 34/200\n",
      "4673/4673 [==============================] - 6s 1ms/step - loss: 0.9862 - accuracy: 0.6938 - mse: 0.2198 - val_loss: 0.5724 - val_accuracy: 0.7573 - val_mse: 0.1783\n",
      "Epoch 35/200\n",
      "4673/4673 [==============================] - 7s 1ms/step - loss: 0.9661 - accuracy: 0.7038 - mse: 0.2147 - val_loss: 0.6023 - val_accuracy: 0.7432 - val_mse: 0.1886\n",
      "Epoch 36/200\n",
      "4673/4673 [==============================] - 6s 1ms/step - loss: 0.9503 - accuracy: 0.7167 - mse: 0.2059 - val_loss: 0.5603 - val_accuracy: 0.7595 - val_mse: 0.1725\n",
      "Epoch 37/200\n",
      "4673/4673 [==============================] - 4s 789us/step - loss: 0.9382 - accuracy: 0.7150 - mse: 0.2059 - val_loss: 0.5260 - val_accuracy: 0.7911 - val_mse: 0.1599\n",
      "Epoch 38/200\n",
      "4673/4673 [==============================] - 5s 975us/step - loss: 0.9254 - accuracy: 0.7246 - mse: 0.2008 - val_loss: 0.5181 - val_accuracy: 0.7943 - val_mse: 0.1568\n",
      "Epoch 39/200\n",
      "4673/4673 [==============================] - 4s 842us/step - loss: 0.9040 - accuracy: 0.7336 - mse: 0.1949 - val_loss: 0.5009 - val_accuracy: 0.8020 - val_mse: 0.1501\n",
      "Epoch 40/200\n",
      "4673/4673 [==============================] - 4s 804us/step - loss: 0.8993 - accuracy: 0.7406 - mse: 0.1903 - val_loss: 0.4835 - val_accuracy: 0.8085 - val_mse: 0.1438\n",
      "Epoch 41/200\n",
      "4673/4673 [==============================] - 4s 907us/step - loss: 0.8851 - accuracy: 0.7492 - mse: 0.1843 - val_loss: 0.5492 - val_accuracy: 0.7682 - val_mse: 0.1667\n",
      "Epoch 42/200\n",
      "4673/4673 [==============================] - 4s 825us/step - loss: 0.8718 - accuracy: 0.7468 - mse: 0.1849 - val_loss: 0.5031 - val_accuracy: 0.8020 - val_mse: 0.1496\n",
      "Epoch 43/200\n",
      "4673/4673 [==============================] - 4s 823us/step - loss: 0.8594 - accuracy: 0.7563 - mse: 0.1792 - val_loss: 0.4700 - val_accuracy: 0.8194 - val_mse: 0.1379\n",
      "Epoch 44/200\n",
      "4673/4673 [==============================] - 4s 962us/step - loss: 0.8495 - accuracy: 0.7612 - mse: 0.1752 - val_loss: 0.5030 - val_accuracy: 0.8030 - val_mse: 0.1491\n",
      "Epoch 45/200\n",
      "4673/4673 [==============================] - 4s 878us/step - loss: 0.8363 - accuracy: 0.7627 - mse: 0.1745 - val_loss: 0.4479 - val_accuracy: 0.8346 - val_mse: 0.1301\n",
      "Epoch 46/200\n",
      "4673/4673 [==============================] - 4s 875us/step - loss: 0.8240 - accuracy: 0.7774 - mse: 0.1668 - val_loss: 0.4435 - val_accuracy: 0.8368 - val_mse: 0.1281\n",
      "Epoch 47/200\n",
      "4673/4673 [==============================] - 5s 1ms/step - loss: 0.8084 - accuracy: 0.7794 - mse: 0.1635 - val_loss: 0.6926 - val_accuracy: 0.7073 - val_mse: 0.2123\n",
      "Epoch 48/200\n",
      "4673/4673 [==============================] - 4s 791us/step - loss: 0.8065 - accuracy: 0.7685 - mse: 0.1710 - val_loss: 0.4161 - val_accuracy: 0.8498 - val_mse: 0.1191\n",
      "Epoch 49/200\n",
      "4673/4673 [==============================] - 4s 908us/step - loss: 0.7881 - accuracy: 0.7909 - mse: 0.1580 - val_loss: 0.4295 - val_accuracy: 0.8411 - val_mse: 0.1234\n",
      "Epoch 50/200\n",
      "4673/4673 [==============================] - 4s 813us/step - loss: 0.7874 - accuracy: 0.7886 - mse: 0.1571 - val_loss: 0.4267 - val_accuracy: 0.8487 - val_mse: 0.1219\n",
      "Epoch 51/200\n",
      "4673/4673 [==============================] - 4s 780us/step - loss: 0.7674 - accuracy: 0.7991 - mse: 0.1506 - val_loss: 0.4460 - val_accuracy: 0.8324 - val_mse: 0.1283\n",
      "Epoch 52/200\n",
      "4673/4673 [==============================] - 4s 921us/step - loss: 0.7612 - accuracy: 0.8003 - mse: 0.1506 - val_loss: 0.4231 - val_accuracy: 0.8455 - val_mse: 0.1207\n",
      "Epoch 53/200\n",
      "4673/4673 [==============================] - 4s 793us/step - loss: 0.7518 - accuracy: 0.8001 - mse: 0.1499 - val_loss: 0.4317 - val_accuracy: 0.8444 - val_mse: 0.1231\n",
      "Epoch 54/200\n",
      "4673/4673 [==============================] - 4s 956us/step - loss: 0.7453 - accuracy: 0.8065 - mse: 0.1468 - val_loss: 0.4456 - val_accuracy: 0.8368 - val_mse: 0.1268\n",
      "Epoch 55/200\n",
      "4673/4673 [==============================] - 5s 968us/step - loss: 0.7403 - accuracy: 0.8040 - mse: 0.1493 - val_loss: 0.4081 - val_accuracy: 0.8531 - val_mse: 0.1147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/200\n",
      "4673/4673 [==============================] - 6s 1ms/step - loss: 0.7336 - accuracy: 0.8119 - mse: 0.1417 - val_loss: 0.4429 - val_accuracy: 0.8346 - val_mse: 0.1262\n",
      "Epoch 57/200\n",
      "4673/4673 [==============================] - 6s 1ms/step - loss: 0.7208 - accuracy: 0.8093 - mse: 0.1429 - val_loss: 0.3845 - val_accuracy: 0.8640 - val_mse: 0.1078\n",
      "Epoch 58/200\n",
      "4673/4673 [==============================] - 6s 1ms/step - loss: 0.7118 - accuracy: 0.8224 - mse: 0.1366 - val_loss: 0.3640 - val_accuracy: 0.8716 - val_mse: 0.1010\n",
      "Epoch 59/200\n",
      "4673/4673 [==============================] - 4s 919us/step - loss: 0.7104 - accuracy: 0.8256 - mse: 0.1344 - val_loss: 0.4014 - val_accuracy: 0.8564 - val_mse: 0.1126\n",
      "Epoch 60/200\n",
      "4673/4673 [==============================] - 4s 842us/step - loss: 0.7019 - accuracy: 0.8177 - mse: 0.1379 - val_loss: 0.4494 - val_accuracy: 0.8303 - val_mse: 0.1285\n",
      "Epoch 61/200\n",
      "4673/4673 [==============================] - 4s 810us/step - loss: 0.6993 - accuracy: 0.8230 - mse: 0.1337 - val_loss: 0.3663 - val_accuracy: 0.8694 - val_mse: 0.1016\n",
      "Epoch 62/200\n",
      "4673/4673 [==============================] - 4s 811us/step - loss: 0.6777 - accuracy: 0.8282 - mse: 0.1313 - val_loss: 0.3737 - val_accuracy: 0.8662 - val_mse: 0.1041\n",
      "Epoch 63/200\n",
      "4673/4673 [==============================] - 6s 1ms/step - loss: 0.6739 - accuracy: 0.8333 - mse: 0.1275 - val_loss: 0.3825 - val_accuracy: 0.8618 - val_mse: 0.1068\n",
      "Epoch 64/200\n",
      "4673/4673 [==============================] - 4s 869us/step - loss: 0.6681 - accuracy: 0.8322 - mse: 0.1269 - val_loss: 0.3848 - val_accuracy: 0.8585 - val_mse: 0.1074\n",
      "Epoch 65/200\n",
      "4673/4673 [==============================] - 4s 774us/step - loss: 0.6739 - accuracy: 0.8359 - mse: 0.1260 - val_loss: 0.3520 - val_accuracy: 0.8727 - val_mse: 0.0971\n",
      "Epoch 66/200\n",
      "4673/4673 [==============================] - 4s 873us/step - loss: 0.6558 - accuracy: 0.8357 - mse: 0.1264 - val_loss: 0.3229 - val_accuracy: 0.8868 - val_mse: 0.0883\n",
      "Epoch 67/200\n",
      "4673/4673 [==============================] - 5s 1ms/step - loss: 0.6619 - accuracy: 0.8406 - mse: 0.1234 - val_loss: 0.4332 - val_accuracy: 0.8390 - val_mse: 0.1227\n",
      "Epoch 68/200\n",
      "4673/4673 [==============================] - 4s 797us/step - loss: 0.6524 - accuracy: 0.8404 - mse: 0.1233 - val_loss: 0.3636 - val_accuracy: 0.8683 - val_mse: 0.1010\n",
      "Epoch 69/200\n",
      "4673/4673 [==============================] - 4s 867us/step - loss: 0.6459 - accuracy: 0.8429 - mse: 0.1205 - val_loss: 0.3693 - val_accuracy: 0.8672 - val_mse: 0.1023\n",
      "Epoch 70/200\n",
      "4673/4673 [==============================] - 4s 870us/step - loss: 0.6314 - accuracy: 0.8478 - mse: 0.1188 - val_loss: 0.3720 - val_accuracy: 0.8662 - val_mse: 0.1033\n",
      "Epoch 71/200\n",
      "4673/4673 [==============================] - 4s 773us/step - loss: 0.6317 - accuracy: 0.8410 - mse: 0.1214 - val_loss: 0.3380 - val_accuracy: 0.8749 - val_mse: 0.0930\n",
      "Epoch 72/200\n",
      "4673/4673 [==============================] - 4s 847us/step - loss: 0.6363 - accuracy: 0.8498 - mse: 0.1184 - val_loss: 0.3261 - val_accuracy: 0.8857 - val_mse: 0.0892\n",
      "Epoch 73/200\n",
      "4673/4673 [==============================] - 4s 833us/step - loss: 0.6202 - accuracy: 0.8511 - mse: 0.1143 - val_loss: 0.3456 - val_accuracy: 0.8716 - val_mse: 0.0950\n",
      "Epoch 74/200\n",
      "4673/4673 [==============================] - 4s 785us/step - loss: 0.6173 - accuracy: 0.8521 - mse: 0.1154 - val_loss: 0.2862 - val_accuracy: 0.9119 - val_mse: 0.0771\n",
      "Epoch 75/200\n",
      "4673/4673 [==============================] - 4s 831us/step - loss: 0.6168 - accuracy: 0.8493 - mse: 0.1146 - val_loss: 0.3363 - val_accuracy: 0.8770 - val_mse: 0.0925\n",
      "Epoch 76/200\n",
      "4673/4673 [==============================] - 4s 809us/step - loss: 0.6100 - accuracy: 0.8568 - mse: 0.1106 - val_loss: 0.3076 - val_accuracy: 0.8977 - val_mse: 0.0838\n",
      "Epoch 77/200\n",
      "4673/4673 [==============================] - 4s 840us/step - loss: 0.6163 - accuracy: 0.8543 - mse: 0.1134 - val_loss: 1.6160 - val_accuracy: 0.4113 - val_mse: 0.4465\n",
      "Epoch 78/200\n",
      "4673/4673 [==============================] - 4s 793us/step - loss: 0.7271 - accuracy: 0.7875 - mse: 0.1574 - val_loss: 0.4110 - val_accuracy: 0.8585 - val_mse: 0.1104\n",
      "Epoch 79/200\n",
      "4673/4673 [==============================] - 4s 770us/step - loss: 0.6571 - accuracy: 0.8337 - mse: 0.1265 - val_loss: 0.7378 - val_accuracy: 0.7106 - val_mse: 0.2132\n",
      "Epoch 80/200\n",
      "4673/4673 [==============================] - 7s 2ms/step - loss: 0.6530 - accuracy: 0.8331 - mse: 0.1272 - val_loss: 1.2331 - val_accuracy: 0.5288 - val_mse: 0.3508\n",
      "Epoch 81/200\n",
      "4673/4673 [==============================] - 6s 1ms/step - loss: 0.7281 - accuracy: 0.7815 - mse: 0.1615 - val_loss: 0.5179 - val_accuracy: 0.8030 - val_mse: 0.1436\n",
      "Epoch 82/200\n",
      "4673/4673 [==============================] - 5s 971us/step - loss: 0.6527 - accuracy: 0.8282 - mse: 0.1299 - val_loss: 0.3190 - val_accuracy: 0.8988 - val_mse: 0.0834\n",
      "Epoch 83/200\n",
      "4673/4673 [==============================] - 6s 1ms/step - loss: 0.6322 - accuracy: 0.8464 - mse: 0.1177 - val_loss: 0.2764 - val_accuracy: 0.9206 - val_mse: 0.0717\n",
      "Epoch 84/200\n",
      "4673/4673 [==============================] - 8s 2ms/step - loss: 0.6110 - accuracy: 0.8496 - mse: 0.1142 - val_loss: 0.2468 - val_accuracy: 0.9271 - val_mse: 0.0640\n",
      "Epoch 85/200\n",
      "4673/4673 [==============================] - 4s 912us/step - loss: 0.5997 - accuracy: 0.8585 - mse: 0.1099 - val_loss: 0.2809 - val_accuracy: 0.9140 - val_mse: 0.0734\n",
      "Epoch 86/200\n",
      "4673/4673 [==============================] - 5s 1ms/step - loss: 0.5926 - accuracy: 0.8551 - mse: 0.1102 - val_loss: 0.3139 - val_accuracy: 0.8955 - val_mse: 0.0835\n",
      "Epoch 87/200\n",
      "4673/4673 [==============================] - 5s 1ms/step - loss: 0.5996 - accuracy: 0.8635 - mse: 0.1071 - val_loss: 1.0677 - val_accuracy: 0.5941 - val_mse: 0.3045\n",
      "Epoch 88/200\n",
      "4673/4673 [==============================] - 5s 1ms/step - loss: 0.6462 - accuracy: 0.8247 - mse: 0.1338 - val_loss: 0.3180 - val_accuracy: 0.8977 - val_mse: 0.0828\n",
      "Epoch 89/200\n",
      "4673/4673 [==============================] - 4s 901us/step - loss: 0.6077 - accuracy: 0.8532 - mse: 0.1130 - val_loss: 0.3107 - val_accuracy: 0.8999 - val_mse: 0.0816\n",
      "Epoch 90/200\n",
      "4673/4673 [==============================] - 5s 970us/step - loss: 0.6005 - accuracy: 0.8549 - mse: 0.1116 - val_loss: 0.4192 - val_accuracy: 0.8433 - val_mse: 0.1159\n",
      "Epoch 91/200\n",
      "4673/4673 [==============================] - 4s 860us/step - loss: 0.5788 - accuracy: 0.8605 - mse: 0.1077 - val_loss: 0.3196 - val_accuracy: 0.8923 - val_mse: 0.0857\n",
      "Epoch 92/200\n",
      "4673/4673 [==============================] - 4s 848us/step - loss: 0.5602 - accuracy: 0.8705 - mse: 0.1024 - val_loss: 0.2879 - val_accuracy: 0.9042 - val_mse: 0.0769\n",
      "Epoch 93/200\n",
      "4673/4673 [==============================] - 4s 888us/step - loss: 0.5626 - accuracy: 0.8665 - mse: 0.1026 - val_loss: 0.3000 - val_accuracy: 0.8977 - val_mse: 0.0810\n",
      "Epoch 94/200\n",
      "4673/4673 [==============================] - 4s 926us/step - loss: 0.5529 - accuracy: 0.8671 - mse: 0.1024 - val_loss: 0.2446 - val_accuracy: 0.9227 - val_mse: 0.0648\n",
      "Epoch 95/200\n",
      "4673/4673 [==============================] - 4s 861us/step - loss: 0.5510 - accuracy: 0.8703 - mse: 0.1006 - val_loss: 0.2786 - val_accuracy: 0.9075 - val_mse: 0.0746\n",
      "Epoch 96/200\n",
      "4673/4673 [==============================] - 5s 1ms/step - loss: 0.5411 - accuracy: 0.8727 - mse: 0.0998 - val_loss: 0.3710 - val_accuracy: 0.8607 - val_mse: 0.1031\n",
      "Epoch 97/200\n",
      "4673/4673 [==============================] - 4s 785us/step - loss: 0.5435 - accuracy: 0.8735 - mse: 0.0983 - val_loss: 0.3219 - val_accuracy: 0.8857 - val_mse: 0.0882\n",
      "Epoch 98/200\n",
      "4673/4673 [==============================] - 5s 974us/step - loss: 0.5276 - accuracy: 0.8767 - mse: 0.0976 - val_loss: 0.2744 - val_accuracy: 0.9108 - val_mse: 0.0741\n",
      "Epoch 99/200\n",
      "4673/4673 [==============================] - 4s 896us/step - loss: 0.5328 - accuracy: 0.8733 - mse: 0.0981 - val_loss: 0.3161 - val_accuracy: 0.8879 - val_mse: 0.0867\n",
      "Epoch 100/200\n",
      "4673/4673 [==============================] - 4s 794us/step - loss: 0.5337 - accuracy: 0.8757 - mse: 0.0967 - val_loss: 0.2342 - val_accuracy: 0.9227 - val_mse: 0.0624\n",
      "Epoch 101/200\n",
      "4673/4673 [==============================] - 4s 837us/step - loss: 0.5198 - accuracy: 0.8791 - mse: 0.0942 - val_loss: 0.2127 - val_accuracy: 0.9325 - val_mse: 0.0568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/200\n",
      "4673/4673 [==============================] - 7s 2ms/step - loss: 0.5226 - accuracy: 0.8761 - mse: 0.0935 - val_loss: 0.4028 - val_accuracy: 0.8444 - val_mse: 0.1140\n",
      "Epoch 103/200\n",
      "4673/4673 [==============================] - 4s 914us/step - loss: 0.5140 - accuracy: 0.8814 - mse: 0.0930 - val_loss: 0.2809 - val_accuracy: 0.9053 - val_mse: 0.0766\n",
      "Epoch 104/200\n",
      "4673/4673 [==============================] - 4s 904us/step - loss: 0.5163 - accuracy: 0.8795 - mse: 0.0927 - val_loss: 0.2452 - val_accuracy: 0.9195 - val_mse: 0.0660\n",
      "Epoch 105/200\n",
      "4673/4673 [==============================] - 3s 747us/step - loss: 0.5128 - accuracy: 0.8791 - mse: 0.0941 - val_loss: 0.3081 - val_accuracy: 0.8912 - val_mse: 0.0851\n",
      "Epoch 106/200\n",
      "4673/4673 [==============================] - 4s 864us/step - loss: 0.5084 - accuracy: 0.8757 - mse: 0.0935 - val_loss: 0.2590 - val_accuracy: 0.9140 - val_mse: 0.0702\n",
      "Epoch 107/200\n",
      "4673/4673 [==============================] - 4s 857us/step - loss: 0.5069 - accuracy: 0.8802 - mse: 0.0915 - val_loss: 0.3268 - val_accuracy: 0.8770 - val_mse: 0.0910\n",
      "Epoch 108/200\n",
      "4673/4673 [==============================] - 5s 969us/step - loss: 0.5040 - accuracy: 0.8834 - mse: 0.0912 - val_loss: 0.2650 - val_accuracy: 0.9086 - val_mse: 0.0725\n",
      "Epoch 109/200\n",
      "4673/4673 [==============================] - 4s 829us/step - loss: 0.4945 - accuracy: 0.8872 - mse: 0.0880 - val_loss: 0.2200 - val_accuracy: 0.9293 - val_mse: 0.0594\n",
      "Epoch 110/200\n",
      "4673/4673 [==============================] - 5s 993us/step - loss: 0.5047 - accuracy: 0.8797 - mse: 0.0930 - val_loss: 0.2123 - val_accuracy: 0.9314 - val_mse: 0.0572\n",
      "Epoch 111/200\n",
      "4673/4673 [==============================] - 4s 888us/step - loss: 0.4969 - accuracy: 0.8849 - mse: 0.0863 - val_loss: 0.3150 - val_accuracy: 0.8836 - val_mse: 0.0873\n",
      "Epoch 112/200\n",
      "4673/4673 [==============================] - 4s 842us/step - loss: 0.4889 - accuracy: 0.8836 - mse: 0.0891 - val_loss: 0.2591 - val_accuracy: 0.9129 - val_mse: 0.0707\n",
      "Epoch 113/200\n",
      "4673/4673 [==============================] - 4s 831us/step - loss: 0.4899 - accuracy: 0.8855 - mse: 0.0868 - val_loss: 0.2359 - val_accuracy: 0.9227 - val_mse: 0.0640\n",
      "Epoch 114/200\n",
      "4673/4673 [==============================] - 4s 940us/step - loss: 0.4874 - accuracy: 0.8868 - mse: 0.0872 - val_loss: 0.2613 - val_accuracy: 0.9075 - val_mse: 0.0715\n",
      "Epoch 115/200\n",
      "4673/4673 [==============================] - 4s 787us/step - loss: 0.4851 - accuracy: 0.8823 - mse: 0.0896 - val_loss: 0.2044 - val_accuracy: 0.9336 - val_mse: 0.0552\n",
      "Epoch 116/200\n",
      "4673/4673 [==============================] - 6s 1ms/step - loss: 0.4960 - accuracy: 0.8844 - mse: 0.0877 - val_loss: 0.2348 - val_accuracy: 0.9217 - val_mse: 0.0639\n",
      "Epoch 117/200\n",
      "4673/4673 [==============================] - 5s 1ms/step - loss: 0.4841 - accuracy: 0.8832 - mse: 0.0882 - val_loss: 0.3345 - val_accuracy: 0.8770 - val_mse: 0.0939\n",
      "Epoch 118/200\n",
      "4673/4673 [==============================] - 5s 1ms/step - loss: 0.4874 - accuracy: 0.8849 - mse: 0.0878 - val_loss: 0.2144 - val_accuracy: 0.9293 - val_mse: 0.0581\n",
      "Epoch 119/200\n",
      "4673/4673 [==============================] - 4s 779us/step - loss: 0.4781 - accuracy: 0.8857 - mse: 0.0873 - val_loss: 0.2767 - val_accuracy: 0.9053 - val_mse: 0.0768\n",
      "Epoch 120/200\n",
      "4673/4673 [==============================] - 3s 744us/step - loss: 0.4790 - accuracy: 0.8889 - mse: 0.0848 - val_loss: 0.2392 - val_accuracy: 0.9217 - val_mse: 0.0653\n",
      "Epoch 121/200\n",
      "4673/4673 [==============================] - 3s 742us/step - loss: 0.4690 - accuracy: 0.8874 - mse: 0.0864 - val_loss: 0.2072 - val_accuracy: 0.9325 - val_mse: 0.0564\n",
      "Epoch 122/200\n",
      "4673/4673 [==============================] - 3s 747us/step - loss: 0.4729 - accuracy: 0.8904 - mse: 0.0829 - val_loss: 0.2340 - val_accuracy: 0.9195 - val_mse: 0.0642\n",
      "Epoch 123/200\n",
      "4673/4673 [==============================] - 4s 767us/step - loss: 0.4727 - accuracy: 0.8896 - mse: 0.0851 - val_loss: 0.4661 - val_accuracy: 0.8226 - val_mse: 0.1320\n",
      "Epoch 124/200\n",
      "4673/4673 [==============================] - 4s 767us/step - loss: 0.4754 - accuracy: 0.8785 - mse: 0.0944 - val_loss: 0.2224 - val_accuracy: 0.9293 - val_mse: 0.0612\n",
      "Epoch 125/200\n",
      "4673/4673 [==============================] - 4s 769us/step - loss: 0.4613 - accuracy: 0.8892 - mse: 0.0840 - val_loss: 0.2137 - val_accuracy: 0.9304 - val_mse: 0.0585\n",
      "Epoch 126/200\n",
      "4673/4673 [==============================] - 4s 764us/step - loss: 0.4690 - accuracy: 0.8877 - mse: 0.0853 - val_loss: 0.2245 - val_accuracy: 0.9227 - val_mse: 0.0617\n",
      "Epoch 127/200\n",
      "4673/4673 [==============================] - 4s 752us/step - loss: 0.4625 - accuracy: 0.8913 - mse: 0.0833 - val_loss: 0.2163 - val_accuracy: 0.9282 - val_mse: 0.0592\n",
      "Epoch 128/200\n",
      "4673/4673 [==============================] - 3s 734us/step - loss: 0.4633 - accuracy: 0.8917 - mse: 0.0826 - val_loss: 0.2638 - val_accuracy: 0.9075 - val_mse: 0.0731\n",
      "Epoch 129/200\n",
      "4673/4673 [==============================] - 3s 746us/step - loss: 0.4495 - accuracy: 0.8904 - mse: 0.0833 - val_loss: 0.1978 - val_accuracy: 0.9336 - val_mse: 0.0540\n",
      "Epoch 130/200\n",
      "4673/4673 [==============================] - 5s 1ms/step - loss: 0.4565 - accuracy: 0.8932 - mse: 0.0806 - val_loss: 0.2386 - val_accuracy: 0.9173 - val_mse: 0.0657\n",
      "Epoch 131/200\n",
      "4673/4673 [==============================] - 4s 767us/step - loss: 0.4527 - accuracy: 0.8939 - mse: 0.0819 - val_loss: 0.2351 - val_accuracy: 0.9195 - val_mse: 0.0647\n",
      "Epoch 132/200\n",
      "4673/4673 [==============================] - 4s 791us/step - loss: 0.4517 - accuracy: 0.8926 - mse: 0.0817 - val_loss: 0.3829 - val_accuracy: 0.8564 - val_mse: 0.1085\n",
      "Epoch 133/200\n",
      "4673/4673 [==============================] - 4s 782us/step - loss: 0.4495 - accuracy: 0.8877 - mse: 0.0869 - val_loss: 0.2006 - val_accuracy: 0.9314 - val_mse: 0.0552\n",
      "Epoch 134/200\n",
      "4673/4673 [==============================] - 4s 848us/step - loss: 0.4511 - accuracy: 0.8906 - mse: 0.0827 - val_loss: 0.1978 - val_accuracy: 0.9325 - val_mse: 0.0542\n",
      "Epoch 135/200\n",
      "4673/4673 [==============================] - 4s 813us/step - loss: 0.4490 - accuracy: 0.8971 - mse: 0.0793 - val_loss: 0.2201 - val_accuracy: 0.9260 - val_mse: 0.0604\n",
      "Epoch 136/200\n",
      "4673/4673 [==============================] - 4s 831us/step - loss: 0.4432 - accuracy: 0.8921 - mse: 0.0803 - val_loss: 0.2402 - val_accuracy: 0.9162 - val_mse: 0.0669\n",
      "Epoch 137/200\n",
      "4673/4673 [==============================] - 4s 846us/step - loss: 0.4344 - accuracy: 0.8964 - mse: 0.0786 - val_loss: 0.1880 - val_accuracy: 0.9358 - val_mse: 0.0513\n",
      "Epoch 138/200\n",
      "4673/4673 [==============================] - 4s 808us/step - loss: 0.4443 - accuracy: 0.8981 - mse: 0.0786 - val_loss: 0.5271 - val_accuracy: 0.8096 - val_mse: 0.1433\n",
      "Epoch 139/200\n",
      "4673/4673 [==============================] - 4s 854us/step - loss: 0.4824 - accuracy: 0.8729 - mse: 0.0978 - val_loss: 0.2450 - val_accuracy: 0.9119 - val_mse: 0.0676\n",
      "Epoch 140/200\n",
      "4673/4673 [==============================] - 4s 830us/step - loss: 0.4477 - accuracy: 0.8894 - mse: 0.0845 - val_loss: 0.1908 - val_accuracy: 0.9325 - val_mse: 0.0524\n",
      "Epoch 141/200\n",
      "4673/4673 [==============================] - 4s 834us/step - loss: 0.4403 - accuracy: 0.8969 - mse: 0.0789 - val_loss: 0.2044 - val_accuracy: 0.9282 - val_mse: 0.0563\n",
      "Epoch 142/200\n",
      "4673/4673 [==============================] - 4s 827us/step - loss: 0.4255 - accuracy: 0.8947 - mse: 0.0792 - val_loss: 0.1926 - val_accuracy: 0.9304 - val_mse: 0.0530\n",
      "Epoch 143/200\n",
      "4673/4673 [==============================] - 4s 831us/step - loss: 0.4378 - accuracy: 0.8958 - mse: 0.0791 - val_loss: 1.5767 - val_accuracy: 0.5103 - val_mse: 0.3969\n",
      "Epoch 144/200\n",
      "4673/4673 [==============================] - 4s 793us/step - loss: 0.5075 - accuracy: 0.8588 - mse: 0.1064 - val_loss: 0.2164 - val_accuracy: 0.9260 - val_mse: 0.0606\n",
      "Epoch 145/200\n",
      "4673/4673 [==============================] - 4s 860us/step - loss: 0.4414 - accuracy: 0.8936 - mse: 0.0814 - val_loss: 0.1705 - val_accuracy: 0.9380 - val_mse: 0.0473\n",
      "Epoch 146/200\n",
      "4673/4673 [==============================] - 4s 852us/step - loss: 0.4412 - accuracy: 0.8934 - mse: 0.0805 - val_loss: 0.2021 - val_accuracy: 0.9271 - val_mse: 0.0560\n",
      "Epoch 147/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4673/4673 [==============================] - 4s 757us/step - loss: 0.4384 - accuracy: 0.8988 - mse: 0.0772 - val_loss: 0.1673 - val_accuracy: 0.9391 - val_mse: 0.0459\n",
      "Epoch 148/200\n",
      "4673/4673 [==============================] - 4s 777us/step - loss: 0.4293 - accuracy: 0.8951 - mse: 0.0792 - val_loss: 0.2204 - val_accuracy: 0.9238 - val_mse: 0.0612\n",
      "Epoch 149/200\n",
      "4673/4673 [==============================] - 4s 753us/step - loss: 0.4258 - accuracy: 0.9039 - mse: 0.0748 - val_loss: 0.5962 - val_accuracy: 0.7878 - val_mse: 0.1621\n",
      "Epoch 150/200\n",
      "4673/4673 [==============================] - 4s 785us/step - loss: 0.4858 - accuracy: 0.8598 - mse: 0.1059 - val_loss: 0.2568 - val_accuracy: 0.9108 - val_mse: 0.0688\n",
      "Epoch 151/200\n",
      "4673/4673 [==============================] - 4s 752us/step - loss: 0.4468 - accuracy: 0.8915 - mse: 0.0818 - val_loss: 0.1967 - val_accuracy: 0.9325 - val_mse: 0.0522\n",
      "Epoch 152/200\n",
      "4673/4673 [==============================] - 4s 766us/step - loss: 0.4452 - accuracy: 0.8939 - mse: 0.0807 - val_loss: 0.1870 - val_accuracy: 0.9347 - val_mse: 0.0496\n",
      "Epoch 153/200\n",
      "4673/4673 [==============================] - 5s 1ms/step - loss: 0.4400 - accuracy: 0.8977 - mse: 0.0776 - val_loss: 0.1982 - val_accuracy: 0.9325 - val_mse: 0.0528\n",
      "Epoch 154/200\n",
      "4673/4673 [==============================] - 4s 860us/step - loss: 0.4177 - accuracy: 0.8966 - mse: 0.0760 - val_loss: 0.2362 - val_accuracy: 0.9195 - val_mse: 0.0634\n",
      "Epoch 155/200\n",
      "4673/4673 [==============================] - 4s 801us/step - loss: 0.4291 - accuracy: 0.8954 - mse: 0.0792 - val_loss: 1.3013 - val_accuracy: 0.5941 - val_mse: 0.3270\n",
      "Epoch 156/200\n",
      "4673/4673 [==============================] - 4s 832us/step - loss: 0.5612 - accuracy: 0.8393 - mse: 0.1202 - val_loss: 0.3725 - val_accuracy: 0.8672 - val_mse: 0.1021\n",
      "Epoch 157/200\n",
      "4673/4673 [==============================] - 4s 790us/step - loss: 0.4586 - accuracy: 0.8885 - mse: 0.0835 - val_loss: 0.4607 - val_accuracy: 0.8303 - val_mse: 0.1261\n",
      "Epoch 158/200\n",
      "4673/4673 [==============================] - 4s 878us/step - loss: 0.4639 - accuracy: 0.8836 - mse: 0.0893 - val_loss: 0.2666 - val_accuracy: 0.9064 - val_mse: 0.0716\n",
      "Epoch 159/200\n",
      "4673/4673 [==============================] - 4s 896us/step - loss: 0.4328 - accuracy: 0.8936 - mse: 0.0808 - val_loss: 0.2771 - val_accuracy: 0.9021 - val_mse: 0.0750\n",
      "Epoch 160/200\n",
      "4673/4673 [==============================] - 5s 964us/step - loss: 0.4302 - accuracy: 0.8986 - mse: 0.0771 - val_loss: 0.2403 - val_accuracy: 0.9184 - val_mse: 0.0649\n",
      "Epoch 161/200\n",
      "4673/4673 [==============================] - 4s 898us/step - loss: 0.4286 - accuracy: 0.9003 - mse: 0.0769 - val_loss: 0.2377 - val_accuracy: 0.9227 - val_mse: 0.0640\n",
      "Epoch 162/200\n",
      "4673/4673 [==============================] - 4s 846us/step - loss: 0.4272 - accuracy: 0.8958 - mse: 0.0775 - val_loss: 0.1974 - val_accuracy: 0.9358 - val_mse: 0.0533\n",
      "Epoch 163/200\n",
      "4673/4673 [==============================] - 4s 899us/step - loss: 0.4135 - accuracy: 0.8999 - mse: 0.0759 - val_loss: 0.2462 - val_accuracy: 0.9140 - val_mse: 0.0671\n",
      "Epoch 164/200\n",
      "4673/4673 [==============================] - 4s 836us/step - loss: 0.4145 - accuracy: 0.9009 - mse: 0.0747 - val_loss: 0.2132 - val_accuracy: 0.9282 - val_mse: 0.0576\n",
      "Epoch 165/200\n",
      "4673/4673 [==============================] - 4s 756us/step - loss: 0.4083 - accuracy: 0.9039 - mse: 0.0718 - val_loss: 0.1952 - val_accuracy: 0.9347 - val_mse: 0.0526\n",
      "Epoch 166/200\n",
      "4673/4673 [==============================] - 4s 761us/step - loss: 0.4026 - accuracy: 0.8992 - mse: 0.0750 - val_loss: 0.2411 - val_accuracy: 0.9140 - val_mse: 0.0659\n",
      "Epoch 167/200\n",
      "4673/4673 [==============================] - 4s 759us/step - loss: 0.4009 - accuracy: 0.9046 - mse: 0.0711 - val_loss: 0.1801 - val_accuracy: 0.9380 - val_mse: 0.0478\n",
      "Epoch 168/200\n",
      "4673/4673 [==============================] - 4s 833us/step - loss: 0.4155 - accuracy: 0.8947 - mse: 0.0799 - val_loss: 0.2162 - val_accuracy: 0.9271 - val_mse: 0.0583\n",
      "Epoch 169/200\n",
      "4673/4673 [==============================] - 4s 768us/step - loss: 0.4022 - accuracy: 0.9035 - mse: 0.0734 - val_loss: 0.2284 - val_accuracy: 0.9206 - val_mse: 0.0623\n",
      "Epoch 170/200\n",
      "4673/4673 [==============================] - 4s 795us/step - loss: 0.4036 - accuracy: 0.9048 - mse: 0.0719 - val_loss: 0.3743 - val_accuracy: 0.8672 - val_mse: 0.1029\n",
      "Epoch 171/200\n",
      "4673/4673 [==============================] - 4s 767us/step - loss: 0.4057 - accuracy: 0.8934 - mse: 0.0805 - val_loss: 0.2467 - val_accuracy: 0.9097 - val_mse: 0.0667\n",
      "Epoch 172/200\n",
      "4673/4673 [==============================] - 4s 834us/step - loss: 0.3998 - accuracy: 0.9052 - mse: 0.0723 - val_loss: 0.1953 - val_accuracy: 0.9358 - val_mse: 0.0521\n",
      "Epoch 173/200\n",
      "4673/4673 [==============================] - 4s 834us/step - loss: 0.3924 - accuracy: 0.9054 - mse: 0.0712 - val_loss: 0.1657 - val_accuracy: 0.9423 - val_mse: 0.0439\n",
      "Epoch 174/200\n",
      "4673/4673 [==============================] - 4s 827us/step - loss: 0.3957 - accuracy: 0.9013 - mse: 0.0746 - val_loss: 0.2292 - val_accuracy: 0.9217 - val_mse: 0.0621\n",
      "Epoch 175/200\n",
      "4673/4673 [==============================] - 4s 786us/step - loss: 0.3926 - accuracy: 0.9065 - mse: 0.0696 - val_loss: 0.2762 - val_accuracy: 0.8988 - val_mse: 0.0764\n",
      "Epoch 176/200\n",
      "4673/4673 [==============================] - 4s 806us/step - loss: 0.3940 - accuracy: 0.9043 - mse: 0.0718 - val_loss: 0.2322 - val_accuracy: 0.9162 - val_mse: 0.0636\n",
      "Epoch 177/200\n",
      "4673/4673 [==============================] - 4s 807us/step - loss: 0.3838 - accuracy: 0.9080 - mse: 0.0689 - val_loss: 1.5807 - val_accuracy: 0.4820 - val_mse: 0.4048\n",
      "Epoch 178/200\n",
      "4673/4673 [==============================] - 4s 826us/step - loss: 0.4253 - accuracy: 0.8838 - mse: 0.0870 - val_loss: 0.1826 - val_accuracy: 0.9402 - val_mse: 0.0473\n",
      "Epoch 179/200\n",
      "4673/4673 [==============================] - 4s 891us/step - loss: 0.4211 - accuracy: 0.8928 - mse: 0.0823 - val_loss: 0.1640 - val_accuracy: 0.9434 - val_mse: 0.0437\n",
      "Epoch 180/200\n",
      "4673/4673 [==============================] - 4s 855us/step - loss: 0.3893 - accuracy: 0.9037 - mse: 0.0728 - val_loss: 0.1537 - val_accuracy: 0.9445 - val_mse: 0.0417\n",
      "Epoch 181/200\n",
      "4673/4673 [==============================] - 4s 873us/step - loss: 0.3931 - accuracy: 0.9086 - mse: 0.0695 - val_loss: 0.1932 - val_accuracy: 0.9336 - val_mse: 0.0514\n",
      "Epoch 182/200\n",
      "4673/4673 [==============================] - 3s 744us/step - loss: 0.3816 - accuracy: 0.9091 - mse: 0.0693 - val_loss: 0.1759 - val_accuracy: 0.9391 - val_mse: 0.0470\n",
      "Epoch 183/200\n",
      "4673/4673 [==============================] - 3s 742us/step - loss: 0.3881 - accuracy: 0.9041 - mse: 0.0700 - val_loss: 0.1861 - val_accuracy: 0.9358 - val_mse: 0.0499\n",
      "Epoch 184/200\n",
      "4673/4673 [==============================] - 4s 795us/step - loss: 0.3782 - accuracy: 0.9082 - mse: 0.0678 - val_loss: 0.1970 - val_accuracy: 0.9325 - val_mse: 0.0532\n",
      "Epoch 185/200\n",
      "4673/4673 [==============================] - 4s 916us/step - loss: 0.3836 - accuracy: 0.9046 - mse: 0.0694 - val_loss: 0.1982 - val_accuracy: 0.9347 - val_mse: 0.0535\n",
      "Epoch 186/200\n",
      "4673/4673 [==============================] - 4s 952us/step - loss: 0.3823 - accuracy: 0.9093 - mse: 0.0673 - val_loss: 0.3090 - val_accuracy: 0.8847 - val_mse: 0.0860\n",
      "Epoch 187/200\n",
      "4673/4673 [==============================] - 4s 861us/step - loss: 0.3800 - accuracy: 0.9073 - mse: 0.0687 - val_loss: 0.2304 - val_accuracy: 0.9162 - val_mse: 0.0631\n",
      "Epoch 188/200\n",
      "4673/4673 [==============================] - 4s 859us/step - loss: 0.3773 - accuracy: 0.9097 - mse: 0.0672 - val_loss: 0.1649 - val_accuracy: 0.9456 - val_mse: 0.0442\n",
      "Epoch 189/200\n",
      "4673/4673 [==============================] - 4s 926us/step - loss: 0.3727 - accuracy: 0.9071 - mse: 0.0679 - val_loss: 0.1785 - val_accuracy: 0.9369 - val_mse: 0.0470\n",
      "Epoch 190/200\n",
      "4673/4673 [==============================] - 4s 825us/step - loss: 0.4531 - accuracy: 0.8718 - mse: 0.0987 - val_loss: 0.2644 - val_accuracy: 0.9064 - val_mse: 0.0714\n",
      "Epoch 191/200\n",
      "4673/4673 [==============================] - 4s 819us/step - loss: 0.3997 - accuracy: 0.8986 - mse: 0.0758 - val_loss: 0.3744 - val_accuracy: 0.8694 - val_mse: 0.1048\n",
      "Epoch 192/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4673/4673 [==============================] - 4s 777us/step - loss: 0.3835 - accuracy: 0.9039 - mse: 0.0707 - val_loss: 0.2709 - val_accuracy: 0.8999 - val_mse: 0.0749\n",
      "Epoch 193/200\n",
      "4673/4673 [==============================] - 4s 766us/step - loss: 0.3735 - accuracy: 0.9076 - mse: 0.0685 - val_loss: 0.2694 - val_accuracy: 0.8977 - val_mse: 0.0749\n",
      "Epoch 194/200\n",
      "4673/4673 [==============================] - 4s 778us/step - loss: 0.3758 - accuracy: 0.9114 - mse: 0.0661 - val_loss: 0.1958 - val_accuracy: 0.9314 - val_mse: 0.0532\n",
      "Epoch 195/200\n",
      "4673/4673 [==============================] - 4s 750us/step - loss: 0.3644 - accuracy: 0.9088 - mse: 0.0689 - val_loss: 0.1951 - val_accuracy: 0.9304 - val_mse: 0.0533\n",
      "Epoch 196/200\n",
      "4673/4673 [==============================] - 3s 739us/step - loss: 0.3723 - accuracy: 0.9101 - mse: 0.0673 - val_loss: 0.1748 - val_accuracy: 0.9347 - val_mse: 0.0475\n",
      "Epoch 197/200\n",
      "4673/4673 [==============================] - 4s 760us/step - loss: 0.3623 - accuracy: 0.9116 - mse: 0.0661 - val_loss: 0.1745 - val_accuracy: 0.9369 - val_mse: 0.0473\n",
      "Epoch 198/200\n",
      "4673/4673 [==============================] - 4s 772us/step - loss: 0.3613 - accuracy: 0.9118 - mse: 0.0657 - val_loss: 0.1805 - val_accuracy: 0.9347 - val_mse: 0.0492\n",
      "Epoch 199/200\n",
      "4673/4673 [==============================] - 4s 826us/step - loss: 0.3608 - accuracy: 0.9144 - mse: 0.0643 - val_loss: 0.1457 - val_accuracy: 0.9489 - val_mse: 0.0392\n",
      "Epoch 200/200\n",
      "4673/4673 [==============================] - 4s 769us/step - loss: 0.3617 - accuracy: 0.9093 - mse: 0.0679 - val_loss: 0.1735 - val_accuracy: 0.9358 - val_mse: 0.0467\n",
      "475/475 [==============================] - 0s 328us/step\n",
      "Final loss = 0.15439988409218036\n",
      "Final accuracy = 0.9389473795890808\n",
      "Left out subject = diana\n",
      "Test subjects = ['aggie', 'andrius', 'jack', 'joao', 'lukasz', 'nikita', 'rim', 'ron', 'santi', 'seb', 'sharan', 'zoe']\n",
      "Validation subjects = ['adela', 'teo']\n",
      "--------------------------------------------------------------------------------\n",
      "Removing outliers\n",
      "--------------------------------------------------------------------------------\n",
      "Original dataframe length = 118117\n",
      "New dataframe length = 115584\n",
      "Removed 2533 outliers\n",
      "Original dataframe length = 8350\n",
      "New dataframe length = 8203\n",
      "Removed 147 outliers\n",
      "--------------------------------------------------------------------------------\n",
      "Standardising\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Normalising\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generating datasets\n",
      "--------------------------------------------------------------------------------\n",
      "Total samples for true activity = 4812\n",
      "Total samples for false activity = 0\n",
      "Training set shapes: X_train = (4812, 38, 3), y_train = (4812, 2)\n",
      "Total samples for true activity = 845\n",
      "Total samples for false activity = 0\n",
      "Valid set shapes: X_valid = (845, 38, 3), y_valid = (845, 2)\n",
      "Total samples for true activity = 398\n",
      "Total samples for false activity = 0\n",
      "Test set shapes: X_test = (398, 38, 3), y_test = (398, 2)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 36, 64)            640       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 34, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 34, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 32, 64)            12352     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               204900    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 230,702\n",
      "Trainable params: 230,574\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Train on 4812 samples, validate on 845 samples\n",
      "Epoch 1/200\n",
      "4812/4812 [==============================] - 6s 1ms/step - loss: 2.3163 - accuracy: 0.1852 - mse: 0.4075 - val_loss: 0.7372 - val_accuracy: 0.1503 - val_mse: 0.2720\n",
      "Epoch 2/200\n",
      "4812/4812 [==============================] - 4s 780us/step - loss: 1.8907 - accuracy: 0.1615 - mse: 0.4929 - val_loss: 0.7758 - val_accuracy: 0.1515 - val_mse: 0.2911\n",
      "Epoch 3/200\n",
      "4812/4812 [==============================] - 4s 791us/step - loss: 1.8057 - accuracy: 0.1627 - mse: 0.4958 - val_loss: 0.8663 - val_accuracy: 0.1527 - val_mse: 0.3344\n",
      "Epoch 4/200\n",
      "4812/4812 [==============================] - 4s 804us/step - loss: 1.7377 - accuracy: 0.1648 - mse: 0.4835 - val_loss: 1.0105 - val_accuracy: 0.1527 - val_mse: 0.3962\n",
      "Epoch 5/200\n",
      "4812/4812 [==============================] - 4s 754us/step - loss: 1.6794 - accuracy: 0.1687 - mse: 0.4673 - val_loss: 1.1086 - val_accuracy: 0.1574 - val_mse: 0.4307\n",
      "Epoch 6/200\n",
      "4812/4812 [==============================] - 4s 768us/step - loss: 1.6235 - accuracy: 0.1814 - mse: 0.4465 - val_loss: 1.1419 - val_accuracy: 0.1669 - val_mse: 0.4391\n",
      "Epoch 7/200\n",
      "4812/4812 [==============================] - 4s 752us/step - loss: 1.5735 - accuracy: 0.1964 - mse: 0.4317 - val_loss: 1.1280 - val_accuracy: 0.1799 - val_mse: 0.4312\n",
      "Epoch 8/200\n",
      "4812/4812 [==============================] - 4s 778us/step - loss: 1.5248 - accuracy: 0.2232 - mse: 0.4166 - val_loss: 1.0947 - val_accuracy: 0.2178 - val_mse: 0.4161\n",
      "Epoch 9/200\n",
      "4812/4812 [==============================] - 4s 923us/step - loss: 1.4781 - accuracy: 0.2579 - mse: 0.3994 - val_loss: 1.0692 - val_accuracy: 0.2568 - val_mse: 0.4037\n",
      "Epoch 10/200\n",
      "4812/4812 [==============================] - 7s 1ms/step - loss: 1.4352 - accuracy: 0.2932 - mse: 0.3849 - val_loss: 1.0154 - val_accuracy: 0.2970 - val_mse: 0.3810\n",
      "Epoch 11/200\n",
      "4812/4812 [==============================] - 8s 2ms/step - loss: 1.3924 - accuracy: 0.3367 - mse: 0.3692 - val_loss: 1.0126 - val_accuracy: 0.3290 - val_mse: 0.3763\n",
      "Epoch 12/200\n",
      "4812/4812 [==============================] - 7s 2ms/step - loss: 1.3581 - accuracy: 0.3724 - mse: 0.3586 - val_loss: 0.9665 - val_accuracy: 0.3775 - val_mse: 0.3568\n",
      "Epoch 13/200\n",
      "4812/4812 [==============================] - 4s 907us/step - loss: 1.3162 - accuracy: 0.4239 - mse: 0.3394 - val_loss: 0.9423 - val_accuracy: 0.4237 - val_mse: 0.3444\n",
      "Epoch 14/200\n",
      "4812/4812 [==============================] - 5s 939us/step - loss: 1.2827 - accuracy: 0.4578 - mse: 0.3289 - val_loss: 0.9197 - val_accuracy: 0.4521 - val_mse: 0.3341\n",
      "Epoch 15/200\n",
      "4812/4812 [==============================] - 4s 848us/step - loss: 1.2496 - accuracy: 0.4944 - mse: 0.3142 - val_loss: 0.9253 - val_accuracy: 0.4627 - val_mse: 0.3332\n",
      "Epoch 16/200\n",
      "4812/4812 [==============================] - 5s 975us/step - loss: 1.2177 - accuracy: 0.5204 - mse: 0.3055 - val_loss: 0.8584 - val_accuracy: 0.5053 - val_mse: 0.3072\n",
      "Epoch 17/200\n",
      "4812/4812 [==============================] - 5s 1ms/step - loss: 1.1919 - accuracy: 0.5461 - mse: 0.2957 - val_loss: 0.8189 - val_accuracy: 0.5479 - val_mse: 0.2898\n",
      "Epoch 18/200\n",
      "4812/4812 [==============================] - 8s 2ms/step - loss: 1.1578 - accuracy: 0.5860 - mse: 0.2791 - val_loss: 0.8827 - val_accuracy: 0.5172 - val_mse: 0.3118\n",
      "Epoch 19/200\n",
      "4812/4812 [==============================] - 5s 1ms/step - loss: 1.1315 - accuracy: 0.5923 - mse: 0.2750 - val_loss: 0.8267 - val_accuracy: 0.5550 - val_mse: 0.2892\n",
      "Epoch 20/200\n",
      "4812/4812 [==============================] - 4s 869us/step - loss: 1.1081 - accuracy: 0.6176 - mse: 0.2631 - val_loss: 0.8247 - val_accuracy: 0.5621 - val_mse: 0.2867\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4812/4812 [==============================] - 5s 992us/step - loss: 1.0826 - accuracy: 0.6340 - mse: 0.2551 - val_loss: 0.8232 - val_accuracy: 0.5669 - val_mse: 0.2847\n",
      "Epoch 22/200\n",
      "4812/4812 [==============================] - 5s 1ms/step - loss: 1.0591 - accuracy: 0.6511 - mse: 0.2482 - val_loss: 0.7860 - val_accuracy: 0.5976 - val_mse: 0.2703\n",
      "Epoch 23/200\n",
      "4812/4812 [==============================] - 5s 972us/step - loss: 1.0353 - accuracy: 0.6598 - mse: 0.2411 - val_loss: 0.7454 - val_accuracy: 0.6331 - val_mse: 0.2540\n",
      "Epoch 24/200\n",
      "4812/4812 [==============================] - 4s 775us/step - loss: 1.0130 - accuracy: 0.6808 - mse: 0.2298 - val_loss: 0.7748 - val_accuracy: 0.6249 - val_mse: 0.2633\n",
      "Epoch 25/200\n",
      "4812/4812 [==============================] - 4s 788us/step - loss: 0.9941 - accuracy: 0.6829 - mse: 0.2277 - val_loss: 0.7315 - val_accuracy: 0.6485 - val_mse: 0.2472\n",
      "Epoch 26/200\n",
      "4812/4812 [==============================] - 4s 771us/step - loss: 0.9767 - accuracy: 0.6978 - mse: 0.2204 - val_loss: 0.7047 - val_accuracy: 0.6675 - val_mse: 0.2361\n",
      "Epoch 27/200\n",
      "4812/4812 [==============================] - 4s 798us/step - loss: 0.9579 - accuracy: 0.7064 - mse: 0.2140 - val_loss: 0.6987 - val_accuracy: 0.6734 - val_mse: 0.2332\n",
      "Epoch 28/200\n",
      "4812/4812 [==============================] - 4s 770us/step - loss: 0.9398 - accuracy: 0.7180 - mse: 0.2061 - val_loss: 0.6628 - val_accuracy: 0.6840 - val_mse: 0.2194\n",
      "Epoch 29/200\n",
      "4812/4812 [==============================] - 4s 782us/step - loss: 0.9230 - accuracy: 0.7271 - mse: 0.2003 - val_loss: 0.6871 - val_accuracy: 0.6757 - val_mse: 0.2269\n",
      "Epoch 30/200\n",
      "4812/4812 [==============================] - 4s 769us/step - loss: 0.9030 - accuracy: 0.7292 - mse: 0.1975 - val_loss: 0.6733 - val_accuracy: 0.6817 - val_mse: 0.2215\n",
      "Epoch 31/200\n",
      "4812/4812 [==============================] - 4s 835us/step - loss: 0.8934 - accuracy: 0.7396 - mse: 0.1917 - val_loss: 0.6925 - val_accuracy: 0.6793 - val_mse: 0.2277\n",
      "Epoch 32/200\n",
      "4812/4812 [==============================] - 4s 779us/step - loss: 0.8755 - accuracy: 0.7386 - mse: 0.1906 - val_loss: 0.6209 - val_accuracy: 0.7124 - val_mse: 0.2023\n",
      "Epoch 33/200\n",
      "4812/4812 [==============================] - 4s 786us/step - loss: 0.8582 - accuracy: 0.7562 - mse: 0.1795 - val_loss: 0.6489 - val_accuracy: 0.6959 - val_mse: 0.2110\n",
      "Epoch 34/200\n",
      "4812/4812 [==============================] - 4s 803us/step - loss: 0.8428 - accuracy: 0.7556 - mse: 0.1792 - val_loss: 0.6299 - val_accuracy: 0.7053 - val_mse: 0.2040\n",
      "Epoch 35/200\n",
      "4812/4812 [==============================] - 4s 846us/step - loss: 0.8344 - accuracy: 0.7614 - mse: 0.1758 - val_loss: 0.5999 - val_accuracy: 0.7207 - val_mse: 0.1930\n",
      "Epoch 36/200\n",
      "4812/4812 [==============================] - 4s 917us/step - loss: 0.8157 - accuracy: 0.7724 - mse: 0.1689 - val_loss: 0.6150 - val_accuracy: 0.7112 - val_mse: 0.1979\n",
      "Epoch 37/200\n",
      "4812/4812 [==============================] - 4s 904us/step - loss: 0.8057 - accuracy: 0.7783 - mse: 0.1655 - val_loss: 0.6162 - val_accuracy: 0.7160 - val_mse: 0.1976\n",
      "Epoch 38/200\n",
      "4812/4812 [==============================] - 4s 803us/step - loss: 0.7910 - accuracy: 0.7764 - mse: 0.1665 - val_loss: 0.5430 - val_accuracy: 0.7467 - val_mse: 0.1719\n",
      "Epoch 39/200\n",
      "4812/4812 [==============================] - 4s 878us/step - loss: 0.7854 - accuracy: 0.7882 - mse: 0.1585 - val_loss: 0.5798 - val_accuracy: 0.7361 - val_mse: 0.1841\n",
      "Epoch 40/200\n",
      "4812/4812 [==============================] - 4s 794us/step - loss: 0.7756 - accuracy: 0.7870 - mse: 0.1574 - val_loss: 0.5985 - val_accuracy: 0.7243 - val_mse: 0.1900\n",
      "Epoch 41/200\n",
      "4812/4812 [==============================] - 4s 835us/step - loss: 0.7619 - accuracy: 0.7889 - mse: 0.1555 - val_loss: 0.5610 - val_accuracy: 0.7408 - val_mse: 0.1772\n",
      "Epoch 42/200\n",
      "4812/4812 [==============================] - 4s 878us/step - loss: 0.7471 - accuracy: 0.7963 - mse: 0.1515 - val_loss: 0.5238 - val_accuracy: 0.7633 - val_mse: 0.1635\n",
      "Epoch 43/200\n",
      "4812/4812 [==============================] - 4s 768us/step - loss: 0.7487 - accuracy: 0.7972 - mse: 0.1500 - val_loss: 0.5276 - val_accuracy: 0.7621 - val_mse: 0.1650\n",
      "Epoch 44/200\n",
      "4812/4812 [==============================] - 4s 921us/step - loss: 0.7298 - accuracy: 0.8063 - mse: 0.1462 - val_loss: 0.5148 - val_accuracy: 0.7645 - val_mse: 0.1604\n",
      "Epoch 45/200\n",
      "4812/4812 [==============================] - 4s 790us/step - loss: 0.7248 - accuracy: 0.8105 - mse: 0.1443 - val_loss: 0.5444 - val_accuracy: 0.7527 - val_mse: 0.1707\n",
      "Epoch 46/200\n",
      "4812/4812 [==============================] - 4s 816us/step - loss: 0.7159 - accuracy: 0.8119 - mse: 0.1413 - val_loss: 0.5689 - val_accuracy: 0.7396 - val_mse: 0.1789\n",
      "Epoch 47/200\n",
      "4812/4812 [==============================] - 4s 809us/step - loss: 0.7037 - accuracy: 0.8159 - mse: 0.1396 - val_loss: 0.5028 - val_accuracy: 0.7775 - val_mse: 0.1557\n",
      "Epoch 48/200\n",
      "4812/4812 [==============================] - 4s 801us/step - loss: 0.6926 - accuracy: 0.8192 - mse: 0.1368 - val_loss: 0.5209 - val_accuracy: 0.7669 - val_mse: 0.1617\n",
      "Epoch 49/200\n",
      "4812/4812 [==============================] - 4s 766us/step - loss: 0.6963 - accuracy: 0.8194 - mse: 0.1368 - val_loss: 0.4959 - val_accuracy: 0.7811 - val_mse: 0.1528\n",
      "Epoch 50/200\n",
      "4812/4812 [==============================] - 4s 813us/step - loss: 0.6745 - accuracy: 0.8252 - mse: 0.1330 - val_loss: 0.4880 - val_accuracy: 0.7858 - val_mse: 0.1501\n",
      "Epoch 51/200\n",
      "4812/4812 [==============================] - 4s 918us/step - loss: 0.6687 - accuracy: 0.8302 - mse: 0.1311 - val_loss: 0.4874 - val_accuracy: 0.7893 - val_mse: 0.1496\n",
      "Epoch 52/200\n",
      "4812/4812 [==============================] - 4s 799us/step - loss: 0.6704 - accuracy: 0.8317 - mse: 0.1292 - val_loss: 0.5153 - val_accuracy: 0.7704 - val_mse: 0.1596\n",
      "Epoch 53/200\n",
      "4812/4812 [==============================] - 4s 810us/step - loss: 0.6519 - accuracy: 0.8344 - mse: 0.1252 - val_loss: 0.5115 - val_accuracy: 0.7740 - val_mse: 0.1582\n",
      "Epoch 54/200\n",
      "4812/4812 [==============================] - 4s 787us/step - loss: 0.6434 - accuracy: 0.8317 - mse: 0.1261 - val_loss: 0.4669 - val_accuracy: 0.8024 - val_mse: 0.1427\n",
      "Epoch 55/200\n",
      "4812/4812 [==============================] - 4s 831us/step - loss: 0.6408 - accuracy: 0.8398 - mse: 0.1223 - val_loss: 0.4750 - val_accuracy: 0.8000 - val_mse: 0.1451\n",
      "Epoch 56/200\n",
      "4812/4812 [==============================] - 4s 805us/step - loss: 0.6393 - accuracy: 0.8362 - mse: 0.1249 - val_loss: 0.4288 - val_accuracy: 0.8201 - val_mse: 0.1295\n",
      "Epoch 57/200\n",
      "4812/4812 [==============================] - 4s 782us/step - loss: 0.6306 - accuracy: 0.8412 - mse: 0.1209 - val_loss: 0.4548 - val_accuracy: 0.8071 - val_mse: 0.1386\n",
      "Epoch 58/200\n",
      "4812/4812 [==============================] - 4s 775us/step - loss: 0.6262 - accuracy: 0.8462 - mse: 0.1187 - val_loss: 0.4693 - val_accuracy: 0.8012 - val_mse: 0.1434\n",
      "Epoch 59/200\n",
      "4812/4812 [==============================] - 4s 759us/step - loss: 0.6152 - accuracy: 0.8456 - mse: 0.1189 - val_loss: 0.4173 - val_accuracy: 0.8272 - val_mse: 0.1254\n",
      "Epoch 60/200\n",
      "4812/4812 [==============================] - 4s 768us/step - loss: 0.6068 - accuracy: 0.8495 - mse: 0.1161 - val_loss: 0.4157 - val_accuracy: 0.8272 - val_mse: 0.1249\n",
      "Epoch 61/200\n",
      "4812/4812 [==============================] - 4s 740us/step - loss: 0.6080 - accuracy: 0.8529 - mse: 0.1128 - val_loss: 0.5071 - val_accuracy: 0.7751 - val_mse: 0.1561\n",
      "Epoch 62/200\n",
      "4812/4812 [==============================] - 4s 749us/step - loss: 0.5938 - accuracy: 0.8491 - mse: 0.1153 - val_loss: 0.4510 - val_accuracy: 0.8047 - val_mse: 0.1367\n",
      "Epoch 63/200\n",
      "4812/4812 [==============================] - 4s 782us/step - loss: 0.5912 - accuracy: 0.8587 - mse: 0.1109 - val_loss: 0.4716 - val_accuracy: 0.7976 - val_mse: 0.1436\n",
      "Epoch 64/200\n",
      "4812/4812 [==============================] - 4s 780us/step - loss: 0.5965 - accuracy: 0.8545 - mse: 0.1130 - val_loss: 0.4835 - val_accuracy: 0.7929 - val_mse: 0.1480\n",
      "Epoch 65/200\n",
      "4812/4812 [==============================] - 4s 752us/step - loss: 0.5840 - accuracy: 0.8533 - mse: 0.1132 - val_loss: 0.3872 - val_accuracy: 0.8426 - val_mse: 0.1151\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4812/4812 [==============================] - 4s 789us/step - loss: 0.5764 - accuracy: 0.8601 - mse: 0.1082 - val_loss: 0.3940 - val_accuracy: 0.8379 - val_mse: 0.1172\n",
      "Epoch 67/200\n",
      "4812/4812 [==============================] - 4s 790us/step - loss: 0.5695 - accuracy: 0.8624 - mse: 0.1070 - val_loss: 0.4088 - val_accuracy: 0.8308 - val_mse: 0.1221\n",
      "Epoch 68/200\n",
      "4812/4812 [==============================] - 4s 875us/step - loss: 0.5681 - accuracy: 0.8614 - mse: 0.1065 - val_loss: 0.4170 - val_accuracy: 0.8260 - val_mse: 0.1252\n",
      "Epoch 69/200\n",
      "4812/4812 [==============================] - 4s 747us/step - loss: 0.5679 - accuracy: 0.8622 - mse: 0.1054 - val_loss: 0.4433 - val_accuracy: 0.8118 - val_mse: 0.1340\n",
      "Epoch 70/200\n",
      "4812/4812 [==============================] - 4s 764us/step - loss: 0.5603 - accuracy: 0.8628 - mse: 0.1053 - val_loss: 0.4492 - val_accuracy: 0.8130 - val_mse: 0.1360\n",
      "Epoch 71/200\n",
      "4812/4812 [==============================] - 4s 778us/step - loss: 0.5582 - accuracy: 0.8655 - mse: 0.1036 - val_loss: 0.4679 - val_accuracy: 0.7988 - val_mse: 0.1425\n",
      "Epoch 72/200\n",
      "4812/4812 [==============================] - 7s 1ms/step - loss: 0.5506 - accuracy: 0.8649 - mse: 0.1048 - val_loss: 0.3922 - val_accuracy: 0.8343 - val_mse: 0.1169\n",
      "Epoch 73/200\n",
      "4812/4812 [==============================] - 5s 1ms/step - loss: 0.5419 - accuracy: 0.8670 - mse: 0.1016 - val_loss: 0.3834 - val_accuracy: 0.8426 - val_mse: 0.1135\n",
      "Epoch 74/200\n",
      "4812/4812 [==============================] - 4s 843us/step - loss: 0.5339 - accuracy: 0.8685 - mse: 0.1003 - val_loss: 0.3988 - val_accuracy: 0.8355 - val_mse: 0.1190\n",
      "Epoch 75/200\n",
      "4812/4812 [==============================] - 4s 786us/step - loss: 0.5318 - accuracy: 0.8699 - mse: 0.0998 - val_loss: 0.3628 - val_accuracy: 0.8544 - val_mse: 0.1065\n",
      "Epoch 76/200\n",
      "4812/4812 [==============================] - 4s 762us/step - loss: 0.5360 - accuracy: 0.8722 - mse: 0.0991 - val_loss: 0.3829 - val_accuracy: 0.8402 - val_mse: 0.1135\n",
      "Epoch 77/200\n",
      "4812/4812 [==============================] - 4s 741us/step - loss: 0.5306 - accuracy: 0.8722 - mse: 0.0974 - val_loss: 0.4572 - val_accuracy: 0.8036 - val_mse: 0.1388\n",
      "Epoch 78/200\n",
      "4812/4812 [==============================] - 4s 748us/step - loss: 0.5159 - accuracy: 0.8747 - mse: 0.0973 - val_loss: 0.3426 - val_accuracy: 0.8627 - val_mse: 0.0997\n",
      "Epoch 79/200\n",
      "4812/4812 [==============================] - 4s 756us/step - loss: 0.5204 - accuracy: 0.8784 - mse: 0.0952 - val_loss: 0.4039 - val_accuracy: 0.8272 - val_mse: 0.1209\n",
      "Epoch 80/200\n",
      "4812/4812 [==============================] - 4s 756us/step - loss: 0.5141 - accuracy: 0.8749 - mse: 0.0966 - val_loss: 0.3412 - val_accuracy: 0.8663 - val_mse: 0.0997\n",
      "Epoch 81/200\n",
      "4812/4812 [==============================] - 4s 779us/step - loss: 0.5104 - accuracy: 0.8768 - mse: 0.0948 - val_loss: 0.3346 - val_accuracy: 0.8651 - val_mse: 0.0974\n",
      "Epoch 82/200\n",
      "4812/4812 [==============================] - 4s 780us/step - loss: 0.4984 - accuracy: 0.8836 - mse: 0.0917 - val_loss: 0.3624 - val_accuracy: 0.8521 - val_mse: 0.1068\n",
      "Epoch 83/200\n",
      "4812/4812 [==============================] - 5s 1ms/step - loss: 0.4989 - accuracy: 0.8803 - mse: 0.0933 - val_loss: 0.3382 - val_accuracy: 0.8627 - val_mse: 0.0984\n",
      "Epoch 84/200\n",
      "4812/4812 [==============================] - 4s 765us/step - loss: 0.4982 - accuracy: 0.8818 - mse: 0.0919 - val_loss: 0.3516 - val_accuracy: 0.8556 - val_mse: 0.1032\n",
      "Epoch 85/200\n",
      "4812/4812 [==============================] - 4s 818us/step - loss: 0.4843 - accuracy: 0.8813 - mse: 0.0908 - val_loss: 0.3359 - val_accuracy: 0.8639 - val_mse: 0.0978\n",
      "Epoch 86/200\n",
      "4812/4812 [==============================] - 4s 749us/step - loss: 0.4965 - accuracy: 0.8815 - mse: 0.0902 - val_loss: 0.3857 - val_accuracy: 0.8414 - val_mse: 0.1145\n",
      "Epoch 87/200\n",
      "4812/4812 [==============================] - 4s 814us/step - loss: 0.4861 - accuracy: 0.8855 - mse: 0.0884 - val_loss: 0.4312 - val_accuracy: 0.8166 - val_mse: 0.1300\n",
      "Epoch 88/200\n",
      "4812/4812 [==============================] - 4s 737us/step - loss: 0.4802 - accuracy: 0.8834 - mse: 0.0902 - val_loss: 0.3169 - val_accuracy: 0.8746 - val_mse: 0.0917\n",
      "Epoch 89/200\n",
      "4812/4812 [==============================] - 4s 738us/step - loss: 0.4792 - accuracy: 0.8851 - mse: 0.0891 - val_loss: 0.3353 - val_accuracy: 0.8615 - val_mse: 0.0984\n",
      "Epoch 90/200\n",
      "4812/4812 [==============================] - 4s 754us/step - loss: 0.4774 - accuracy: 0.8917 - mse: 0.0853 - val_loss: 0.3886 - val_accuracy: 0.8343 - val_mse: 0.1157\n",
      "Epoch 91/200\n",
      "4812/4812 [==============================] - 4s 754us/step - loss: 0.4818 - accuracy: 0.8865 - mse: 0.0898 - val_loss: 0.3585 - val_accuracy: 0.8509 - val_mse: 0.1062\n",
      "Epoch 92/200\n",
      "4812/4812 [==============================] - 4s 878us/step - loss: 0.4689 - accuracy: 0.8878 - mse: 0.0858 - val_loss: 0.3668 - val_accuracy: 0.8521 - val_mse: 0.1087\n",
      "Epoch 93/200\n",
      "4812/4812 [==============================] - 4s 863us/step - loss: 0.4664 - accuracy: 0.8894 - mse: 0.0851 - val_loss: 0.3798 - val_accuracy: 0.8402 - val_mse: 0.1129\n",
      "Epoch 94/200\n",
      "4812/4812 [==============================] - 4s 799us/step - loss: 0.4633 - accuracy: 0.8890 - mse: 0.0870 - val_loss: 0.3039 - val_accuracy: 0.8793 - val_mse: 0.0879\n",
      "Epoch 95/200\n",
      "4812/4812 [==============================] - 4s 751us/step - loss: 0.4678 - accuracy: 0.8894 - mse: 0.0846 - val_loss: 0.3296 - val_accuracy: 0.8675 - val_mse: 0.0965\n",
      "Epoch 96/200\n",
      "4812/4812 [==============================] - 4s 770us/step - loss: 0.4581 - accuracy: 0.8924 - mse: 0.0826 - val_loss: 0.4300 - val_accuracy: 0.8154 - val_mse: 0.1299\n",
      "Epoch 97/200\n",
      "4812/4812 [==============================] - 5s 1ms/step - loss: 0.4445 - accuracy: 0.8917 - mse: 0.0837 - val_loss: 0.3334 - val_accuracy: 0.8675 - val_mse: 0.0975\n",
      "Epoch 98/200\n",
      "4812/4812 [==============================] - 4s 923us/step - loss: 0.4532 - accuracy: 0.8953 - mse: 0.0828 - val_loss: 0.3036 - val_accuracy: 0.8793 - val_mse: 0.0876\n",
      "Epoch 99/200\n",
      "4812/4812 [==============================] - 4s 809us/step - loss: 0.4508 - accuracy: 0.8936 - mse: 0.0815 - val_loss: 0.3877 - val_accuracy: 0.8296 - val_mse: 0.1163\n",
      "Epoch 100/200\n",
      "4812/4812 [==============================] - 4s 921us/step - loss: 0.4455 - accuracy: 0.8944 - mse: 0.0827 - val_loss: 0.3105 - val_accuracy: 0.8793 - val_mse: 0.0901\n",
      "Epoch 101/200\n",
      "4812/4812 [==============================] - 4s 861us/step - loss: 0.4443 - accuracy: 0.8959 - mse: 0.0805 - val_loss: 0.3908 - val_accuracy: 0.8272 - val_mse: 0.1170\n",
      "Epoch 102/200\n",
      "4812/4812 [==============================] - 4s 827us/step - loss: 0.4396 - accuracy: 0.8944 - mse: 0.0806 - val_loss: 0.3326 - val_accuracy: 0.8698 - val_mse: 0.0974\n",
      "Epoch 103/200\n",
      "4812/4812 [==============================] - 4s 840us/step - loss: 0.4337 - accuracy: 0.8975 - mse: 0.0798 - val_loss: 0.3206 - val_accuracy: 0.8734 - val_mse: 0.0936\n",
      "Epoch 104/200\n",
      "4812/4812 [==============================] - 4s 903us/step - loss: 0.4239 - accuracy: 0.9007 - mse: 0.0773 - val_loss: 0.3572 - val_accuracy: 0.8509 - val_mse: 0.1060\n",
      "Epoch 105/200\n",
      "4812/4812 [==============================] - 4s 883us/step - loss: 0.4347 - accuracy: 0.8965 - mse: 0.0804 - val_loss: 0.2845 - val_accuracy: 0.8864 - val_mse: 0.0818\n",
      "Epoch 106/200\n",
      "4812/4812 [==============================] - 4s 869us/step - loss: 0.4338 - accuracy: 0.9013 - mse: 0.0769 - val_loss: 0.4333 - val_accuracy: 0.8166 - val_mse: 0.1313\n",
      "Epoch 107/200\n",
      "4812/4812 [==============================] - 5s 1ms/step - loss: 0.4246 - accuracy: 0.8946 - mse: 0.0796 - val_loss: 0.2897 - val_accuracy: 0.8876 - val_mse: 0.0835\n",
      "Epoch 108/200\n",
      "4812/4812 [==============================] - 6s 1ms/step - loss: 0.4283 - accuracy: 0.9002 - mse: 0.0775 - val_loss: 0.3485 - val_accuracy: 0.8521 - val_mse: 0.1032\n",
      "Epoch 109/200\n",
      "4812/4812 [==============================] - 4s 775us/step - loss: 0.4165 - accuracy: 0.9007 - mse: 0.0773 - val_loss: 0.2862 - val_accuracy: 0.8864 - val_mse: 0.0825\n",
      "Epoch 110/200\n",
      "4812/4812 [==============================] - 4s 888us/step - loss: 0.4201 - accuracy: 0.8988 - mse: 0.0758 - val_loss: 0.3030 - val_accuracy: 0.8781 - val_mse: 0.0882\n",
      "Epoch 111/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4812/4812 [==============================] - 5s 1ms/step - loss: 0.4124 - accuracy: 0.9009 - mse: 0.0761 - val_loss: 0.3003 - val_accuracy: 0.8805 - val_mse: 0.0873\n",
      "Epoch 112/200\n",
      "4812/4812 [==============================] - 5s 1ms/step - loss: 0.4198 - accuracy: 0.8978 - mse: 0.0762 - val_loss: 0.3534 - val_accuracy: 0.8462 - val_mse: 0.1052\n",
      "Epoch 113/200\n",
      "4812/4812 [==============================] - 5s 974us/step - loss: 0.4031 - accuracy: 0.9015 - mse: 0.0749 - val_loss: 0.2910 - val_accuracy: 0.8852 - val_mse: 0.0845\n",
      "Epoch 114/200\n",
      "4812/4812 [==============================] - 4s 835us/step - loss: 0.4034 - accuracy: 0.9054 - mse: 0.0727 - val_loss: 0.3340 - val_accuracy: 0.8615 - val_mse: 0.0983\n",
      "Epoch 115/200\n",
      "4812/4812 [==============================] - 4s 888us/step - loss: 0.3996 - accuracy: 0.9030 - mse: 0.0735 - val_loss: 0.3099 - val_accuracy: 0.8734 - val_mse: 0.0910\n",
      "Epoch 116/200\n",
      "4812/4812 [==============================] - 4s 839us/step - loss: 0.4051 - accuracy: 0.8992 - mse: 0.0751 - val_loss: 0.2597 - val_accuracy: 0.8970 - val_mse: 0.0744\n",
      "Epoch 117/200\n",
      "4812/4812 [==============================] - 4s 823us/step - loss: 0.3898 - accuracy: 0.9065 - mse: 0.0706 - val_loss: 0.3057 - val_accuracy: 0.8769 - val_mse: 0.0892\n",
      "Epoch 118/200\n",
      "4812/4812 [==============================] - 4s 876us/step - loss: 0.4022 - accuracy: 0.9042 - mse: 0.0734 - val_loss: 0.3462 - val_accuracy: 0.8533 - val_mse: 0.1032\n",
      "Epoch 119/200\n",
      "4812/4812 [==============================] - 4s 870us/step - loss: 0.3971 - accuracy: 0.9054 - mse: 0.0731 - val_loss: 0.3100 - val_accuracy: 0.8710 - val_mse: 0.0912\n",
      "Epoch 120/200\n",
      "4812/4812 [==============================] - 5s 991us/step - loss: 0.3907 - accuracy: 0.9069 - mse: 0.0704 - val_loss: 0.3322 - val_accuracy: 0.8592 - val_mse: 0.0983\n",
      "Epoch 121/200\n",
      "4812/4812 [==============================] - 4s 853us/step - loss: 0.3852 - accuracy: 0.9057 - mse: 0.0725 - val_loss: 0.2583 - val_accuracy: 0.8994 - val_mse: 0.0739\n",
      "Epoch 122/200\n",
      "4812/4812 [==============================] - 4s 815us/step - loss: 0.3853 - accuracy: 0.9106 - mse: 0.0690 - val_loss: 0.3536 - val_accuracy: 0.8414 - val_mse: 0.1057\n",
      "Epoch 123/200\n",
      "4812/4812 [==============================] - 5s 985us/step - loss: 0.3900 - accuracy: 0.9048 - mse: 0.0724 - val_loss: 0.2979 - val_accuracy: 0.8793 - val_mse: 0.0877\n",
      "Epoch 124/200\n",
      "4812/4812 [==============================] - 5s 1ms/step - loss: 0.3833 - accuracy: 0.9113 - mse: 0.0677 - val_loss: 0.3744 - val_accuracy: 0.8331 - val_mse: 0.1120\n",
      "Epoch 125/200\n",
      "4812/4812 [==============================] - 4s 811us/step - loss: 0.3865 - accuracy: 0.9069 - mse: 0.0713 - val_loss: 0.3107 - val_accuracy: 0.8698 - val_mse: 0.0913\n",
      "Epoch 126/200\n",
      "4812/4812 [==============================] - 4s 817us/step - loss: 0.3828 - accuracy: 0.9069 - mse: 0.0707 - val_loss: 0.3049 - val_accuracy: 0.8757 - val_mse: 0.0893\n",
      "Epoch 127/200\n",
      "4812/4812 [==============================] - 4s 765us/step - loss: 0.3754 - accuracy: 0.9096 - mse: 0.0688 - val_loss: 0.3417 - val_accuracy: 0.8533 - val_mse: 0.1019\n",
      "Epoch 128/200\n",
      "4812/4812 [==============================] - 4s 773us/step - loss: 0.3699 - accuracy: 0.9108 - mse: 0.0683 - val_loss: 0.2928 - val_accuracy: 0.8817 - val_mse: 0.0856\n",
      "Epoch 129/200\n",
      "4812/4812 [==============================] - 4s 775us/step - loss: 0.3722 - accuracy: 0.9094 - mse: 0.0681 - val_loss: 0.2533 - val_accuracy: 0.9030 - val_mse: 0.0726\n",
      "Epoch 130/200\n",
      "4812/4812 [==============================] - 4s 754us/step - loss: 0.3645 - accuracy: 0.9144 - mse: 0.0660 - val_loss: 0.2958 - val_accuracy: 0.8817 - val_mse: 0.0863\n",
      "Epoch 131/200\n",
      "4812/4812 [==============================] - 4s 760us/step - loss: 0.3688 - accuracy: 0.9117 - mse: 0.0678 - val_loss: 0.2834 - val_accuracy: 0.8840 - val_mse: 0.0828\n",
      "Epoch 132/200\n",
      "4812/4812 [==============================] - 4s 793us/step - loss: 0.3681 - accuracy: 0.9133 - mse: 0.0656 - val_loss: 0.2896 - val_accuracy: 0.8805 - val_mse: 0.0849\n",
      "Epoch 133/200\n",
      "4812/4812 [==============================] - 4s 784us/step - loss: 0.3685 - accuracy: 0.9142 - mse: 0.0666 - val_loss: 0.3119 - val_accuracy: 0.8722 - val_mse: 0.0924\n",
      "Epoch 134/200\n",
      "4812/4812 [==============================] - 4s 793us/step - loss: 0.3609 - accuracy: 0.9127 - mse: 0.0665 - val_loss: 0.2725 - val_accuracy: 0.8959 - val_mse: 0.0792\n",
      "Epoch 135/200\n",
      "4812/4812 [==============================] - 4s 789us/step - loss: 0.3582 - accuracy: 0.9150 - mse: 0.0647 - val_loss: 0.2909 - val_accuracy: 0.8793 - val_mse: 0.0856\n",
      "Epoch 136/200\n",
      "4812/4812 [==============================] - 4s 778us/step - loss: 0.3555 - accuracy: 0.9154 - mse: 0.0647 - val_loss: 0.2837 - val_accuracy: 0.8840 - val_mse: 0.0831\n",
      "Epoch 137/200\n",
      "4812/4812 [==============================] - 4s 842us/step - loss: 0.3599 - accuracy: 0.9098 - mse: 0.0668 - val_loss: 0.2658 - val_accuracy: 0.8947 - val_mse: 0.0773\n",
      "Epoch 138/200\n",
      "4812/4812 [==============================] - 4s 751us/step - loss: 0.3561 - accuracy: 0.9129 - mse: 0.0650 - val_loss: 0.2559 - val_accuracy: 0.8982 - val_mse: 0.0741\n",
      "Epoch 139/200\n",
      "4812/4812 [==============================] - 4s 834us/step - loss: 0.3645 - accuracy: 0.9127 - mse: 0.0660 - val_loss: 0.2549 - val_accuracy: 0.9006 - val_mse: 0.0738\n",
      "Epoch 140/200\n",
      "4812/4812 [==============================] - 4s 831us/step - loss: 0.3529 - accuracy: 0.9148 - mse: 0.0648 - val_loss: 0.2442 - val_accuracy: 0.9053 - val_mse: 0.0701\n",
      "Epoch 141/200\n",
      "4812/4812 [==============================] - 4s 852us/step - loss: 0.3540 - accuracy: 0.9179 - mse: 0.0635 - val_loss: 0.2526 - val_accuracy: 0.9006 - val_mse: 0.0731\n",
      "Epoch 142/200\n",
      "4812/4812 [==============================] - 4s 863us/step - loss: 0.3405 - accuracy: 0.9181 - mse: 0.0621 - val_loss: 0.2682 - val_accuracy: 0.8959 - val_mse: 0.0781\n",
      "Epoch 143/200\n",
      "4812/4812 [==============================] - 4s 767us/step - loss: 0.3446 - accuracy: 0.9152 - mse: 0.0633 - val_loss: 0.2607 - val_accuracy: 0.8970 - val_mse: 0.0755\n",
      "Epoch 144/200\n",
      "4812/4812 [==============================] - 4s 758us/step - loss: 0.3408 - accuracy: 0.9173 - mse: 0.0623 - val_loss: 0.2393 - val_accuracy: 0.9065 - val_mse: 0.0686\n",
      "Epoch 145/200\n",
      "4812/4812 [==============================] - 4s 766us/step - loss: 0.3412 - accuracy: 0.9171 - mse: 0.0617 - val_loss: 0.2695 - val_accuracy: 0.8923 - val_mse: 0.0784\n",
      "Epoch 146/200\n",
      "4812/4812 [==============================] - 4s 758us/step - loss: 0.3433 - accuracy: 0.9177 - mse: 0.0626 - val_loss: 0.2855 - val_accuracy: 0.8852 - val_mse: 0.0840\n",
      "Epoch 147/200\n",
      "4812/4812 [==============================] - 4s 748us/step - loss: 0.3438 - accuracy: 0.9173 - mse: 0.0620 - val_loss: 0.2760 - val_accuracy: 0.8899 - val_mse: 0.0807\n",
      "Epoch 148/200\n",
      "4812/4812 [==============================] - 4s 780us/step - loss: 0.3417 - accuracy: 0.9179 - mse: 0.0629 - val_loss: 0.2750 - val_accuracy: 0.8888 - val_mse: 0.0808\n",
      "Epoch 149/200\n",
      "4812/4812 [==============================] - 4s 731us/step - loss: 0.3457 - accuracy: 0.9190 - mse: 0.0617 - val_loss: 0.2966 - val_accuracy: 0.8769 - val_mse: 0.0875\n",
      "Epoch 150/200\n",
      "4812/4812 [==============================] - 4s 851us/step - loss: 0.3339 - accuracy: 0.9210 - mse: 0.0618 - val_loss: 0.2670 - val_accuracy: 0.8923 - val_mse: 0.0780\n",
      "Epoch 151/200\n",
      "4812/4812 [==============================] - 4s 799us/step - loss: 0.3304 - accuracy: 0.9229 - mse: 0.0592 - val_loss: 0.2613 - val_accuracy: 0.8959 - val_mse: 0.0761\n",
      "Epoch 152/200\n",
      "4812/4812 [==============================] - 6s 1ms/step - loss: 0.3276 - accuracy: 0.9200 - mse: 0.0600 - val_loss: 0.3000 - val_accuracy: 0.8734 - val_mse: 0.0890\n",
      "Epoch 153/200\n",
      "4812/4812 [==============================] - 5s 937us/step - loss: 0.3317 - accuracy: 0.9212 - mse: 0.0604 - val_loss: 0.2599 - val_accuracy: 0.8970 - val_mse: 0.0755\n",
      "Epoch 154/200\n",
      "4812/4812 [==============================] - 4s 837us/step - loss: 0.3230 - accuracy: 0.9227 - mse: 0.0593 - val_loss: 0.2740 - val_accuracy: 0.8911 - val_mse: 0.0805\n",
      "Epoch 155/200\n",
      "4812/4812 [==============================] - 4s 810us/step - loss: 0.3286 - accuracy: 0.9202 - mse: 0.0603 - val_loss: 0.2490 - val_accuracy: 0.9030 - val_mse: 0.0722\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4812/4812 [==============================] - 5s 1ms/step - loss: 0.3209 - accuracy: 0.9208 - mse: 0.0592 - val_loss: 0.2161 - val_accuracy: 0.9183 - val_mse: 0.0618\n",
      "Epoch 157/200\n",
      "4812/4812 [==============================] - 4s 824us/step - loss: 0.3195 - accuracy: 0.9250 - mse: 0.0574 - val_loss: 0.2484 - val_accuracy: 0.9041 - val_mse: 0.0721\n",
      "Epoch 158/200\n",
      "4812/4812 [==============================] - 4s 752us/step - loss: 0.3204 - accuracy: 0.9248 - mse: 0.0572 - val_loss: 0.2645 - val_accuracy: 0.8947 - val_mse: 0.0771\n",
      "Epoch 159/200\n",
      "4812/4812 [==============================] - 4s 778us/step - loss: 0.3205 - accuracy: 0.9246 - mse: 0.0585 - val_loss: 0.2799 - val_accuracy: 0.8888 - val_mse: 0.0826\n",
      "Epoch 160/200\n",
      "4812/4812 [==============================] - 4s 779us/step - loss: 0.3179 - accuracy: 0.9237 - mse: 0.0578 - val_loss: 0.2486 - val_accuracy: 0.9018 - val_mse: 0.0721\n",
      "Epoch 161/200\n",
      "4812/4812 [==============================] - 4s 759us/step - loss: 0.3136 - accuracy: 0.9252 - mse: 0.0576 - val_loss: 0.2343 - val_accuracy: 0.9077 - val_mse: 0.0676\n",
      "Epoch 162/200\n",
      "4812/4812 [==============================] - 4s 768us/step - loss: 0.3103 - accuracy: 0.9233 - mse: 0.0569 - val_loss: 0.2347 - val_accuracy: 0.9089 - val_mse: 0.0678\n",
      "Epoch 163/200\n",
      "4812/4812 [==============================] - 4s 767us/step - loss: 0.3223 - accuracy: 0.9246 - mse: 0.0581 - val_loss: 0.2368 - val_accuracy: 0.9053 - val_mse: 0.0685\n",
      "Epoch 164/200\n",
      "4812/4812 [==============================] - 4s 816us/step - loss: 0.3139 - accuracy: 0.9293 - mse: 0.0548 - val_loss: 0.3416 - val_accuracy: 0.8497 - val_mse: 0.1018\n",
      "Epoch 165/200\n",
      "4812/4812 [==============================] - 4s 787us/step - loss: 0.3131 - accuracy: 0.9223 - mse: 0.0583 - val_loss: 0.2707 - val_accuracy: 0.8899 - val_mse: 0.0794\n",
      "Epoch 166/200\n",
      "4812/4812 [==============================] - 6s 1ms/step - loss: 0.3084 - accuracy: 0.9271 - mse: 0.0562 - val_loss: 0.2765 - val_accuracy: 0.8876 - val_mse: 0.0810\n",
      "Epoch 167/200\n",
      "4812/4812 [==============================] - 5s 1ms/step - loss: 0.3102 - accuracy: 0.9250 - mse: 0.0557 - val_loss: 0.2610 - val_accuracy: 0.8959 - val_mse: 0.0759\n",
      "Epoch 168/200\n",
      "4812/4812 [==============================] - 5s 1ms/step - loss: 0.3143 - accuracy: 0.9241 - mse: 0.0573 - val_loss: 0.2918 - val_accuracy: 0.8746 - val_mse: 0.0865\n",
      "Epoch 169/200\n",
      "4812/4812 [==============================] - 4s 761us/step - loss: 0.3118 - accuracy: 0.9252 - mse: 0.0576 - val_loss: 0.2353 - val_accuracy: 0.9112 - val_mse: 0.0682\n",
      "Epoch 170/200\n",
      "4812/4812 [==============================] - 4s 854us/step - loss: 0.3034 - accuracy: 0.9254 - mse: 0.0544 - val_loss: 0.2383 - val_accuracy: 0.9101 - val_mse: 0.0693\n",
      "Epoch 171/200\n",
      "4812/4812 [==============================] - 4s 934us/step - loss: 0.3112 - accuracy: 0.9248 - mse: 0.0572 - val_loss: 0.2322 - val_accuracy: 0.9101 - val_mse: 0.0672\n",
      "Epoch 172/200\n",
      "4812/4812 [==============================] - 4s 823us/step - loss: 0.2978 - accuracy: 0.9296 - mse: 0.0529 - val_loss: 0.2477 - val_accuracy: 0.8994 - val_mse: 0.0717\n",
      "Epoch 173/200\n",
      "4812/4812 [==============================] - 4s 750us/step - loss: 0.2947 - accuracy: 0.9262 - mse: 0.0546 - val_loss: 0.2305 - val_accuracy: 0.9136 - val_mse: 0.0668\n",
      "Epoch 174/200\n",
      "4812/4812 [==============================] - 3s 717us/step - loss: 0.2962 - accuracy: 0.9289 - mse: 0.0525 - val_loss: 0.2391 - val_accuracy: 0.9077 - val_mse: 0.0693\n",
      "Epoch 175/200\n",
      "4812/4812 [==============================] - 3s 725us/step - loss: 0.2975 - accuracy: 0.9285 - mse: 0.0547 - val_loss: 0.2224 - val_accuracy: 0.9148 - val_mse: 0.0642\n",
      "Epoch 176/200\n",
      "4812/4812 [==============================] - 3s 715us/step - loss: 0.2952 - accuracy: 0.9273 - mse: 0.0542 - val_loss: 0.2134 - val_accuracy: 0.9219 - val_mse: 0.0613\n",
      "Epoch 177/200\n",
      "4812/4812 [==============================] - 3s 717us/step - loss: 0.2961 - accuracy: 0.9287 - mse: 0.0531 - val_loss: 0.2399 - val_accuracy: 0.9101 - val_mse: 0.0697\n",
      "Epoch 178/200\n",
      "4812/4812 [==============================] - 4s 739us/step - loss: 0.2982 - accuracy: 0.9296 - mse: 0.0539 - val_loss: 0.2200 - val_accuracy: 0.9148 - val_mse: 0.0636\n",
      "Epoch 179/200\n",
      "4812/4812 [==============================] - 3s 718us/step - loss: 0.2890 - accuracy: 0.9302 - mse: 0.0520 - val_loss: 0.2123 - val_accuracy: 0.9219 - val_mse: 0.0609\n",
      "Epoch 180/200\n",
      "4812/4812 [==============================] - 4s 768us/step - loss: 0.2891 - accuracy: 0.9304 - mse: 0.0521 - val_loss: 0.2319 - val_accuracy: 0.9148 - val_mse: 0.0671\n",
      "Epoch 181/200\n",
      "4812/4812 [==============================] - 4s 772us/step - loss: 0.2871 - accuracy: 0.9300 - mse: 0.0520 - val_loss: 0.2182 - val_accuracy: 0.9172 - val_mse: 0.0628\n",
      "Epoch 182/200\n",
      "4812/4812 [==============================] - 4s 751us/step - loss: 0.2958 - accuracy: 0.9293 - mse: 0.0533 - val_loss: 0.2215 - val_accuracy: 0.9148 - val_mse: 0.0640\n",
      "Epoch 183/200\n",
      "4812/4812 [==============================] - 4s 742us/step - loss: 0.2858 - accuracy: 0.9316 - mse: 0.0506 - val_loss: 0.2554 - val_accuracy: 0.8982 - val_mse: 0.0748\n",
      "Epoch 184/200\n",
      "4812/4812 [==============================] - 3s 723us/step - loss: 0.2849 - accuracy: 0.9329 - mse: 0.0517 - val_loss: 0.2455 - val_accuracy: 0.9018 - val_mse: 0.0717\n",
      "Epoch 185/200\n",
      "4812/4812 [==============================] - 4s 743us/step - loss: 0.2905 - accuracy: 0.9325 - mse: 0.0513 - val_loss: 0.3022 - val_accuracy: 0.8722 - val_mse: 0.0900\n",
      "Epoch 186/200\n",
      "4812/4812 [==============================] - 4s 780us/step - loss: 0.2884 - accuracy: 0.9296 - mse: 0.0521 - val_loss: 0.2744 - val_accuracy: 0.8888 - val_mse: 0.0811\n",
      "Epoch 187/200\n",
      "4812/4812 [==============================] - 4s 849us/step - loss: 0.2802 - accuracy: 0.9327 - mse: 0.0513 - val_loss: 0.2454 - val_accuracy: 0.9041 - val_mse: 0.0716\n",
      "Epoch 188/200\n",
      "4812/4812 [==============================] - 4s 797us/step - loss: 0.2805 - accuracy: 0.9306 - mse: 0.0517 - val_loss: 0.2361 - val_accuracy: 0.9101 - val_mse: 0.0688\n",
      "Epoch 189/200\n",
      "4812/4812 [==============================] - 4s 749us/step - loss: 0.2828 - accuracy: 0.9331 - mse: 0.0509 - val_loss: 0.2461 - val_accuracy: 0.9006 - val_mse: 0.0716\n",
      "Epoch 190/200\n",
      "4812/4812 [==============================] - 4s 794us/step - loss: 0.2773 - accuracy: 0.9316 - mse: 0.0501 - val_loss: 0.2557 - val_accuracy: 0.8970 - val_mse: 0.0749\n",
      "Epoch 191/200\n",
      "4812/4812 [==============================] - 4s 763us/step - loss: 0.2717 - accuracy: 0.9325 - mse: 0.0500 - val_loss: 0.2101 - val_accuracy: 0.9183 - val_mse: 0.0607\n",
      "Epoch 192/200\n",
      "4812/4812 [==============================] - 4s 808us/step - loss: 0.2784 - accuracy: 0.9323 - mse: 0.0507 - val_loss: 0.2201 - val_accuracy: 0.9172 - val_mse: 0.0638\n",
      "Epoch 193/200\n",
      "4812/4812 [==============================] - 4s 795us/step - loss: 0.2807 - accuracy: 0.9331 - mse: 0.0498 - val_loss: 0.2290 - val_accuracy: 0.9112 - val_mse: 0.0668\n",
      "Epoch 194/200\n",
      "4812/4812 [==============================] - 4s 742us/step - loss: 0.2665 - accuracy: 0.9350 - mse: 0.0480 - val_loss: 0.2265 - val_accuracy: 0.9136 - val_mse: 0.0658\n",
      "Epoch 195/200\n",
      "4812/4812 [==============================] - 4s 737us/step - loss: 0.2717 - accuracy: 0.9331 - mse: 0.0495 - val_loss: 0.2414 - val_accuracy: 0.9077 - val_mse: 0.0706\n",
      "Epoch 196/200\n",
      "4812/4812 [==============================] - 4s 732us/step - loss: 0.2684 - accuracy: 0.9318 - mse: 0.0493 - val_loss: 0.2080 - val_accuracy: 0.9183 - val_mse: 0.0601\n",
      "Epoch 197/200\n",
      "4812/4812 [==============================] - 4s 756us/step - loss: 0.2704 - accuracy: 0.9329 - mse: 0.0488 - val_loss: 0.2327 - val_accuracy: 0.9112 - val_mse: 0.0677\n",
      "Epoch 198/200\n",
      "4812/4812 [==============================] - 4s 735us/step - loss: 0.2718 - accuracy: 0.9310 - mse: 0.0494 - val_loss: 0.2301 - val_accuracy: 0.9160 - val_mse: 0.0669\n",
      "Epoch 199/200\n",
      "4812/4812 [==============================] - 4s 813us/step - loss: 0.2699 - accuracy: 0.9370 - mse: 0.0483 - val_loss: 0.2483 - val_accuracy: 0.9006 - val_mse: 0.0724\n",
      "Epoch 200/200\n",
      "4812/4812 [==============================] - 4s 849us/step - loss: 0.2609 - accuracy: 0.9370 - mse: 0.0485 - val_loss: 0.2180 - val_accuracy: 0.9148 - val_mse: 0.0632\n",
      "398/398 [==============================] - 0s 347us/step\n",
      "Final loss = 0.29814424605375556\n",
      "Final accuracy = 0.8894472122192383\n",
      "Left out subject = jack\n",
      "Test subjects = ['adela', 'andrius', 'diana', 'joao', 'lukasz', 'nikita', 'rim', 'ron', 'seb', 'sharan', 'teo', 'zoe']\n",
      "Validation subjects = ['aggie', 'santi']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Removing outliers\n",
      "--------------------------------------------------------------------------------\n",
      "Original dataframe length = 116952\n",
      "New dataframe length = 114654\n",
      "Removed 2298 outliers\n",
      "Original dataframe length = 9515\n",
      "New dataframe length = 9336\n",
      "Removed 179 outliers\n",
      "--------------------------------------------------------------------------------\n",
      "Standardising\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Normalising\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generating datasets\n",
      "--------------------------------------------------------------------------------\n",
      "Total samples for true activity = 4919\n",
      "Total samples for false activity = 0\n",
      "Training set shapes: X_train = (4919, 38, 3), y_train = (4919, 2)\n",
      "Total samples for true activity = 697\n",
      "Total samples for false activity = 0\n",
      "Valid set shapes: X_valid = (697, 38, 3), y_valid = (697, 2)\n",
      "Total samples for true activity = 461\n",
      "Total samples for false activity = 0\n",
      "Test set shapes: X_test = (461, 38, 3), y_test = (461, 2)\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_13 (Conv1D)           (None, 36, 64)            640       \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 34, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 34, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 32, 64)            12352     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               204900    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 230,702\n",
      "Trainable params: 230,574\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Train on 4919 samples, validate on 697 samples\n",
      "Epoch 1/200\n",
      "4919/4919 [==============================] - 6s 1ms/step - loss: 2.3478 - accuracy: 0.2167 - mse: 0.3973 - val_loss: 0.7335 - val_accuracy: 0.1506 - val_mse: 0.2702\n",
      "Epoch 2/200\n",
      "4919/4919 [==============================] - 4s 774us/step - loss: 1.8396 - accuracy: 0.1590 - mse: 0.4850 - val_loss: 0.7742 - val_accuracy: 0.1521 - val_mse: 0.2903\n",
      "Epoch 3/200\n",
      "4919/4919 [==============================] - 4s 796us/step - loss: 1.7489 - accuracy: 0.1632 - mse: 0.4811 - val_loss: 0.8657 - val_accuracy: 0.1549 - val_mse: 0.3339\n",
      "Epoch 4/200\n",
      "4919/4919 [==============================] - 4s 812us/step - loss: 1.6843 - accuracy: 0.1710 - mse: 0.4690 - val_loss: 1.0073 - val_accuracy: 0.1593 - val_mse: 0.3929\n",
      "Epoch 5/200\n",
      "4919/4919 [==============================] - 4s 799us/step - loss: 1.6243 - accuracy: 0.1901 - mse: 0.4487 - val_loss: 1.1023 - val_accuracy: 0.1836 - val_mse: 0.4236\n",
      "Epoch 6/200\n",
      "4919/4919 [==============================] - 5s 929us/step - loss: 1.5712 - accuracy: 0.2187 - mse: 0.4301 - val_loss: 1.1263 - val_accuracy: 0.2109 - val_mse: 0.4267\n",
      "Epoch 7/200\n",
      "4919/4919 [==============================] - 5s 1ms/step - loss: 1.5258 - accuracy: 0.2444 - mse: 0.4204 - val_loss: 1.0795 - val_accuracy: 0.2669 - val_mse: 0.4053\n",
      "Epoch 8/200\n",
      "4919/4919 [==============================] - 6s 1ms/step - loss: 1.4761 - accuracy: 0.2860 - mse: 0.3996 - val_loss: 1.0745 - val_accuracy: 0.2912 - val_mse: 0.3998\n",
      "Epoch 9/200\n",
      "4919/4919 [==============================] - 4s 842us/step - loss: 1.4404 - accuracy: 0.3186 - mse: 0.3862 - val_loss: 1.0458 - val_accuracy: 0.3343 - val_mse: 0.3856\n",
      "Epoch 10/200\n",
      "4919/4919 [==============================] - 4s 744us/step - loss: 1.3988 - accuracy: 0.3588 - mse: 0.3701 - val_loss: 1.0354 - val_accuracy: 0.3558 - val_mse: 0.3781\n",
      "Epoch 11/200\n",
      "4919/4919 [==============================] - 4s 739us/step - loss: 1.3613 - accuracy: 0.3832 - mse: 0.3628 - val_loss: 0.9846 - val_accuracy: 0.4075 - val_mse: 0.3563\n",
      "Epoch 12/200\n",
      "4919/4919 [==============================] - 4s 850us/step - loss: 1.3287 - accuracy: 0.4310 - mse: 0.3453 - val_loss: 0.9806 - val_accuracy: 0.4232 - val_mse: 0.3520\n",
      "Epoch 13/200\n",
      "4919/4919 [==============================] - 4s 757us/step - loss: 1.3008 - accuracy: 0.4525 - mse: 0.3390 - val_loss: 0.9231 - val_accuracy: 0.4821 - val_mse: 0.3283\n",
      "Epoch 14/200\n",
      "4919/4919 [==============================] - 4s 712us/step - loss: 1.2659 - accuracy: 0.4944 - mse: 0.3215 - val_loss: 0.9540 - val_accuracy: 0.4720 - val_mse: 0.3363\n",
      "Epoch 15/200\n",
      "4919/4919 [==============================] - 4s 728us/step - loss: 1.2399 - accuracy: 0.5129 - mse: 0.3145 - val_loss: 0.9533 - val_accuracy: 0.4892 - val_mse: 0.3331\n",
      "Epoch 16/200\n",
      "4919/4919 [==============================] - 4s 726us/step - loss: 1.2117 - accuracy: 0.5316 - mse: 0.3077 - val_loss: 0.9044 - val_accuracy: 0.5237 - val_mse: 0.3137\n",
      "Epoch 17/200\n",
      "4919/4919 [==============================] - 4s 754us/step - loss: 1.1866 - accuracy: 0.5538 - mse: 0.2963 - val_loss: 0.8959 - val_accuracy: 0.5366 - val_mse: 0.3085\n",
      "Epoch 18/200\n",
      "4919/4919 [==============================] - 4s 739us/step - loss: 1.1602 - accuracy: 0.5706 - mse: 0.2862 - val_loss: 0.8895 - val_accuracy: 0.5509 - val_mse: 0.3040\n",
      "Epoch 19/200\n",
      "4919/4919 [==============================] - 4s 743us/step - loss: 1.1413 - accuracy: 0.5812 - mse: 0.2814 - val_loss: 0.8699 - val_accuracy: 0.5638 - val_mse: 0.2952\n",
      "Epoch 20/200\n",
      "4919/4919 [==============================] - 4s 739us/step - loss: 1.1201 - accuracy: 0.6009 - mse: 0.2720 - val_loss: 0.8608 - val_accuracy: 0.5710 - val_mse: 0.2903\n",
      "Epoch 21/200\n",
      "4919/4919 [==============================] - 4s 725us/step - loss: 1.0930 - accuracy: 0.6141 - mse: 0.2646 - val_loss: 0.8388 - val_accuracy: 0.5854 - val_mse: 0.2810\n",
      "Epoch 22/200\n",
      "4919/4919 [==============================] - 4s 728us/step - loss: 1.0726 - accuracy: 0.6237 - mse: 0.2589 - val_loss: 0.8076 - val_accuracy: 0.6098 - val_mse: 0.2688\n",
      "Epoch 23/200\n",
      "4919/4919 [==============================] - 4s 730us/step - loss: 1.0583 - accuracy: 0.6398 - mse: 0.2493 - val_loss: 0.8478 - val_accuracy: 0.5882 - val_mse: 0.2805\n",
      "Epoch 24/200\n",
      "4919/4919 [==============================] - 4s 736us/step - loss: 1.0326 - accuracy: 0.6487 - mse: 0.2448 - val_loss: 0.8121 - val_accuracy: 0.6155 - val_mse: 0.2668\n",
      "Epoch 25/200\n",
      "4919/4919 [==============================] - 4s 748us/step - loss: 1.0194 - accuracy: 0.6556 - mse: 0.2405 - val_loss: 0.7663 - val_accuracy: 0.6442 - val_mse: 0.2504\n",
      "Epoch 26/200\n",
      "4919/4919 [==============================] - 4s 733us/step - loss: 1.0030 - accuracy: 0.6705 - mse: 0.2320 - val_loss: 0.7854 - val_accuracy: 0.6341 - val_mse: 0.2552\n",
      "Epoch 27/200\n",
      "4919/4919 [==============================] - 4s 796us/step - loss: 0.9819 - accuracy: 0.6768 - mse: 0.2269 - val_loss: 0.7406 - val_accuracy: 0.6643 - val_mse: 0.2388\n",
      "Epoch 28/200\n",
      "4919/4919 [==============================] - 6s 1ms/step - loss: 0.9694 - accuracy: 0.6859 - mse: 0.2208 - val_loss: 0.7464 - val_accuracy: 0.6614 - val_mse: 0.2394\n",
      "Epoch 29/200\n",
      "4919/4919 [==============================] - 4s 857us/step - loss: 0.9456 - accuracy: 0.6898 - mse: 0.2184 - val_loss: 0.6995 - val_accuracy: 0.6901 - val_mse: 0.2233\n",
      "Epoch 30/200\n",
      "4919/4919 [==============================] - 4s 853us/step - loss: 0.9367 - accuracy: 0.7040 - mse: 0.2102 - val_loss: 0.7192 - val_accuracy: 0.6772 - val_mse: 0.2286\n",
      "Epoch 31/200\n",
      "4919/4919 [==============================] - 4s 838us/step - loss: 0.9179 - accuracy: 0.7075 - mse: 0.2072 - val_loss: 0.7085 - val_accuracy: 0.6901 - val_mse: 0.2240\n",
      "Epoch 32/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4919/4919 [==============================] - 4s 796us/step - loss: 0.9084 - accuracy: 0.7140 - mse: 0.2015 - val_loss: 0.6904 - val_accuracy: 0.6973 - val_mse: 0.2170\n",
      "Epoch 33/200\n",
      "4919/4919 [==============================] - 4s 716us/step - loss: 0.8965 - accuracy: 0.7274 - mse: 0.1958 - val_loss: 0.7385 - val_accuracy: 0.6729 - val_mse: 0.2311\n",
      "Epoch 34/200\n",
      "4919/4919 [==============================] - 4s 729us/step - loss: 0.8768 - accuracy: 0.7239 - mse: 0.1948 - val_loss: 0.7100 - val_accuracy: 0.6872 - val_mse: 0.2215\n",
      "Epoch 35/200\n",
      "4919/4919 [==============================] - 4s 757us/step - loss: 0.8618 - accuracy: 0.7339 - mse: 0.1889 - val_loss: 0.6802 - val_accuracy: 0.7001 - val_mse: 0.2111\n",
      "Epoch 36/200\n",
      "4919/4919 [==============================] - 4s 804us/step - loss: 0.8482 - accuracy: 0.7367 - mse: 0.1860 - val_loss: 0.6705 - val_accuracy: 0.7044 - val_mse: 0.2072\n",
      "Epoch 37/200\n",
      "4919/4919 [==============================] - 4s 764us/step - loss: 0.8372 - accuracy: 0.7453 - mse: 0.1803 - val_loss: 0.6626 - val_accuracy: 0.7073 - val_mse: 0.2037\n",
      "Epoch 38/200\n",
      "4919/4919 [==============================] - 4s 726us/step - loss: 0.8276 - accuracy: 0.7554 - mse: 0.1758 - val_loss: 0.7075 - val_accuracy: 0.6887 - val_mse: 0.2169\n",
      "Epoch 39/200\n",
      "4919/4919 [==============================] - 4s 730us/step - loss: 0.8117 - accuracy: 0.7609 - mse: 0.1732 - val_loss: 0.7020 - val_accuracy: 0.6887 - val_mse: 0.2144\n",
      "Epoch 40/200\n",
      "4919/4919 [==============================] - 4s 727us/step - loss: 0.8024 - accuracy: 0.7575 - mse: 0.1737 - val_loss: 0.6067 - val_accuracy: 0.7418 - val_mse: 0.1837\n",
      "Epoch 41/200\n",
      "4919/4919 [==============================] - 4s 728us/step - loss: 0.7917 - accuracy: 0.7721 - mse: 0.1671 - val_loss: 0.5840 - val_accuracy: 0.7518 - val_mse: 0.1759\n",
      "Epoch 42/200\n",
      "4919/4919 [==============================] - 4s 740us/step - loss: 0.7865 - accuracy: 0.7715 - mse: 0.1649 - val_loss: 0.5957 - val_accuracy: 0.7504 - val_mse: 0.1791\n",
      "Epoch 43/200\n",
      "4919/4919 [==============================] - 4s 729us/step - loss: 0.7763 - accuracy: 0.7764 - mse: 0.1612 - val_loss: 0.6397 - val_accuracy: 0.7231 - val_mse: 0.1922\n",
      "Epoch 44/200\n",
      "4919/4919 [==============================] - 4s 751us/step - loss: 0.7654 - accuracy: 0.7792 - mse: 0.1593 - val_loss: 0.6324 - val_accuracy: 0.7303 - val_mse: 0.1895\n",
      "Epoch 45/200\n",
      "4919/4919 [==============================] - 4s 818us/step - loss: 0.7617 - accuracy: 0.7857 - mse: 0.1559 - val_loss: 0.6625 - val_accuracy: 0.7145 - val_mse: 0.1979\n",
      "Epoch 46/200\n",
      "4919/4919 [==============================] - 4s 803us/step - loss: 0.7490 - accuracy: 0.7880 - mse: 0.1547 - val_loss: 0.5894 - val_accuracy: 0.7504 - val_mse: 0.1747\n",
      "Epoch 47/200\n",
      "4919/4919 [==============================] - 4s 868us/step - loss: 0.7303 - accuracy: 0.7953 - mse: 0.1511 - val_loss: 0.6120 - val_accuracy: 0.7418 - val_mse: 0.1816\n",
      "Epoch 48/200\n",
      "4919/4919 [==============================] - 4s 828us/step - loss: 0.7177 - accuracy: 0.7998 - mse: 0.1472 - val_loss: 0.5926 - val_accuracy: 0.7518 - val_mse: 0.1749\n",
      "Epoch 49/200\n",
      "4919/4919 [==============================] - 4s 740us/step - loss: 0.7152 - accuracy: 0.8012 - mse: 0.1462 - val_loss: 0.5863 - val_accuracy: 0.7590 - val_mse: 0.1723\n",
      "Epoch 50/200\n",
      "4919/4919 [==============================] - 3s 708us/step - loss: 0.7101 - accuracy: 0.8044 - mse: 0.1452 - val_loss: 0.5447 - val_accuracy: 0.7747 - val_mse: 0.1591\n",
      "Epoch 51/200\n",
      "4919/4919 [==============================] - 4s 834us/step - loss: 0.6929 - accuracy: 0.8101 - mse: 0.1395 - val_loss: 0.5960 - val_accuracy: 0.7575 - val_mse: 0.1745\n",
      "Epoch 52/200\n",
      "4919/4919 [==============================] - 4s 845us/step - loss: 0.6942 - accuracy: 0.8118 - mse: 0.1385 - val_loss: 0.5446 - val_accuracy: 0.7747 - val_mse: 0.1584\n",
      "Epoch 53/200\n",
      "4919/4919 [==============================] - 4s 800us/step - loss: 0.6866 - accuracy: 0.8144 - mse: 0.1375 - val_loss: 0.5504 - val_accuracy: 0.7747 - val_mse: 0.1600\n",
      "Epoch 54/200\n",
      "4919/4919 [==============================] - 4s 816us/step - loss: 0.6688 - accuracy: 0.8187 - mse: 0.1337 - val_loss: 0.5689 - val_accuracy: 0.7661 - val_mse: 0.1652\n",
      "Epoch 55/200\n",
      "4919/4919 [==============================] - 7s 1ms/step - loss: 0.6653 - accuracy: 0.8211 - mse: 0.1326 - val_loss: 0.5644 - val_accuracy: 0.7676 - val_mse: 0.1633\n",
      "Epoch 56/200\n",
      "4919/4919 [==============================] - 5s 1ms/step - loss: 0.6600 - accuracy: 0.8221 - mse: 0.1304 - val_loss: 0.5333 - val_accuracy: 0.7805 - val_mse: 0.1533\n",
      "Epoch 57/200\n",
      "4919/4919 [==============================] - 4s 869us/step - loss: 0.6580 - accuracy: 0.8264 - mse: 0.1302 - val_loss: 0.5634 - val_accuracy: 0.7704 - val_mse: 0.1623\n",
      "Epoch 58/200\n",
      "4919/4919 [==============================] - 4s 901us/step - loss: 0.6545 - accuracy: 0.8282 - mse: 0.1281 - val_loss: 0.5223 - val_accuracy: 0.7948 - val_mse: 0.1495\n",
      "Epoch 59/200\n",
      "4919/4919 [==============================] - 4s 861us/step - loss: 0.6473 - accuracy: 0.8292 - mse: 0.1283 - val_loss: 0.4898 - val_accuracy: 0.8106 - val_mse: 0.1398\n",
      "Epoch 60/200\n",
      "4919/4919 [==============================] - 4s 885us/step - loss: 0.6349 - accuracy: 0.8333 - mse: 0.1227 - val_loss: 0.5105 - val_accuracy: 0.8006 - val_mse: 0.1457\n",
      "Epoch 61/200\n",
      "4919/4919 [==============================] - 4s 805us/step - loss: 0.6345 - accuracy: 0.8321 - mse: 0.1247 - val_loss: 0.5479 - val_accuracy: 0.7805 - val_mse: 0.1568\n",
      "Epoch 62/200\n",
      "4919/4919 [==============================] - 4s 836us/step - loss: 0.6217 - accuracy: 0.8388 - mse: 0.1207 - val_loss: 0.5171 - val_accuracy: 0.7991 - val_mse: 0.1471\n",
      "Epoch 63/200\n",
      "4919/4919 [==============================] - 4s 754us/step - loss: 0.6221 - accuracy: 0.8406 - mse: 0.1191 - val_loss: 0.4824 - val_accuracy: 0.8149 - val_mse: 0.1360\n",
      "Epoch 64/200\n",
      "4919/4919 [==============================] - 4s 744us/step - loss: 0.6183 - accuracy: 0.8376 - mse: 0.1195 - val_loss: 0.5638 - val_accuracy: 0.7747 - val_mse: 0.1603\n",
      "Epoch 65/200\n",
      "4919/4919 [==============================] - 4s 757us/step - loss: 0.5992 - accuracy: 0.8435 - mse: 0.1171 - val_loss: 0.4655 - val_accuracy: 0.8221 - val_mse: 0.1310\n",
      "Epoch 66/200\n",
      "4919/4919 [==============================] - 4s 842us/step - loss: 0.5898 - accuracy: 0.8435 - mse: 0.1141 - val_loss: 0.4933 - val_accuracy: 0.8092 - val_mse: 0.1389\n",
      "Epoch 67/200\n",
      "4919/4919 [==============================] - 4s 836us/step - loss: 0.5960 - accuracy: 0.8506 - mse: 0.1129 - val_loss: 0.5044 - val_accuracy: 0.8063 - val_mse: 0.1419\n",
      "Epoch 68/200\n",
      "4919/4919 [==============================] - 4s 751us/step - loss: 0.5907 - accuracy: 0.8477 - mse: 0.1133 - val_loss: 0.4706 - val_accuracy: 0.8235 - val_mse: 0.1319\n",
      "Epoch 69/200\n",
      "4919/4919 [==============================] - 4s 853us/step - loss: 0.5803 - accuracy: 0.8494 - mse: 0.1115 - val_loss: 0.4864 - val_accuracy: 0.8192 - val_mse: 0.1364\n",
      "Epoch 70/200\n",
      "4919/4919 [==============================] - 4s 883us/step - loss: 0.5758 - accuracy: 0.8538 - mse: 0.1081 - val_loss: 0.4868 - val_accuracy: 0.8207 - val_mse: 0.1359\n",
      "Epoch 71/200\n",
      "4919/4919 [==============================] - 4s 781us/step - loss: 0.5771 - accuracy: 0.8544 - mse: 0.1102 - val_loss: 0.5106 - val_accuracy: 0.8049 - val_mse: 0.1429\n",
      "Epoch 72/200\n",
      "4919/4919 [==============================] - 4s 816us/step - loss: 0.5724 - accuracy: 0.8551 - mse: 0.1077 - val_loss: 0.4858 - val_accuracy: 0.8207 - val_mse: 0.1350\n",
      "Epoch 73/200\n",
      "4919/4919 [==============================] - 4s 832us/step - loss: 0.5640 - accuracy: 0.8546 - mse: 0.1098 - val_loss: 0.4467 - val_accuracy: 0.8350 - val_mse: 0.1242\n",
      "Epoch 74/200\n",
      "4919/4919 [==============================] - 4s 795us/step - loss: 0.5569 - accuracy: 0.8650 - mse: 0.1025 - val_loss: 0.4995 - val_accuracy: 0.8135 - val_mse: 0.1386\n",
      "Epoch 75/200\n",
      "4919/4919 [==============================] - 4s 800us/step - loss: 0.5634 - accuracy: 0.8540 - mse: 0.1085 - val_loss: 0.4805 - val_accuracy: 0.8178 - val_mse: 0.1335\n",
      "Epoch 76/200\n",
      "4919/4919 [==============================] - 4s 794us/step - loss: 0.5582 - accuracy: 0.8618 - mse: 0.1043 - val_loss: 0.4882 - val_accuracy: 0.8164 - val_mse: 0.1356\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4919/4919 [==============================] - 4s 762us/step - loss: 0.5559 - accuracy: 0.8654 - mse: 0.1017 - val_loss: 0.5222 - val_accuracy: 0.8020 - val_mse: 0.1449\n",
      "Epoch 78/200\n",
      "4919/4919 [==============================] - 4s 813us/step - loss: 0.5435 - accuracy: 0.8599 - mse: 0.1040 - val_loss: 0.4452 - val_accuracy: 0.8422 - val_mse: 0.1229\n",
      "Epoch 79/200\n",
      "4919/4919 [==============================] - 4s 793us/step - loss: 0.5367 - accuracy: 0.8632 - mse: 0.1018 - val_loss: 0.4926 - val_accuracy: 0.8164 - val_mse: 0.1362\n",
      "Epoch 80/200\n",
      "4919/4919 [==============================] - 4s 755us/step - loss: 0.5438 - accuracy: 0.8705 - mse: 0.0993 - val_loss: 0.4765 - val_accuracy: 0.8235 - val_mse: 0.1314\n",
      "Epoch 81/200\n",
      "4919/4919 [==============================] - 4s 788us/step - loss: 0.5351 - accuracy: 0.8664 - mse: 0.1002 - val_loss: 0.4336 - val_accuracy: 0.8451 - val_mse: 0.1195\n",
      "Epoch 82/200\n",
      "4919/4919 [==============================] - 4s 875us/step - loss: 0.5309 - accuracy: 0.8683 - mse: 0.0998 - val_loss: 0.4755 - val_accuracy: 0.8250 - val_mse: 0.1313\n",
      "Epoch 83/200\n",
      "4919/4919 [==============================] - 4s 836us/step - loss: 0.5331 - accuracy: 0.8701 - mse: 0.0981 - val_loss: 0.4428 - val_accuracy: 0.8436 - val_mse: 0.1213\n",
      "Epoch 84/200\n",
      "4919/4919 [==============================] - 4s 791us/step - loss: 0.5150 - accuracy: 0.8725 - mse: 0.0975 - val_loss: 0.4403 - val_accuracy: 0.8436 - val_mse: 0.1211\n",
      "Epoch 85/200\n",
      "4919/4919 [==============================] - 4s 795us/step - loss: 0.5223 - accuracy: 0.8699 - mse: 0.0977 - val_loss: 0.4487 - val_accuracy: 0.8364 - val_mse: 0.1235\n",
      "Epoch 86/200\n",
      "4919/4919 [==============================] - 4s 789us/step - loss: 0.5247 - accuracy: 0.8740 - mse: 0.0955 - val_loss: 0.4496 - val_accuracy: 0.8364 - val_mse: 0.1233\n",
      "Epoch 87/200\n",
      "4919/4919 [==============================] - 4s 763us/step - loss: 0.5098 - accuracy: 0.8760 - mse: 0.0952 - val_loss: 0.4222 - val_accuracy: 0.8508 - val_mse: 0.1157\n",
      "Epoch 88/200\n",
      "4919/4919 [==============================] - 4s 815us/step - loss: 0.5115 - accuracy: 0.8756 - mse: 0.0936 - val_loss: 0.4454 - val_accuracy: 0.8379 - val_mse: 0.1220\n",
      "Epoch 89/200\n",
      "4919/4919 [==============================] - 4s 774us/step - loss: 0.5109 - accuracy: 0.8764 - mse: 0.0935 - val_loss: 0.5029 - val_accuracy: 0.8178 - val_mse: 0.1373\n",
      "Epoch 90/200\n",
      "4919/4919 [==============================] - 4s 759us/step - loss: 0.4970 - accuracy: 0.8774 - mse: 0.0930 - val_loss: 0.4370 - val_accuracy: 0.8451 - val_mse: 0.1194\n",
      "Epoch 91/200\n",
      "4919/4919 [==============================] - 4s 827us/step - loss: 0.4940 - accuracy: 0.8790 - mse: 0.0915 - val_loss: 0.4453 - val_accuracy: 0.8422 - val_mse: 0.1215\n",
      "Epoch 92/200\n",
      "4919/4919 [==============================] - 4s 767us/step - loss: 0.5014 - accuracy: 0.8762 - mse: 0.0933 - val_loss: 0.4378 - val_accuracy: 0.8479 - val_mse: 0.1197\n",
      "Epoch 93/200\n",
      "4919/4919 [==============================] - 4s 725us/step - loss: 0.4938 - accuracy: 0.8790 - mse: 0.0911 - val_loss: 0.4444 - val_accuracy: 0.8422 - val_mse: 0.1216\n",
      "Epoch 94/200\n",
      "4919/4919 [==============================] - 4s 782us/step - loss: 0.4953 - accuracy: 0.8782 - mse: 0.0909 - val_loss: 0.4256 - val_accuracy: 0.8508 - val_mse: 0.1161\n",
      "Epoch 95/200\n",
      "4919/4919 [==============================] - 4s 757us/step - loss: 0.4871 - accuracy: 0.8819 - mse: 0.0899 - val_loss: 0.4106 - val_accuracy: 0.8537 - val_mse: 0.1123\n",
      "Epoch 96/200\n",
      "4919/4919 [==============================] - 4s 795us/step - loss: 0.4775 - accuracy: 0.8817 - mse: 0.0882 - val_loss: 0.4212 - val_accuracy: 0.8494 - val_mse: 0.1151\n",
      "Epoch 97/200\n",
      "4919/4919 [==============================] - 4s 731us/step - loss: 0.4865 - accuracy: 0.8862 - mse: 0.0866 - val_loss: 0.4224 - val_accuracy: 0.8537 - val_mse: 0.1149\n",
      "Epoch 98/200\n",
      "4919/4919 [==============================] - 4s 736us/step - loss: 0.4790 - accuracy: 0.8851 - mse: 0.0875 - val_loss: 0.4497 - val_accuracy: 0.8407 - val_mse: 0.1224\n",
      "Epoch 99/200\n",
      "4919/4919 [==============================] - 4s 713us/step - loss: 0.4805 - accuracy: 0.8872 - mse: 0.0884 - val_loss: 0.4583 - val_accuracy: 0.8350 - val_mse: 0.1249\n",
      "Epoch 100/200\n",
      "4919/4919 [==============================] - 4s 715us/step - loss: 0.4731 - accuracy: 0.8862 - mse: 0.0859 - val_loss: 0.4177 - val_accuracy: 0.8551 - val_mse: 0.1133\n",
      "Epoch 101/200\n",
      "4919/4919 [==============================] - 4s 727us/step - loss: 0.4748 - accuracy: 0.8847 - mse: 0.0871 - val_loss: 0.3896 - val_accuracy: 0.8623 - val_mse: 0.1057\n",
      "Epoch 102/200\n",
      "4919/4919 [==============================] - 4s 716us/step - loss: 0.4730 - accuracy: 0.8884 - mse: 0.0858 - val_loss: 0.4246 - val_accuracy: 0.8508 - val_mse: 0.1151\n",
      "Epoch 103/200\n",
      "4919/4919 [==============================] - 4s 914us/step - loss: 0.4608 - accuracy: 0.8880 - mse: 0.0847 - val_loss: 0.4200 - val_accuracy: 0.8522 - val_mse: 0.1136\n",
      "Epoch 104/200\n",
      "4919/4919 [==============================] - 4s 795us/step - loss: 0.4580 - accuracy: 0.8896 - mse: 0.0854 - val_loss: 0.3945 - val_accuracy: 0.8594 - val_mse: 0.1071\n",
      "Epoch 105/200\n",
      "4919/4919 [==============================] - 4s 722us/step - loss: 0.4561 - accuracy: 0.8943 - mse: 0.0817 - val_loss: 0.4138 - val_accuracy: 0.8565 - val_mse: 0.1119\n",
      "Epoch 106/200\n",
      "4919/4919 [==============================] - 4s 721us/step - loss: 0.4509 - accuracy: 0.8929 - mse: 0.0818 - val_loss: 0.4076 - val_accuracy: 0.8580 - val_mse: 0.1103\n",
      "Epoch 107/200\n",
      "4919/4919 [==============================] - 4s 734us/step - loss: 0.4480 - accuracy: 0.8933 - mse: 0.0822 - val_loss: 0.4254 - val_accuracy: 0.8537 - val_mse: 0.1149\n",
      "Epoch 108/200\n",
      "4919/4919 [==============================] - 4s 714us/step - loss: 0.4563 - accuracy: 0.8898 - mse: 0.0835 - val_loss: 0.3958 - val_accuracy: 0.8594 - val_mse: 0.1073\n",
      "Epoch 109/200\n",
      "4919/4919 [==============================] - 4s 714us/step - loss: 0.4581 - accuracy: 0.8923 - mse: 0.0816 - val_loss: 0.4144 - val_accuracy: 0.8565 - val_mse: 0.1124\n",
      "Epoch 110/200\n",
      "4919/4919 [==============================] - 4s 726us/step - loss: 0.4431 - accuracy: 0.8949 - mse: 0.0807 - val_loss: 0.4122 - val_accuracy: 0.8565 - val_mse: 0.1117\n",
      "Epoch 111/200\n",
      "4919/4919 [==============================] - 4s 720us/step - loss: 0.4459 - accuracy: 0.8961 - mse: 0.0802 - val_loss: 0.4352 - val_accuracy: 0.8494 - val_mse: 0.1178\n",
      "Epoch 112/200\n",
      "4919/4919 [==============================] - 4s 712us/step - loss: 0.4356 - accuracy: 0.8961 - mse: 0.0805 - val_loss: 0.3982 - val_accuracy: 0.8594 - val_mse: 0.1082\n",
      "Epoch 113/200\n",
      "4919/4919 [==============================] - 4s 722us/step - loss: 0.4448 - accuracy: 0.8967 - mse: 0.0798 - val_loss: 0.4405 - val_accuracy: 0.8508 - val_mse: 0.1189\n",
      "Epoch 114/200\n",
      "4919/4919 [==============================] - 4s 718us/step - loss: 0.4264 - accuracy: 0.8998 - mse: 0.0785 - val_loss: 0.4071 - val_accuracy: 0.8580 - val_mse: 0.1102\n",
      "Epoch 115/200\n",
      "4919/4919 [==============================] - 4s 717us/step - loss: 0.4362 - accuracy: 0.8998 - mse: 0.0783 - val_loss: 0.4234 - val_accuracy: 0.8551 - val_mse: 0.1147\n",
      "Epoch 116/200\n",
      "4919/4919 [==============================] - 4s 730us/step - loss: 0.4313 - accuracy: 0.8959 - mse: 0.0792 - val_loss: 0.4356 - val_accuracy: 0.8508 - val_mse: 0.1178\n",
      "Epoch 117/200\n",
      "4919/4919 [==============================] - 4s 712us/step - loss: 0.4420 - accuracy: 0.8990 - mse: 0.0786 - val_loss: 0.3942 - val_accuracy: 0.8637 - val_mse: 0.1069\n",
      "Epoch 118/200\n",
      "4919/4919 [==============================] - 4s 713us/step - loss: 0.4271 - accuracy: 0.8994 - mse: 0.0771 - val_loss: 0.4488 - val_accuracy: 0.8451 - val_mse: 0.1213\n",
      "Epoch 119/200\n",
      "4919/4919 [==============================] - 4s 731us/step - loss: 0.4278 - accuracy: 0.8979 - mse: 0.0782 - val_loss: 0.3490 - val_accuracy: 0.8752 - val_mse: 0.0951\n",
      "Epoch 120/200\n",
      "4919/4919 [==============================] - 4s 735us/step - loss: 0.4261 - accuracy: 0.8986 - mse: 0.0779 - val_loss: 0.3902 - val_accuracy: 0.8666 - val_mse: 0.1060\n",
      "Epoch 121/200\n",
      "4919/4919 [==============================] - 4s 728us/step - loss: 0.4161 - accuracy: 0.9006 - mse: 0.0748 - val_loss: 0.3949 - val_accuracy: 0.8651 - val_mse: 0.1071\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4919/4919 [==============================] - 4s 743us/step - loss: 0.4289 - accuracy: 0.9020 - mse: 0.0756 - val_loss: 0.3821 - val_accuracy: 0.8680 - val_mse: 0.1033\n",
      "Epoch 123/200\n",
      "4919/4919 [==============================] - 4s 715us/step - loss: 0.4244 - accuracy: 0.8977 - mse: 0.0768 - val_loss: 0.3907 - val_accuracy: 0.8666 - val_mse: 0.1058\n",
      "Epoch 124/200\n",
      "4919/4919 [==============================] - 4s 726us/step - loss: 0.4102 - accuracy: 0.9032 - mse: 0.0742 - val_loss: 0.3977 - val_accuracy: 0.8623 - val_mse: 0.1076\n",
      "Epoch 125/200\n",
      "4919/4919 [==============================] - 4s 729us/step - loss: 0.4199 - accuracy: 0.9038 - mse: 0.0738 - val_loss: 0.4156 - val_accuracy: 0.8565 - val_mse: 0.1121\n",
      "Epoch 126/200\n",
      "4919/4919 [==============================] - 3s 710us/step - loss: 0.4132 - accuracy: 0.9002 - mse: 0.0755 - val_loss: 0.4280 - val_accuracy: 0.8522 - val_mse: 0.1155\n",
      "Epoch 127/200\n",
      "4919/4919 [==============================] - 3s 709us/step - loss: 0.4094 - accuracy: 0.9053 - mse: 0.0735 - val_loss: 0.4204 - val_accuracy: 0.8580 - val_mse: 0.1129\n",
      "Epoch 128/200\n",
      "4919/4919 [==============================] - 4s 721us/step - loss: 0.4041 - accuracy: 0.9049 - mse: 0.0729 - val_loss: 0.4059 - val_accuracy: 0.8608 - val_mse: 0.1100\n",
      "Epoch 129/200\n",
      "4919/4919 [==============================] - 4s 726us/step - loss: 0.4122 - accuracy: 0.9049 - mse: 0.0727 - val_loss: 0.4251 - val_accuracy: 0.8551 - val_mse: 0.1143\n",
      "Epoch 130/200\n",
      "4919/4919 [==============================] - 4s 745us/step - loss: 0.4011 - accuracy: 0.9042 - mse: 0.0725 - val_loss: 0.4243 - val_accuracy: 0.8565 - val_mse: 0.1139\n",
      "Epoch 131/200\n",
      "4919/4919 [==============================] - 4s 732us/step - loss: 0.4018 - accuracy: 0.9038 - mse: 0.0729 - val_loss: 0.3920 - val_accuracy: 0.8651 - val_mse: 0.1058\n",
      "Epoch 132/200\n",
      "4919/4919 [==============================] - 4s 731us/step - loss: 0.4030 - accuracy: 0.9065 - mse: 0.0721 - val_loss: 0.3882 - val_accuracy: 0.8651 - val_mse: 0.1054\n",
      "Epoch 133/200\n",
      "4919/4919 [==============================] - 4s 719us/step - loss: 0.4058 - accuracy: 0.9047 - mse: 0.0721 - val_loss: 0.3560 - val_accuracy: 0.8752 - val_mse: 0.0967\n",
      "Epoch 134/200\n",
      "4919/4919 [==============================] - 4s 720us/step - loss: 0.4003 - accuracy: 0.9093 - mse: 0.0711 - val_loss: 0.3787 - val_accuracy: 0.8737 - val_mse: 0.1027\n",
      "Epoch 135/200\n",
      "4919/4919 [==============================] - 3s 710us/step - loss: 0.4010 - accuracy: 0.9042 - mse: 0.0722 - val_loss: 0.3929 - val_accuracy: 0.8666 - val_mse: 0.1065\n",
      "Epoch 136/200\n",
      "4919/4919 [==============================] - 4s 726us/step - loss: 0.3868 - accuracy: 0.9101 - mse: 0.0707 - val_loss: 0.3892 - val_accuracy: 0.8680 - val_mse: 0.1056\n",
      "Epoch 137/200\n",
      "4919/4919 [==============================] - 4s 715us/step - loss: 0.3860 - accuracy: 0.9122 - mse: 0.0688 - val_loss: 0.3916 - val_accuracy: 0.8694 - val_mse: 0.1059\n",
      "Epoch 138/200\n",
      "4919/4919 [==============================] - 4s 728us/step - loss: 0.3965 - accuracy: 0.9108 - mse: 0.0692 - val_loss: 0.4069 - val_accuracy: 0.8608 - val_mse: 0.1096\n",
      "Epoch 139/200\n",
      "4919/4919 [==============================] - 4s 734us/step - loss: 0.3945 - accuracy: 0.9057 - mse: 0.0716 - val_loss: 0.3881 - val_accuracy: 0.8666 - val_mse: 0.1054\n",
      "Epoch 140/200\n",
      "4919/4919 [==============================] - 4s 725us/step - loss: 0.3902 - accuracy: 0.9091 - mse: 0.0701 - val_loss: 0.4030 - val_accuracy: 0.8637 - val_mse: 0.1091\n",
      "Epoch 141/200\n",
      "4919/4919 [==============================] - 4s 725us/step - loss: 0.3794 - accuracy: 0.9114 - mse: 0.0683 - val_loss: 0.3696 - val_accuracy: 0.8766 - val_mse: 0.1006\n",
      "Epoch 142/200\n",
      "4919/4919 [==============================] - 4s 891us/step - loss: 0.3924 - accuracy: 0.9091 - mse: 0.0698 - val_loss: 0.3727 - val_accuracy: 0.8752 - val_mse: 0.1011\n",
      "Epoch 143/200\n",
      "4919/4919 [==============================] - 6s 1ms/step - loss: 0.3789 - accuracy: 0.9116 - mse: 0.0687 - val_loss: 0.3701 - val_accuracy: 0.8752 - val_mse: 0.1008\n",
      "Epoch 144/200\n",
      "4919/4919 [==============================] - 5s 948us/step - loss: 0.3727 - accuracy: 0.9108 - mse: 0.0687 - val_loss: 0.3840 - val_accuracy: 0.8723 - val_mse: 0.1043\n",
      "Epoch 145/200\n",
      "4919/4919 [==============================] - 4s 803us/step - loss: 0.3776 - accuracy: 0.9138 - mse: 0.0655 - val_loss: 0.3699 - val_accuracy: 0.8752 - val_mse: 0.1006\n",
      "Epoch 146/200\n",
      "4919/4919 [==============================] - 4s 847us/step - loss: 0.3742 - accuracy: 0.9120 - mse: 0.0677 - val_loss: 0.3479 - val_accuracy: 0.8809 - val_mse: 0.0956\n",
      "Epoch 147/200\n",
      "4919/4919 [==============================] - 4s 783us/step - loss: 0.3703 - accuracy: 0.9148 - mse: 0.0658 - val_loss: 0.3720 - val_accuracy: 0.8780 - val_mse: 0.1011\n",
      "Epoch 148/200\n",
      "4919/4919 [==============================] - 4s 802us/step - loss: 0.3686 - accuracy: 0.9154 - mse: 0.0661 - val_loss: 0.3604 - val_accuracy: 0.8780 - val_mse: 0.0986\n",
      "Epoch 149/200\n",
      "4919/4919 [==============================] - 4s 738us/step - loss: 0.3714 - accuracy: 0.9181 - mse: 0.0646 - val_loss: 0.4024 - val_accuracy: 0.8666 - val_mse: 0.1086\n",
      "Epoch 150/200\n",
      "4919/4919 [==============================] - 4s 737us/step - loss: 0.3736 - accuracy: 0.9132 - mse: 0.0679 - val_loss: 0.3566 - val_accuracy: 0.8795 - val_mse: 0.0971\n",
      "Epoch 151/200\n",
      "4919/4919 [==============================] - 4s 760us/step - loss: 0.3790 - accuracy: 0.9128 - mse: 0.0673 - val_loss: 0.3949 - val_accuracy: 0.8651 - val_mse: 0.1069\n",
      "Epoch 152/200\n",
      "4919/4919 [==============================] - 4s 776us/step - loss: 0.3574 - accuracy: 0.9132 - mse: 0.0660 - val_loss: 0.3405 - val_accuracy: 0.8795 - val_mse: 0.0935\n",
      "Epoch 153/200\n",
      "4919/4919 [==============================] - 4s 762us/step - loss: 0.3625 - accuracy: 0.9164 - mse: 0.0632 - val_loss: 0.3928 - val_accuracy: 0.8709 - val_mse: 0.1060\n",
      "Epoch 154/200\n",
      "4919/4919 [==============================] - 4s 752us/step - loss: 0.3711 - accuracy: 0.9130 - mse: 0.0658 - val_loss: 0.3523 - val_accuracy: 0.8795 - val_mse: 0.0961\n",
      "Epoch 155/200\n",
      "4919/4919 [==============================] - 4s 784us/step - loss: 0.3651 - accuracy: 0.9142 - mse: 0.0659 - val_loss: 0.3911 - val_accuracy: 0.8666 - val_mse: 0.1059\n",
      "Epoch 156/200\n",
      "4919/4919 [==============================] - 4s 820us/step - loss: 0.3631 - accuracy: 0.9187 - mse: 0.0634 - val_loss: 0.3778 - val_accuracy: 0.8795 - val_mse: 0.1025\n",
      "Epoch 157/200\n",
      "4919/4919 [==============================] - 4s 867us/step - loss: 0.3512 - accuracy: 0.9158 - mse: 0.0645 - val_loss: 0.3421 - val_accuracy: 0.8809 - val_mse: 0.0936\n",
      "Epoch 158/200\n",
      "4919/4919 [==============================] - 4s 787us/step - loss: 0.3603 - accuracy: 0.9154 - mse: 0.0639 - val_loss: 0.3705 - val_accuracy: 0.8795 - val_mse: 0.1009\n",
      "Epoch 159/200\n",
      "4919/4919 [==============================] - 4s 750us/step - loss: 0.3537 - accuracy: 0.9197 - mse: 0.0624 - val_loss: 0.3840 - val_accuracy: 0.8766 - val_mse: 0.1038\n",
      "Epoch 160/200\n",
      "4919/4919 [==============================] - 4s 749us/step - loss: 0.3543 - accuracy: 0.9183 - mse: 0.0634 - val_loss: 0.3581 - val_accuracy: 0.8809 - val_mse: 0.0977\n",
      "Epoch 161/200\n",
      "4919/4919 [==============================] - 4s 742us/step - loss: 0.3492 - accuracy: 0.9203 - mse: 0.0628 - val_loss: 0.3647 - val_accuracy: 0.8795 - val_mse: 0.0994\n",
      "Epoch 162/200\n",
      "4919/4919 [==============================] - 4s 770us/step - loss: 0.3464 - accuracy: 0.9199 - mse: 0.0617 - val_loss: 0.3514 - val_accuracy: 0.8809 - val_mse: 0.0961\n",
      "Epoch 163/200\n",
      "4919/4919 [==============================] - 4s 763us/step - loss: 0.3532 - accuracy: 0.9191 - mse: 0.0628 - val_loss: 0.3537 - val_accuracy: 0.8809 - val_mse: 0.0966\n",
      "Epoch 164/200\n",
      "4919/4919 [==============================] - 4s 750us/step - loss: 0.3467 - accuracy: 0.9219 - mse: 0.0600 - val_loss: 0.4252 - val_accuracy: 0.8580 - val_mse: 0.1137\n",
      "Epoch 165/200\n",
      "4919/4919 [==============================] - 4s 840us/step - loss: 0.3410 - accuracy: 0.9179 - mse: 0.0630 - val_loss: 0.3401 - val_accuracy: 0.8809 - val_mse: 0.0936\n",
      "Epoch 166/200\n",
      "4919/4919 [==============================] - 4s 803us/step - loss: 0.3484 - accuracy: 0.9219 - mse: 0.0596 - val_loss: 0.4065 - val_accuracy: 0.8637 - val_mse: 0.1089\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4919/4919 [==============================] - 4s 739us/step - loss: 0.3447 - accuracy: 0.9217 - mse: 0.0627 - val_loss: 0.3686 - val_accuracy: 0.8780 - val_mse: 0.1002\n",
      "Epoch 168/200\n",
      "4919/4919 [==============================] - 4s 761us/step - loss: 0.3394 - accuracy: 0.9207 - mse: 0.0603 - val_loss: 0.3653 - val_accuracy: 0.8795 - val_mse: 0.0994\n",
      "Epoch 169/200\n",
      "4919/4919 [==============================] - 4s 808us/step - loss: 0.3472 - accuracy: 0.9221 - mse: 0.0606 - val_loss: 0.3565 - val_accuracy: 0.8809 - val_mse: 0.0971\n",
      "Epoch 170/200\n",
      "4919/4919 [==============================] - 4s 762us/step - loss: 0.3375 - accuracy: 0.9187 - mse: 0.0612 - val_loss: 0.3566 - val_accuracy: 0.8809 - val_mse: 0.0974\n",
      "Epoch 171/200\n",
      "4919/4919 [==============================] - 4s 789us/step - loss: 0.3336 - accuracy: 0.9246 - mse: 0.0586 - val_loss: 0.3733 - val_accuracy: 0.8795 - val_mse: 0.1013\n",
      "Epoch 172/200\n",
      "4919/4919 [==============================] - 4s 771us/step - loss: 0.3419 - accuracy: 0.9236 - mse: 0.0603 - val_loss: 0.3574 - val_accuracy: 0.8809 - val_mse: 0.0973\n",
      "Epoch 173/200\n",
      "4919/4919 [==============================] - 4s 772us/step - loss: 0.3365 - accuracy: 0.9223 - mse: 0.0595 - val_loss: 0.3510 - val_accuracy: 0.8809 - val_mse: 0.0957\n",
      "Epoch 174/200\n",
      "4919/4919 [==============================] - 4s 763us/step - loss: 0.3302 - accuracy: 0.9227 - mse: 0.0593 - val_loss: 0.3558 - val_accuracy: 0.8795 - val_mse: 0.0970\n",
      "Epoch 175/200\n",
      "4919/4919 [==============================] - 4s 775us/step - loss: 0.3336 - accuracy: 0.9230 - mse: 0.0594 - val_loss: 0.3371 - val_accuracy: 0.8838 - val_mse: 0.0924\n",
      "Epoch 176/200\n",
      "4919/4919 [==============================] - 4s 746us/step - loss: 0.3256 - accuracy: 0.9244 - mse: 0.0577 - val_loss: 0.3626 - val_accuracy: 0.8795 - val_mse: 0.0989\n",
      "Epoch 177/200\n",
      "4919/4919 [==============================] - 4s 790us/step - loss: 0.3346 - accuracy: 0.9209 - mse: 0.0599 - val_loss: 0.3671 - val_accuracy: 0.8795 - val_mse: 0.0998\n",
      "Epoch 178/200\n",
      "4919/4919 [==============================] - 4s 802us/step - loss: 0.3355 - accuracy: 0.9221 - mse: 0.0598 - val_loss: 0.3620 - val_accuracy: 0.8795 - val_mse: 0.0985\n",
      "Epoch 179/200\n",
      "4919/4919 [==============================] - 4s 748us/step - loss: 0.3207 - accuracy: 0.9272 - mse: 0.0579 - val_loss: 0.3379 - val_accuracy: 0.8838 - val_mse: 0.0929\n",
      "Epoch 180/200\n",
      "4919/4919 [==============================] - 4s 777us/step - loss: 0.3208 - accuracy: 0.9246 - mse: 0.0564 - val_loss: 0.3536 - val_accuracy: 0.8824 - val_mse: 0.0961\n",
      "Epoch 181/200\n",
      "4919/4919 [==============================] - 4s 769us/step - loss: 0.3199 - accuracy: 0.9250 - mse: 0.0576 - val_loss: 0.3136 - val_accuracy: 0.8881 - val_mse: 0.0870\n",
      "Epoch 182/200\n",
      "4919/4919 [==============================] - 4s 810us/step - loss: 0.3197 - accuracy: 0.9278 - mse: 0.0560 - val_loss: 0.3723 - val_accuracy: 0.8780 - val_mse: 0.1014\n",
      "Epoch 183/200\n",
      "4919/4919 [==============================] - 4s 908us/step - loss: 0.3234 - accuracy: 0.9225 - mse: 0.0589 - val_loss: 0.3414 - val_accuracy: 0.8838 - val_mse: 0.0940\n",
      "Epoch 184/200\n",
      "4919/4919 [==============================] - 4s 877us/step - loss: 0.3155 - accuracy: 0.9268 - mse: 0.0557 - val_loss: 0.3334 - val_accuracy: 0.8824 - val_mse: 0.0917\n",
      "Epoch 185/200\n",
      "4919/4919 [==============================] - 4s 858us/step - loss: 0.3201 - accuracy: 0.9270 - mse: 0.0562 - val_loss: 0.3305 - val_accuracy: 0.8852 - val_mse: 0.0910\n",
      "Epoch 186/200\n",
      "4919/4919 [==============================] - 4s 753us/step - loss: 0.3136 - accuracy: 0.9270 - mse: 0.0556 - val_loss: 0.3575 - val_accuracy: 0.8795 - val_mse: 0.0975\n",
      "Epoch 187/200\n",
      "4919/4919 [==============================] - 4s 766us/step - loss: 0.3122 - accuracy: 0.9274 - mse: 0.0555 - val_loss: 0.3552 - val_accuracy: 0.8824 - val_mse: 0.0968\n",
      "Epoch 188/200\n",
      "4919/4919 [==============================] - 4s 859us/step - loss: 0.3203 - accuracy: 0.9264 - mse: 0.0574 - val_loss: 0.3485 - val_accuracy: 0.8795 - val_mse: 0.0955\n",
      "Epoch 189/200\n",
      "4919/4919 [==============================] - 4s 739us/step - loss: 0.3048 - accuracy: 0.9297 - mse: 0.0543 - val_loss: 0.3344 - val_accuracy: 0.8838 - val_mse: 0.0920\n",
      "Epoch 190/200\n",
      "4919/4919 [==============================] - 4s 758us/step - loss: 0.3079 - accuracy: 0.9297 - mse: 0.0545 - val_loss: 0.3618 - val_accuracy: 0.8795 - val_mse: 0.0983\n",
      "Epoch 191/200\n",
      "4919/4919 [==============================] - 4s 776us/step - loss: 0.3137 - accuracy: 0.9297 - mse: 0.0553 - val_loss: 0.3519 - val_accuracy: 0.8809 - val_mse: 0.0963\n",
      "Epoch 192/200\n",
      "4919/4919 [==============================] - 4s 769us/step - loss: 0.3078 - accuracy: 0.9274 - mse: 0.0557 - val_loss: 0.3336 - val_accuracy: 0.8852 - val_mse: 0.0920\n",
      "Epoch 193/200\n",
      "4919/4919 [==============================] - 4s 760us/step - loss: 0.3075 - accuracy: 0.9295 - mse: 0.0543 - val_loss: 0.3591 - val_accuracy: 0.8795 - val_mse: 0.0983\n",
      "Epoch 194/200\n",
      "4919/4919 [==============================] - 4s 756us/step - loss: 0.3091 - accuracy: 0.9295 - mse: 0.0542 - val_loss: 0.3424 - val_accuracy: 0.8824 - val_mse: 0.0942\n",
      "Epoch 195/200\n",
      "4919/4919 [==============================] - 4s 766us/step - loss: 0.2950 - accuracy: 0.9307 - mse: 0.0535 - val_loss: 0.3010 - val_accuracy: 0.8924 - val_mse: 0.0842\n",
      "Epoch 196/200\n",
      "4919/4919 [==============================] - 4s 768us/step - loss: 0.3051 - accuracy: 0.9291 - mse: 0.0533 - val_loss: 0.3124 - val_accuracy: 0.8881 - val_mse: 0.0870\n",
      "Epoch 197/200\n",
      "4919/4919 [==============================] - 4s 748us/step - loss: 0.3047 - accuracy: 0.9299 - mse: 0.0536 - val_loss: 0.3378 - val_accuracy: 0.8838 - val_mse: 0.0930\n",
      "Epoch 198/200\n",
      "4919/4919 [==============================] - 4s 761us/step - loss: 0.2968 - accuracy: 0.9313 - mse: 0.0527 - val_loss: 0.3580 - val_accuracy: 0.8809 - val_mse: 0.0977\n",
      "Epoch 199/200\n",
      "4919/4919 [==============================] - 4s 771us/step - loss: 0.3069 - accuracy: 0.9309 - mse: 0.0542 - val_loss: 0.3371 - val_accuracy: 0.8852 - val_mse: 0.0927\n",
      "Epoch 200/200\n",
      "4919/4919 [==============================] - 4s 778us/step - loss: 0.2925 - accuracy: 0.9321 - mse: 0.0529 - val_loss: 0.3507 - val_accuracy: 0.8838 - val_mse: 0.0960\n",
      "461/461 [==============================] - 0s 359us/step\n",
      "Final loss = 0.2940661326503288\n",
      "Final accuracy = 0.9045553207397461\n",
      "Left out subject = joao\n",
      "Test subjects = ['adela', 'aggie', 'diana', 'jack', 'lukasz', 'nikita', 'rim', 'santi', 'seb', 'sharan', 'teo', 'zoe']\n",
      "Validation subjects = ['ron', 'andrius']\n",
      "--------------------------------------------------------------------------------\n",
      "Removing outliers\n",
      "--------------------------------------------------------------------------------\n",
      "Original dataframe length = 120900\n",
      "New dataframe length = 118548\n",
      "Removed 2352 outliers\n",
      "Original dataframe length = 5567\n",
      "New dataframe length = 5422\n",
      "Removed 145 outliers\n",
      "--------------------------------------------------------------------------------\n",
      "Standardising\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Normalising\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generating datasets\n",
      "--------------------------------------------------------------------------------\n",
      "Total samples for true activity = 4805\n",
      "Total samples for false activity = 0\n",
      "Training set shapes: X_train = (4805, 38, 3), y_train = (4805, 2)\n",
      "Total samples for true activity = 1018\n",
      "Total samples for false activity = 0\n",
      "Valid set shapes: X_valid = (1018, 38, 3), y_valid = (1018, 2)\n",
      "Total samples for true activity = 255\n",
      "Total samples for false activity = 0\n",
      "Test set shapes: X_test = (255, 38, 3), y_test = (255, 2)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 36, 64)            640       \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 34, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 34, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 32, 64)            12352     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               204900    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 230,702\n",
      "Trainable params: 230,574\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4805 samples, validate on 1018 samples\n",
      "Epoch 1/200\n",
      "4805/4805 [==============================] - 6s 1ms/step - loss: 2.2996 - accuracy: 0.2094 - mse: 0.4327 - val_loss: 0.7771 - val_accuracy: 0.1415 - val_mse: 0.2918\n",
      "Epoch 2/200\n",
      "4805/4805 [==============================] - 4s 803us/step - loss: 1.9217 - accuracy: 0.1702 - mse: 0.5002 - val_loss: 0.7942 - val_accuracy: 0.1582 - val_mse: 0.3001\n",
      "Epoch 3/200\n",
      "4805/4805 [==============================] - 4s 838us/step - loss: 1.8167 - accuracy: 0.1771 - mse: 0.4880 - val_loss: 0.8702 - val_accuracy: 0.1758 - val_mse: 0.3354\n",
      "Epoch 4/200\n",
      "4805/4805 [==============================] - 4s 887us/step - loss: 1.7300 - accuracy: 0.1904 - mse: 0.4684 - val_loss: 0.9943 - val_accuracy: 0.2033 - val_mse: 0.3852\n",
      "Epoch 5/200\n",
      "4805/4805 [==============================] - 4s 892us/step - loss: 1.6638 - accuracy: 0.2144 - mse: 0.4426 - val_loss: 1.1085 - val_accuracy: 0.2358 - val_mse: 0.4201\n",
      "Epoch 6/200\n",
      "4805/4805 [==============================] - 4s 900us/step - loss: 1.6044 - accuracy: 0.2337 - mse: 0.4315 - val_loss: 1.1022 - val_accuracy: 0.2721 - val_mse: 0.4104\n",
      "Epoch 7/200\n",
      "4805/4805 [==============================] - 4s 862us/step - loss: 1.5465 - accuracy: 0.2751 - mse: 0.4092 - val_loss: 1.0744 - val_accuracy: 0.3143 - val_mse: 0.3955\n",
      "Epoch 8/200\n",
      "4805/4805 [==============================] - 4s 790us/step - loss: 1.4983 - accuracy: 0.3149 - mse: 0.3917 - val_loss: 1.0443 - val_accuracy: 0.3615 - val_mse: 0.3803\n",
      "Epoch 9/200\n",
      "4805/4805 [==============================] - 4s 822us/step - loss: 1.4579 - accuracy: 0.3417 - mse: 0.3834 - val_loss: 1.0185 - val_accuracy: 0.4008 - val_mse: 0.3672\n",
      "Epoch 10/200\n",
      "4805/4805 [==============================] - 4s 794us/step - loss: 1.4168 - accuracy: 0.3827 - mse: 0.3703 - val_loss: 0.9649 - val_accuracy: 0.4440 - val_mse: 0.3461\n",
      "Epoch 11/200\n",
      "4805/4805 [==============================] - 4s 793us/step - loss: 1.3776 - accuracy: 0.4310 - mse: 0.3494 - val_loss: 0.9627 - val_accuracy: 0.4617 - val_mse: 0.3430\n",
      "Epoch 12/200\n",
      "4805/4805 [==============================] - 4s 798us/step - loss: 1.3418 - accuracy: 0.4529 - mse: 0.3429 - val_loss: 0.9445 - val_accuracy: 0.4853 - val_mse: 0.3342\n",
      "Epoch 13/200\n",
      "4805/4805 [==============================] - 4s 798us/step - loss: 1.3096 - accuracy: 0.4758 - mse: 0.3333 - val_loss: 0.9083 - val_accuracy: 0.5285 - val_mse: 0.3188\n",
      "Epoch 14/200\n",
      "4805/4805 [==============================] - 4s 795us/step - loss: 1.2806 - accuracy: 0.5030 - mse: 0.3226 - val_loss: 0.8749 - val_accuracy: 0.5511 - val_mse: 0.3059\n",
      "Epoch 15/200\n",
      "4805/4805 [==============================] - 4s 781us/step - loss: 1.2496 - accuracy: 0.5230 - mse: 0.3123 - val_loss: 0.8371 - val_accuracy: 0.5766 - val_mse: 0.2907\n",
      "Epoch 16/200\n",
      "4805/4805 [==============================] - 4s 815us/step - loss: 1.2226 - accuracy: 0.5459 - mse: 0.3035 - val_loss: 0.8262 - val_accuracy: 0.5943 - val_mse: 0.2858\n",
      "Epoch 17/200\n",
      "4805/4805 [==============================] - 4s 820us/step - loss: 1.1924 - accuracy: 0.5673 - mse: 0.2917 - val_loss: 0.8174 - val_accuracy: 0.5982 - val_mse: 0.2812\n",
      "Epoch 18/200\n",
      "4805/4805 [==============================] - 4s 788us/step - loss: 1.1729 - accuracy: 0.5744 - mse: 0.2867 - val_loss: 0.7874 - val_accuracy: 0.6198 - val_mse: 0.2700\n",
      "Epoch 19/200\n",
      "4805/4805 [==============================] - 4s 805us/step - loss: 1.1494 - accuracy: 0.5906 - mse: 0.2783 - val_loss: 0.7715 - val_accuracy: 0.6267 - val_mse: 0.2637\n",
      "Epoch 20/200\n",
      "4805/4805 [==============================] - 4s 878us/step - loss: 1.1243 - accuracy: 0.6035 - mse: 0.2723 - val_loss: 0.7586 - val_accuracy: 0.6306 - val_mse: 0.2584\n",
      "Epoch 21/200\n",
      "4805/4805 [==============================] - 4s 783us/step - loss: 1.1072 - accuracy: 0.6108 - mse: 0.2672 - val_loss: 0.7368 - val_accuracy: 0.6444 - val_mse: 0.2502\n",
      "Epoch 22/200\n",
      "4805/4805 [==============================] - 4s 775us/step - loss: 1.0903 - accuracy: 0.6304 - mse: 0.2549 - val_loss: 0.7486 - val_accuracy: 0.6395 - val_mse: 0.2536\n",
      "Epoch 23/200\n",
      "4805/4805 [==============================] - 4s 819us/step - loss: 1.0667 - accuracy: 0.6300 - mse: 0.2542 - val_loss: 0.7165 - val_accuracy: 0.6601 - val_mse: 0.2421\n",
      "Epoch 24/200\n",
      "4805/4805 [==============================] - 4s 782us/step - loss: 1.0479 - accuracy: 0.6468 - mse: 0.2444 - val_loss: 0.7119 - val_accuracy: 0.6631 - val_mse: 0.2397\n",
      "Epoch 25/200\n",
      "4805/4805 [==============================] - 4s 797us/step - loss: 1.0274 - accuracy: 0.6554 - mse: 0.2398 - val_loss: 0.7242 - val_accuracy: 0.6591 - val_mse: 0.2431\n",
      "Epoch 26/200\n",
      "4805/4805 [==============================] - 4s 829us/step - loss: 1.0153 - accuracy: 0.6610 - mse: 0.2393 - val_loss: 0.6770 - val_accuracy: 0.6758 - val_mse: 0.2264\n",
      "Epoch 27/200\n",
      "4805/4805 [==============================] - 4s 813us/step - loss: 0.9937 - accuracy: 0.6758 - mse: 0.2285 - val_loss: 0.6579 - val_accuracy: 0.6857 - val_mse: 0.2193\n",
      "Epoch 28/200\n",
      "4805/4805 [==============================] - 4s 818us/step - loss: 0.9882 - accuracy: 0.6737 - mse: 0.2293 - val_loss: 0.6242 - val_accuracy: 0.7004 - val_mse: 0.2078\n",
      "Epoch 29/200\n",
      "4805/4805 [==============================] - 4s 805us/step - loss: 0.9675 - accuracy: 0.6924 - mse: 0.2178 - val_loss: 0.6508 - val_accuracy: 0.6916 - val_mse: 0.2162\n",
      "Epoch 30/200\n",
      "4805/4805 [==============================] - 4s 802us/step - loss: 0.9470 - accuracy: 0.6978 - mse: 0.2138 - val_loss: 0.6631 - val_accuracy: 0.6935 - val_mse: 0.2194\n",
      "Epoch 31/200\n",
      "4805/4805 [==============================] - 4s 865us/step - loss: 0.9339 - accuracy: 0.6978 - mse: 0.2135 - val_loss: 0.6499 - val_accuracy: 0.6955 - val_mse: 0.2148\n",
      "Epoch 32/200\n",
      "4805/4805 [==============================] - 4s 911us/step - loss: 0.9143 - accuracy: 0.7072 - mse: 0.2057 - val_loss: 0.6226 - val_accuracy: 0.7092 - val_mse: 0.2050\n",
      "Epoch 33/200\n",
      "4805/4805 [==============================] - 6s 1ms/step - loss: 0.9025 - accuracy: 0.7161 - mse: 0.2006 - val_loss: 0.5938 - val_accuracy: 0.7259 - val_mse: 0.1947\n",
      "Epoch 34/200\n",
      "4805/4805 [==============================] - 8s 2ms/step - loss: 0.8935 - accuracy: 0.7201 - mse: 0.1968 - val_loss: 0.6106 - val_accuracy: 0.7141 - val_mse: 0.1995\n",
      "Epoch 35/200\n",
      "4805/4805 [==============================] - 5s 951us/step - loss: 0.8797 - accuracy: 0.7226 - mse: 0.1971 - val_loss: 0.5583 - val_accuracy: 0.7358 - val_mse: 0.1813\n",
      "Epoch 36/200\n",
      "4805/4805 [==============================] - 4s 835us/step - loss: 0.8634 - accuracy: 0.7286 - mse: 0.1898 - val_loss: 0.5983 - val_accuracy: 0.7250 - val_mse: 0.1946\n",
      "Epoch 37/200\n",
      "4805/4805 [==============================] - 4s 863us/step - loss: 0.8454 - accuracy: 0.7369 - mse: 0.1863 - val_loss: 0.5824 - val_accuracy: 0.7318 - val_mse: 0.1889\n",
      "Epoch 38/200\n",
      "4805/4805 [==============================] - 4s 897us/step - loss: 0.8378 - accuracy: 0.7436 - mse: 0.1822 - val_loss: 0.5598 - val_accuracy: 0.7387 - val_mse: 0.1809\n",
      "Epoch 39/200\n",
      "4805/4805 [==============================] - 4s 822us/step - loss: 0.8316 - accuracy: 0.7484 - mse: 0.1797 - val_loss: 0.5413 - val_accuracy: 0.7466 - val_mse: 0.1745\n",
      "Epoch 40/200\n",
      "4805/4805 [==============================] - 4s 885us/step - loss: 0.8182 - accuracy: 0.7476 - mse: 0.1771 - val_loss: 0.5345 - val_accuracy: 0.7525 - val_mse: 0.1720\n",
      "Epoch 41/200\n",
      "4805/4805 [==============================] - 4s 814us/step - loss: 0.8046 - accuracy: 0.7575 - mse: 0.1712 - val_loss: 0.5008 - val_accuracy: 0.7672 - val_mse: 0.1603\n",
      "Epoch 42/200\n",
      "4805/4805 [==============================] - 4s 813us/step - loss: 0.7915 - accuracy: 0.7623 - mse: 0.1695 - val_loss: 0.5212 - val_accuracy: 0.7593 - val_mse: 0.1666\n",
      "Epoch 43/200\n",
      "4805/4805 [==============================] - 4s 859us/step - loss: 0.7839 - accuracy: 0.7677 - mse: 0.1656 - val_loss: 0.4994 - val_accuracy: 0.7692 - val_mse: 0.1589\n",
      "Epoch 44/200\n",
      "1120/4805 [=====>........................] - ETA: 3s - loss: 0.7173 - accuracy: 0.7777 - mse: 0.1601"
     ]
    }
   ],
   "source": [
    "losoxv_stats, cm, histories, mean_cm = losoxv_one_vs_all(experiment_name, one_vs_all_activity, random_seed, correctness,\n",
    "                 n_train_subjects, n_validation_subjects,\n",
    "                 n_time_steps, step, n_features,\n",
    "                 features, num_filters, kernel_size, activation,\n",
    "                 lr, batch_size, epochs, downsample_rate, positive_class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_stats_file = \"../Plots/experiment_stats.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_f1(cms):\n",
    "    \n",
    "    f1_m = []\n",
    "    \n",
    "    for subject, cm in cms.items():\n",
    "        precision = cms[subject][0][0] / (cms[subject][0][0] + cms[subject][1][0])\n",
    "        recall = cms[subject][0][0] / (cms[subject][0][0] + cms[subject][0][1])\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        \n",
    "        f1_m.append(f1)\n",
    "    f1_m = np.array(f1_m)\n",
    "    return f1_m.mean(), f1_m.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes the variables are in the namespace\n",
    "def save_experiment_stats():\n",
    "    with open(experiment_stats_file, 'a') as f: \n",
    "        f.write(\"{},\".format(experiment_name))\n",
    "        f.write(\"{},\".format(activity_name))\n",
    "        f.write(\"{},\".format(correctness))\n",
    "        f.write(\"{},\".format(random_seed))\n",
    "        f.write(\"{},\".format(n_train_subjects))\n",
    "        f.write(\"{},\".format(n_validation_subjects))\n",
    "        f.write(\"{},\".format(n_time_steps))\n",
    "        f.write(\"{},\".format(step))\n",
    "        f.write(\"{},\".format(n_features))\n",
    "        f.write(\"{},\".format(features_name))\n",
    "        f.write(\"{},\".format(num_filters))\n",
    "        f.write(\"{},\".format(kernel_size))\n",
    "        f.write(\"{},\".format(activation))\n",
    "        f.write(\"{},\".format(lr))\n",
    "        f.write(\"{},\".format(batch_size))\n",
    "        f.write(\"{},\".format(epochs))\n",
    "        f.write(\"{},\".format(downsample_rate))\n",
    "        f.write(\"{},\".format(positive_class_weight))\n",
    "        \n",
    "        # accuracy, loss - train\n",
    "        mean_accuracy_train = losoxv_stats['train_acc'].mean()\n",
    "        mean_loss_train = losoxv_stats['train_loss'].mean()\n",
    "        \n",
    "        std_accuracy_train = losoxv_stats['train_acc'].std()\n",
    "        std_loss_train = losoxv_stats['train_loss'].std()\n",
    "        \n",
    "        f.write(\"{},\".format(mean_accuracy_train))\n",
    "        f.write(\"{},\".format(mean_loss_train))\n",
    "        f.write(\"{},\".format(std_accuracy_train))\n",
    "        f.write(\"{},\".format(std_loss_train))\n",
    "        \n",
    "        # accuracy, loss - validation\n",
    "        mean_accuracy_valid = losoxv_stats['valid_acc'].mean()\n",
    "        mean_loss_valid = losoxv_stats['valid_loss'].mean()\n",
    "        \n",
    "        std_accuracy_valid = losoxv_stats['valid_acc'].std()\n",
    "        std_loss_valid = losoxv_stats['valid_loss'].std()\n",
    "        \n",
    "        f.write(\"{},\".format(mean_accuracy_valid))\n",
    "        f.write(\"{},\".format(mean_loss_valid))\n",
    "        f.write(\"{},\".format(std_accuracy_valid))\n",
    "        f.write(\"{},\".format(std_loss_valid))\n",
    "        \n",
    "        # accuracy, f1, loss - test\n",
    "        mean_accuracy_test = losoxv_stats['test_acc'].mean()\n",
    "        mean_loss_test = losoxv_stats['test_loss'].mean()\n",
    "        mean_f1_test, std_f1_test = mean_std_f1(cms)\n",
    "        \n",
    "        std_accuracy_test = losoxv_stats['test_acc'].std()\n",
    "        std_loss_test = losoxv_stats['test_loss'].std()\n",
    "        \n",
    "        f.write(\"{},\".format(mean_accuracy_test))\n",
    "        f.write(\"{},\".format(mean_f1_test))\n",
    "        f.write(\"{},\".format(mean_loss_test))\n",
    "        f.write(\"{},\".format(std_accuracy_test))\n",
    "        f.write(\"{},\".format(std_f1_test))\n",
    "        f.write(\"{},\".format(std_loss_test))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
